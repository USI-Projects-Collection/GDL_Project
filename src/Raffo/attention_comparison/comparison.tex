\documentclass[12pt]{article}
 
\usepackage[margin=0.5in]{geometry} 
\usepackage{amsmath,amsthm,amssymb}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{tabularx}
\usepackage{booktabs}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

\setlength{\parindent}{0pt}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{solution}{\begin{proof}[Solution]}{\end{proof}}
 
\begin{document}

\title{Comparison of Attention Mechanisms:\\From Vanilla to Linear Topological Masking}
\author{}
\date{}
\maketitle

\section{Simple Attention (Vanilla Softmax)}

This is the standard attention mechanism found in the original Transformer. It treats the input as a set (or sequence) and is permutation invariant regarding graph structure.

\subsection*{Formulas and Mechanism}
Given Query ($Q$), Key ($K$), and Value ($V$) matrices of size $N \times d$:
\begin{equation}
    \text{Att}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V
\end{equation}
\textbf{What it concretely does:}
\begin{itemize}
    \item It computes a similarity score (dot product) between \textit{every} pair of tokens.
    \item It normalizes these scores using Softmax so they sum to 1.
    \item It computes a weighted sum of Value vectors based on these scores.
    \item \textbf{Critique:} It is unaware of the graph topology. Node $i$ attends to Node $j$ purely based on feature similarity, ignoring whether they are neighbors or far apart.
\end{itemize}

\subsection*{Time Complexity Derivation}
The bottleneck is the explicit computation of the attention matrix $A = QK^\top$.
\begin{enumerate}
    \item Matrix multiplication $Q \times K^\top$: $(N \times d) \times (d \times N) \rightarrow (N \times N)$.
    \item Complexity: $\mathcal{O}(N^2 d)$. Since $d$ is usually small constant, we write $\mathcal{O}(N^2)$.
    \item Space Complexity: We must store the $N \times N$ matrix in memory.
\end{enumerate}

\hrulefill

\section{Softmax Attention with Topological Masking}

This is the "Previous Solution" mentioned in the script. We explicitly inject graph structure into the attention scores.

\subsection*{Formulas and Mechanism}
We calculate a topological mask $M(\mathcal{G})$ (e.g., using a power series of the adjacency matrix $W$):
\begin{equation}
    M_{\alpha}(\mathcal{G}) := \sum_{k=0}^{\infty} \alpha_{k}W^{k}
\end{equation}
We then inject this into the attention mechanism using the Hadamard (element-wise) product $\odot$:
\begin{equation}
    \text{Att}_{\text{masked}} = \text{softmax}\left( \frac{QK^\top}{\sqrt{d}} + \log(M_{\alpha}) \right) V \quad \text{or} \quad D^{-1} \left( \exp\left(\frac{QK^\top}{\sqrt{d}}\right) \odot M_{\alpha} \right) V
\end{equation}
\textbf{What it concretely does:}
\begin{itemize}
    \item It forces the attention scores to respect the graph. If nodes $i$ and $j$ are far apart, $M_{ij} \approx 0$, effectively killing the attention between them.
    \item It introduces a strong structural inductive bias.
\end{itemize}

\subsection*{Time Complexity Derivation}
Despite the benefits, the complexity remains quadratic.
\begin{enumerate}
    \item We must still compute the full $N \times N$ attention matrix $QK^\top$.
    \item We must also compute/store the dense $N \times N$ mask matrix $M$.
    \item The element-wise product is performed on $N^2$ entries.
    \item Total Complexity: $\mathcal{O}(N^2)$.
\end{enumerate}

\hrulefill

\section{Linear Attention (Vanilla)}

This is the optimization that solves the memory issue but loses the masking capability.

\subsection*{Formulas and Mechanism}
We replace the Softmax kernel with a feature map $\phi(\cdot)$ (e.g., ReLU or Random Features). This allows us to use the associativity of matrix multiplication:
\begin{equation}
    \text{Att}_{\text{linear}}(Q, K, V) = \phi(Q) \left( \phi(K)^\top V \right)
\end{equation}
\textbf{What it concretely does:}
\begin{itemize}
    \item Instead of computing $(Q \times K^\top) \times V$, it computes $Q \times (K^\top \times V)$.
    \item It avoids ever creating the $N \times N$ attention matrix.
    \item \textbf{Critique:} It effectively assumes an "all-to-all" attention (or a specific kernel approximation), but we cannot easily insert the topological mask $M$ because $(A \times B) \odot M \neq A \times (B \odot M)$.
\end{itemize}

\subsection*{Time Complexity Derivation}
\begin{enumerate}
    \item First, compute $\phi(K)^\top V$: Dimensions are $(d \times N) \times (N \times d) \rightarrow (d \times d)$. Complexity: $\mathcal{O}(N d^2)$.
    \item Second, compute $\phi(Q) \times (\text{Result})$: Dimensions are $(N \times d) \times (d \times d) \rightarrow (N \times d)$. Complexity: $\mathcal{O}(N d^2)$.
    \item Total Complexity: $\mathcal{O}(N)$ (linear with respect to sequence length $N$).
\end{enumerate}

\hrulefill

\section{Linear Attention with GRF Topological Masking}

This is the paper's main contribution. It combines the $\mathcal{O}(N)$ speed of Linear Attention with the structural bias of Topological Masking.

\subsection*{Formulas and Mechanism}
We decompose the mask $M$ into Graph Random Features (GRFs) $\hat{\phi}_{\mathcal{G}}$ via random walks, such that $M_{ij} \approx \hat{\phi}_{\mathcal{G}}(v_i)^\top \hat{\phi}_{\mathcal{G}}(v_j)$.
We merge semantic features $\phi(q)$ and graph features $\hat{\phi}_{\mathcal{G}}$ using the outer product:
\begin{equation}
    \Phi_{\text{new}}(i) = \text{vec}\left( \phi(q_i) \otimes \hat{\phi}_{\mathcal{G}}(v_i) \right)
\end{equation}
The attention becomes:
\begin{equation}
    \text{Att}_{\text{GRF}} = \Phi_{\text{new}}(Q) \left( \Phi_{\text{new}}(K)^\top V \right)
\end{equation}
\textbf{What it concretely does:}
\begin{itemize}
    \item It creates a "super-feature" that contains both the token's content and its position in the graph (via random walk collision probability).
    \item It allows us to use the linear associativity trick again, because we "baked" the mask into the features themselves.
\end{itemize}

\subsection*{Time Complexity Derivation}
At first glance, $\hat{\phi}_{\mathcal{G}}$ is size $N$, making the feature vector size $N \cdot d$, which would imply quadratic cost. However, \textbf{Theorem 3.2} saves us:
\begin{enumerate}
    \item \textbf{Sparsity:} Because random walks halt with probability $p_{\text{halt}}$, the number of non-zero entries in $\hat{\phi}_{\mathcal{G}}$ is a constant $C \ll N$.
    \item The sparse matrix multiplication cost depends on non-zero entries (NNZ).
    \item Computing $\Phi_{\text{new}}(K)^\top V$: The number of operations is proportional to $N \times C \times d$.
    \item Since $C$ and $d$ are constants relative to $N$, the complexity is $\mathcal{O}(N)$.
\end{enumerate}

\newpage

\section*{Summary of Differences}

\begin{table}[h]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{Time Complexity} & \textbf{Uses Graph Topology?} & \textbf{Mask Implementation} \\ \midrule
1. Vanilla Softmax & $\mathcal{O}(N^2)$ & No & None \\
2. Masked Softmax & $\mathcal{O}(N^2)$ & \textbf{Yes} & Hadamard Product ($\odot$) \\
3. Vanilla Linear & $\mathcal{O}(N)$ & No & None \\
4. \textbf{GRF Linear (Ours)} & \textbf{$\mathcal{O}(N)$} & \textbf{Yes} & \textbf{Feature Tensor Product ($\otimes$)} \\ \bottomrule
\end{tabular}
\caption{Comparison of attention mechanisms. The GRF method is the only one that achieves both linear complexity and topological awareness.}
\end{table}

\end{document}