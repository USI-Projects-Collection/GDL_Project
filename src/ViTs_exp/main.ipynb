{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce44722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d78610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ATTENTION MODULES ---\n",
    "\n",
    "class SoftmaxAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "        kv = k.transpose(-2, -1) @ v\n",
    "        z = 1 / (q @ k.sum(dim=-2, keepdim=True).transpose(-2, -1) + self.eps)\n",
    "        out = (q @ kv) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class GRFExactAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_patches, n_walks, p_halt, device):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "        self.register_buffer('mask', self._generate_grf_mask(num_patches, n_walks, p_halt, device))\n",
    "\n",
    "    def _generate_grf_mask(self, N, n_walks, p_halt, device):\n",
    "        side = int(np.sqrt(N))\n",
    "        G = nx.grid_2d_graph(side, side)\n",
    "        mapping = {node: i for i, node in enumerate(sorted(list(G.nodes())))}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "        mask = torch.zeros(N, N)\n",
    "        for start_node in range(N):\n",
    "            for _ in range(n_walks):\n",
    "                curr = start_node\n",
    "                while True:\n",
    "                    mask[start_node, curr] += 1.0\n",
    "                    if np.random.rand() < p_halt: break\n",
    "                    neighbors = sorted(list(G.neighbors(curr)))\n",
    "                    if not neighbors: break\n",
    "                    curr = np.random.choice(neighbors)\n",
    "            mask[start_node] /= max(n_walks, 1)\n",
    "        return mask.to(device)\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "\n",
    "        q_graph = (q.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        k_graph = (k.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        q = q + 0.1 * q_graph\n",
    "        k = k + 0.1 * k_graph\n",
    "\n",
    "        linear_kernel = q @ k.transpose(-2, -1)\n",
    "        masked_kernel = linear_kernel * self.mask.unsqueeze(0).unsqueeze(0)\n",
    "        z = 1 / (masked_kernel.sum(dim=-1, keepdim=True) + self.eps)\n",
    "        out = (masked_kernel @ v) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class MAlphaAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_patches, device, order=5, decay=0.5):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "        self.register_buffer('mask', self._generate_exact_mask(num_patches, order, decay, device))\n",
    "\n",
    "    def _generate_exact_mask(self, N, order, decay, device):\n",
    "        side = int(np.sqrt(N))\n",
    "        G = nx.grid_2d_graph(side, side)\n",
    "        mapping = {node: i for i, node in enumerate(sorted(list(G.nodes())))}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "        A = nx.to_numpy_array(G)\n",
    "        D_inv = np.diag(1.0 / np.maximum(A.sum(axis=1), 1))\n",
    "        W = D_inv @ A\n",
    "        M = np.eye(N)\n",
    "        W_k = np.eye(N)\n",
    "        coeff = 1.0\n",
    "        for _ in range(order):\n",
    "            W_k = W_k @ W\n",
    "            coeff *= decay\n",
    "            M += coeff * W_k\n",
    "        M = M / M.sum(axis=1, keepdims=True)\n",
    "        return torch.tensor(M, dtype=torch.float32).to(device)\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "        q_graph = (q.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        k_graph = (k.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        q = q + 0.1 * q_graph\n",
    "        k = k + 0.1 * k_graph\n",
    "        linear_kernel = q @ k.transpose(-2, -1)\n",
    "        masked_kernel = linear_kernel * self.mask.unsqueeze(0).unsqueeze(0)\n",
    "        z = 1 / (masked_kernel.sum(dim=-1, keepdim=True) + self.eps)\n",
    "        out = (masked_kernel @ v) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class ToeplitzAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_patches, device, decay=0.8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "        self.register_buffer('mask', self._generate_toeplitz_mask(num_patches, decay, device))\n",
    "\n",
    "    def _generate_toeplitz_mask(self, N, decay, device):\n",
    "        side = int(np.sqrt(N))\n",
    "        mask = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                xi, yi = i // side, i % side\n",
    "                xj, yj = j // side, j % side\n",
    "                dist = abs(xi - xj) + abs(yi - yj)\n",
    "                mask[i, j] = decay ** dist\n",
    "        mask = mask / mask.sum(axis=1, keepdims=True)\n",
    "        return torch.tensor(mask, dtype=torch.float32).to(device)\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "        q_graph = (q.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        k_graph = (k.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        q = q + 0.1 * q_graph\n",
    "        k = k + 0.1 * k_graph\n",
    "        linear_kernel = q @ k.transpose(-2, -1)\n",
    "        masked_kernel = linear_kernel * self.mask.unsqueeze(0).unsqueeze(0)\n",
    "        z = 1 / (masked_kernel.sum(dim=-1, keepdim=True) + self.eps)\n",
    "        out = (masked_kernel @ v) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# --- 4. MODEL ---\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size, image_size, dim, depth, num_heads, dropout, mlp_dim, device, channels=3, attention_type='softmax', n_walks=50, p_halt=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "        self.patch_embed = nn.Linear(patch_dim, dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            if attention_type == 'softmax':\n",
    "                attn = SoftmaxAttention(dim, num_heads)\n",
    "            elif attention_type == 'linear':\n",
    "                attn = LinearAttention(dim, num_heads)\n",
    "            elif attention_type == 'grf':\n",
    "                attn = GRFExactAttention(dim, num_heads, num_patches, n_walks, p_halt, device)\n",
    "            elif attention_type == 'm_alpha':\n",
    "                attn = MAlphaAttention(dim, num_heads, num_patches, device)\n",
    "            elif attention_type == 'toeplitz':\n",
    "                attn = ToeplitzAttention(dim, num_heads, num_patches, device)\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                nn.LayerNorm(dim),\n",
    "                attn,\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dim, mlp_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "                    nn.Linear(mlp_dim, dim), nn.Dropout(dropout)\n",
    "                )\n",
    "            ]))\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))\n",
    "\n",
    "    def forward(self, img):\n",
    "        p = self.patch_size\n",
    "        x = img.unfold(2, p, p).unfold(3, p, p).reshape(img.shape[0], -1, self.channels * p * p)\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, _ = x.shape\n",
    "        x += self.pos_embed[:, :N]\n",
    "        for norm1, attn, norm2, mlp in self.layers:\n",
    "            x = x + attn(norm1(x))\n",
    "            x = x + mlp(norm2(x))\n",
    "        return self.mlp_head(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce139e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_dataloaders(dataset_name, batch_size, resize_image=32, manipulate_images=False):\n",
    "    transform_compose_list = [\n",
    "            transforms.Resize((resize_image, resize_image)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    "\n",
    "    if manipulate_images:\n",
    "        transform_compose_list = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(resize_image, padding=4),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
    "        ] + transform_compose_list\n",
    "        \n",
    "    if 'mnist' in dataset_name.lower():\n",
    "        # Resize to 32x32 to match patch logic easily\n",
    "        transform_compose_list = transform_compose_list[:-1] + [transforms.Normalize((0.5,), (0.5,))]\n",
    "\n",
    "    transform = transforms.Compose(transform_compose_list)\n",
    "        \n",
    "    if dataset_name.lower() == 'cifar10':\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        channels = 3\n",
    "    elif dataset_name.lower() == 'cifar100':\n",
    "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 100\n",
    "        channels = 3\n",
    "    elif dataset_name.lower() == 'mnist':\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        channels = 1\n",
    "    elif dataset_name.lower() == 'fashionmnist':\n",
    "        trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        channels = 1\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return trainloader, testloader, num_classes, channels\n",
    "\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 15\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE, manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "def replicate_table_1_complete(dataset_name, manipulate_images=False):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"5.2 Visual transformer training on {dataset_name}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    acc_softmax = train_and_evaluate('softmax', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_toeplitz = train_and_evaluate('toeplitz', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_m_alpha = train_and_evaluate('m_alpha', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_grf = train_and_evaluate('grf', dataset_name=dataset_name, n_walks=50, p_halt=0.1, manipulate_images=manipulate_images)\n",
    "    acc_linear = train_and_evaluate('linear', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "\n",
    "    print(f\"\\nCOMPLETE RESULT - {dataset_name}\")\n",
    "    print(f\"{'Method':<25} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Unmasked Softmax':<25} {acc_softmax:<10.2f} \")\n",
    "    print(f\"{'Toeplitz-masked Linear':<25} {acc_toeplitz:<10.2f}\")\n",
    "    print(f\"{'M_alpha(G)-masked':<25} {acc_m_alpha:<10.2f} \")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'GRF-masked Linear':<25} {acc_grf:<10.2f}\")\n",
    "    print(f\"{'Unmasked Linear':<25} {acc_linear:<10.2f}\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963f7c9",
   "metadata": {},
   "source": [
    "## CIFAR10 - 15 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b285f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:03<00:00, 42.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“œ REPLICATING TABLE 1 (COMPLETE): All 5 Methods\n",
      "   Goal: Softmax > (GRF ~= M_alpha ~= Toeplitz) > Linear\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX ---\n",
      "   -> Result: Acc = 55.06%\n",
      "\n",
      "--- Training TOEPLITZ ---\n",
      "   -> Result: Acc = 55.71%\n",
      "\n",
      "--- Training M_ALPHA ---\n",
      "   -> Result: Acc = 52.21%\n",
      "\n",
      "--- Training GRF ---\n",
      "   -> Result: Acc = 55.24%\n",
      "\n",
      "--- Training LINEAR ---\n",
      "   -> Result: Acc = 54.62%\n",
      "\n",
      "[TABLE 1 COMPLETE RESULT]\n",
      "Method                    Accuracy   Type\n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          55.06      Exact Dense\n",
      "Toeplitz-masked Linear    55.71      Structure Bias\n",
      "M_alpha(G)-masked         52.21      Exact Topo\n",
      "GRF-masked Linear         55.24      Stochastic Topo\n",
      "Unmasked Linear           54.62      Baseline\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 15\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "DATASET_NAME = 'cifar10'\n",
    "\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186f22d",
   "metadata": {},
   "source": [
    "## CIFAR100 - 30 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e13f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Dataset: CIFAR100\n",
      "\n",
      "======================================================================\n",
      "ðŸ“œ REPLICATING TABLE 1 (COMPLETE) on CIFAR100\n",
      "   Goal: Softmax > (GRF ~= M_alpha ~= Toeplitz) > Linear\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.2521 | Test Acc: 10.79%\n",
      "Epoch 2/30 | Loss: 3.7075 | Test Acc: 16.11%\n",
      "Epoch 3/30 | Loss: 3.5043 | Test Acc: 17.90%\n",
      "Epoch 4/30 | Loss: 3.3753 | Test Acc: 19.46%\n",
      "Epoch 5/30 | Loss: 3.2727 | Test Acc: 21.17%\n",
      "Epoch 6/30 | Loss: 3.1827 | Test Acc: 22.13%\n",
      "Epoch 7/30 | Loss: 3.1048 | Test Acc: 23.59%\n",
      "Epoch 8/30 | Loss: 3.0445 | Test Acc: 23.02%\n",
      "Epoch 9/30 | Loss: 2.9824 | Test Acc: 24.19%\n",
      "Epoch 10/30 | Loss: 2.9311 | Test Acc: 24.77%\n",
      "Epoch 11/30 | Loss: 2.8859 | Test Acc: 25.55%\n",
      "Epoch 12/30 | Loss: 2.8359 | Test Acc: 25.60%\n",
      "Epoch 13/30 | Loss: 2.7955 | Test Acc: 26.32%\n",
      "Epoch 14/30 | Loss: 2.7549 | Test Acc: 26.43%\n",
      "Epoch 15/30 | Loss: 2.7153 | Test Acc: 26.26%\n",
      "Epoch 16/30 | Loss: 2.6819 | Test Acc: 27.20%\n",
      "Epoch 17/30 | Loss: 2.6482 | Test Acc: 27.31%\n",
      "Epoch 18/30 | Loss: 2.6161 | Test Acc: 27.53%\n",
      "Epoch 19/30 | Loss: 2.5878 | Test Acc: 27.39%\n",
      "Epoch 20/30 | Loss: 2.5604 | Test Acc: 27.53%\n",
      "Epoch 21/30 | Loss: 2.5289 | Test Acc: 28.05%\n",
      "Epoch 22/30 | Loss: 2.5030 | Test Acc: 27.43%\n",
      "Epoch 23/30 | Loss: 2.4800 | Test Acc: 28.35%\n",
      "Epoch 24/30 | Loss: 2.4512 | Test Acc: 28.79%\n",
      "Epoch 25/30 | Loss: 2.4278 | Test Acc: 28.68%\n",
      "Epoch 26/30 | Loss: 2.4059 | Test Acc: 29.11%\n",
      "Epoch 27/30 | Loss: 2.3857 | Test Acc: 29.37%\n",
      "Epoch 28/30 | Loss: 2.3610 | Test Acc: 28.81%\n",
      "Epoch 29/30 | Loss: 2.3430 | Test Acc: 28.63%\n",
      "Epoch 30/30 | Loss: 2.3251 | Test Acc: 28.68%\n",
      "   -> Final Result: Acc = 28.68%\n",
      "\n",
      "--- Training TOEPLITZ on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.2379 | Test Acc: 9.46%\n",
      "Epoch 2/30 | Loss: 3.7198 | Test Acc: 16.12%\n",
      "Epoch 3/30 | Loss: 3.4995 | Test Acc: 17.36%\n",
      "Epoch 4/30 | Loss: 3.3632 | Test Acc: 20.43%\n",
      "Epoch 5/30 | Loss: 3.2515 | Test Acc: 22.38%\n",
      "Epoch 6/30 | Loss: 3.1628 | Test Acc: 23.02%\n",
      "Epoch 7/30 | Loss: 3.0816 | Test Acc: 24.07%\n",
      "Epoch 8/30 | Loss: 3.0178 | Test Acc: 24.50%\n",
      "Epoch 9/30 | Loss: 2.9566 | Test Acc: 25.32%\n",
      "Epoch 10/30 | Loss: 2.9007 | Test Acc: 26.05%\n",
      "Epoch 11/30 | Loss: 2.8526 | Test Acc: 26.53%\n",
      "Epoch 12/30 | Loss: 2.8090 | Test Acc: 26.73%\n",
      "Epoch 13/30 | Loss: 2.7667 | Test Acc: 27.54%\n",
      "Epoch 14/30 | Loss: 2.7312 | Test Acc: 27.94%\n",
      "Epoch 15/30 | Loss: 2.6846 | Test Acc: 28.07%\n",
      "Epoch 16/30 | Loss: 2.6480 | Test Acc: 28.31%\n",
      "Epoch 17/30 | Loss: 2.6241 | Test Acc: 29.16%\n",
      "Epoch 18/30 | Loss: 2.5850 | Test Acc: 29.35%\n",
      "Epoch 19/30 | Loss: 2.5559 | Test Acc: 29.41%\n",
      "Epoch 20/30 | Loss: 2.5273 | Test Acc: 28.88%\n",
      "Epoch 21/30 | Loss: 2.4952 | Test Acc: 30.51%\n",
      "Epoch 22/30 | Loss: 2.4669 | Test Acc: 30.18%\n",
      "Epoch 23/30 | Loss: 2.4439 | Test Acc: 30.49%\n",
      "Epoch 24/30 | Loss: 2.4137 | Test Acc: 30.50%\n",
      "Epoch 25/30 | Loss: 2.3968 | Test Acc: 30.81%\n",
      "Epoch 26/30 | Loss: 2.3695 | Test Acc: 31.12%\n",
      "Epoch 27/30 | Loss: 2.3487 | Test Acc: 31.40%\n",
      "Epoch 28/30 | Loss: 2.3261 | Test Acc: 31.56%\n",
      "Epoch 29/30 | Loss: 2.3083 | Test Acc: 31.32%\n",
      "Epoch 30/30 | Loss: 2.2857 | Test Acc: 31.36%\n",
      "   -> Final Result: Acc = 31.36%\n",
      "\n",
      "--- Training M_ALPHA on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.2482 | Test Acc: 9.19%\n",
      "Epoch 2/30 | Loss: 3.7591 | Test Acc: 15.32%\n",
      "Epoch 3/30 | Loss: 3.5550 | Test Acc: 17.33%\n",
      "Epoch 4/30 | Loss: 3.4244 | Test Acc: 19.40%\n",
      "Epoch 5/30 | Loss: 3.3332 | Test Acc: 21.02%\n",
      "Epoch 6/30 | Loss: 3.2563 | Test Acc: 21.34%\n",
      "Epoch 7/30 | Loss: 3.1883 | Test Acc: 23.05%\n",
      "Epoch 8/30 | Loss: 3.1370 | Test Acc: 22.22%\n",
      "Epoch 9/30 | Loss: 3.0793 | Test Acc: 24.52%\n",
      "Epoch 10/30 | Loss: 3.0293 | Test Acc: 24.08%\n",
      "Epoch 11/30 | Loss: 2.9832 | Test Acc: 24.65%\n",
      "Epoch 12/30 | Loss: 2.9418 | Test Acc: 25.05%\n",
      "Epoch 13/30 | Loss: 2.8986 | Test Acc: 26.00%\n",
      "Epoch 14/30 | Loss: 2.8619 | Test Acc: 26.25%\n",
      "Epoch 15/30 | Loss: 2.8201 | Test Acc: 26.98%\n",
      "Epoch 16/30 | Loss: 2.7889 | Test Acc: 27.49%\n",
      "Epoch 17/30 | Loss: 2.7524 | Test Acc: 27.46%\n",
      "Epoch 18/30 | Loss: 2.7182 | Test Acc: 27.99%\n",
      "Epoch 19/30 | Loss: 2.6884 | Test Acc: 28.38%\n",
      "Epoch 20/30 | Loss: 2.6562 | Test Acc: 28.94%\n",
      "Epoch 21/30 | Loss: 2.6259 | Test Acc: 28.79%\n",
      "Epoch 22/30 | Loss: 2.6028 | Test Acc: 28.51%\n",
      "Epoch 23/30 | Loss: 2.5764 | Test Acc: 29.39%\n",
      "Epoch 24/30 | Loss: 2.5500 | Test Acc: 29.88%\n",
      "Epoch 25/30 | Loss: 2.5240 | Test Acc: 29.68%\n",
      "Epoch 26/30 | Loss: 2.5025 | Test Acc: 29.47%\n",
      "Epoch 27/30 | Loss: 2.4799 | Test Acc: 30.58%\n",
      "Epoch 28/30 | Loss: 2.4605 | Test Acc: 30.29%\n",
      "Epoch 29/30 | Loss: 2.4397 | Test Acc: 30.11%\n",
      "Epoch 30/30 | Loss: 2.4209 | Test Acc: 30.60%\n",
      "   -> Final Result: Acc = 30.60%\n",
      "\n",
      "--- Training GRF on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.2932 | Test Acc: 8.30%\n",
      "Epoch 2/30 | Loss: 3.8035 | Test Acc: 14.35%\n",
      "Epoch 3/30 | Loss: 3.5814 | Test Acc: 16.46%\n",
      "Epoch 4/30 | Loss: 3.4456 | Test Acc: 18.59%\n",
      "Epoch 5/30 | Loss: 3.3418 | Test Acc: 21.17%\n",
      "Epoch 6/30 | Loss: 3.2486 | Test Acc: 21.65%\n",
      "Epoch 7/30 | Loss: 3.1658 | Test Acc: 23.27%\n",
      "Epoch 8/30 | Loss: 3.1037 | Test Acc: 23.90%\n",
      "Epoch 9/30 | Loss: 3.0386 | Test Acc: 24.93%\n",
      "Epoch 10/30 | Loss: 2.9849 | Test Acc: 25.26%\n",
      "Epoch 11/30 | Loss: 2.9347 | Test Acc: 26.01%\n",
      "Epoch 12/30 | Loss: 2.8911 | Test Acc: 26.35%\n",
      "Epoch 13/30 | Loss: 2.8514 | Test Acc: 26.73%\n",
      "Epoch 14/30 | Loss: 2.8076 | Test Acc: 27.85%\n",
      "Epoch 15/30 | Loss: 2.7646 | Test Acc: 28.44%\n",
      "Epoch 16/30 | Loss: 2.7312 | Test Acc: 28.77%\n",
      "Epoch 17/30 | Loss: 2.6997 | Test Acc: 28.26%\n",
      "Epoch 18/30 | Loss: 2.6612 | Test Acc: 29.62%\n",
      "Epoch 19/30 | Loss: 2.6297 | Test Acc: 30.05%\n",
      "Epoch 20/30 | Loss: 2.6014 | Test Acc: 29.43%\n",
      "Epoch 21/30 | Loss: 2.5686 | Test Acc: 30.98%\n",
      "Epoch 22/30 | Loss: 2.5412 | Test Acc: 30.16%\n",
      "Epoch 23/30 | Loss: 2.5193 | Test Acc: 30.97%\n",
      "Epoch 24/30 | Loss: 2.4885 | Test Acc: 31.36%\n",
      "Epoch 25/30 | Loss: 2.4667 | Test Acc: 31.48%\n",
      "Epoch 26/30 | Loss: 2.4439 | Test Acc: 31.82%\n",
      "Epoch 27/30 | Loss: 2.4245 | Test Acc: 32.28%\n",
      "Epoch 28/30 | Loss: 2.4045 | Test Acc: 32.32%\n",
      "Epoch 29/30 | Loss: 2.3815 | Test Acc: 32.27%\n",
      "Epoch 30/30 | Loss: 2.3619 | Test Acc: 32.25%\n",
      "   -> Final Result: Acc = 32.25%\n",
      "\n",
      "--- Training LINEAR on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.2236 | Test Acc: 10.11%\n",
      "Epoch 2/30 | Loss: 3.7143 | Test Acc: 16.03%\n",
      "Epoch 3/30 | Loss: 3.4982 | Test Acc: 17.62%\n",
      "Epoch 4/30 | Loss: 3.3583 | Test Acc: 20.58%\n",
      "Epoch 5/30 | Loss: 3.2712 | Test Acc: 18.73%\n",
      "Epoch 6/30 | Loss: 3.2173 | Test Acc: 22.15%\n",
      "Epoch 7/30 | Loss: 3.1214 | Test Acc: 24.07%\n",
      "Epoch 8/30 | Loss: 3.0480 | Test Acc: 24.19%\n",
      "Epoch 9/30 | Loss: 2.9864 | Test Acc: 24.98%\n",
      "Epoch 10/30 | Loss: 2.9376 | Test Acc: 25.57%\n",
      "Epoch 11/30 | Loss: 2.8945 | Test Acc: 26.25%\n",
      "Epoch 12/30 | Loss: 2.8497 | Test Acc: 26.75%\n",
      "Epoch 13/30 | Loss: 2.8154 | Test Acc: 27.30%\n",
      "Epoch 14/30 | Loss: 2.7833 | Test Acc: 27.83%\n",
      "Epoch 15/30 | Loss: 2.7364 | Test Acc: 27.46%\n",
      "Epoch 16/30 | Loss: 2.7168 | Test Acc: 27.83%\n",
      "Epoch 17/30 | Loss: 2.6874 | Test Acc: 28.11%\n",
      "Epoch 18/30 | Loss: 2.6502 | Test Acc: 29.34%\n",
      "Epoch 19/30 | Loss: 2.6300 | Test Acc: 28.90%\n",
      "Epoch 20/30 | Loss: 2.6505 | Test Acc: 28.13%\n",
      "Epoch 21/30 | Loss: 2.5970 | Test Acc: 28.74%\n",
      "Epoch 22/30 | Loss: 2.6064 | Test Acc: 29.04%\n",
      "Epoch 23/30 | Loss: 2.5453 | Test Acc: 30.04%\n",
      "Epoch 24/30 | Loss: 2.5261 | Test Acc: 30.55%\n",
      "Epoch 25/30 | Loss: 2.5102 | Test Acc: 30.51%\n",
      "Epoch 26/30 | Loss: 2.4696 | Test Acc: 30.93%\n",
      "Epoch 27/30 | Loss: 2.4523 | Test Acc: 30.31%\n",
      "Epoch 28/30 | Loss: 2.4407 | Test Acc: 30.65%\n",
      "Epoch 29/30 | Loss: 2.4637 | Test Acc: 30.88%\n",
      "Epoch 30/30 | Loss: 2.3962 | Test Acc: 30.66%\n",
      "   -> Final Result: Acc = 30.66%\n",
      "\n",
      "[TABLE 1 COMPLETE RESULT - CIFAR100]\n",
      "Method                    Accuracy   Type\n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          28.68      Exact Dense\n",
      "Toeplitz-masked Linear    31.36      Structure Bias\n",
      "M_alpha(G)-masked         30.60      Exact Topo\n",
      "GRF-masked Linear         32.25      Stochastic Topo\n",
      "Unmasked Linear           30.66      Baseline\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 30\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = 'CIFAR100'  # Options: 'CIFAR10', 'CIFAR100'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "# TODO Rename title in output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b04ce",
   "metadata": {},
   "source": [
    "## CIFAR10 - 30 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94e673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Dataset: CIFAR10\n",
      "\n",
      "======================================================================\n",
      "ðŸ“œ REPLICATING TABLE 1 (COMPLETE) on CIFAR10\n",
      "   Goal: Softmax > (GRF ~= M_alpha ~= Toeplitz) > Linear\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.8658 | Test Acc: 41.35%\n",
      "Epoch 2/30 | Loss: 1.5644 | Test Acc: 45.68%\n",
      "Epoch 3/30 | Loss: 1.4739 | Test Acc: 48.10%\n",
      "Epoch 4/30 | Loss: 1.4151 | Test Acc: 49.58%\n",
      "Epoch 5/30 | Loss: 1.3672 | Test Acc: 50.60%\n",
      "Epoch 6/30 | Loss: 1.3245 | Test Acc: 51.40%\n",
      "Epoch 7/30 | Loss: 1.2899 | Test Acc: 52.32%\n",
      "Epoch 8/30 | Loss: 1.2563 | Test Acc: 52.89%\n",
      "Epoch 9/30 | Loss: 1.2254 | Test Acc: 52.65%\n",
      "Epoch 10/30 | Loss: 1.1943 | Test Acc: 54.11%\n",
      "Epoch 11/30 | Loss: 1.1692 | Test Acc: 53.70%\n",
      "Epoch 12/30 | Loss: 1.1387 | Test Acc: 55.77%\n",
      "Epoch 13/30 | Loss: 1.1125 | Test Acc: 54.95%\n",
      "Epoch 14/30 | Loss: 1.0911 | Test Acc: 55.80%\n",
      "Epoch 15/30 | Loss: 1.0627 | Test Acc: 55.29%\n",
      "Epoch 16/30 | Loss: 1.0433 | Test Acc: 55.57%\n",
      "Epoch 17/30 | Loss: 1.0187 | Test Acc: 56.27%\n",
      "Epoch 18/30 | Loss: 0.9997 | Test Acc: 55.98%\n",
      "Epoch 19/30 | Loss: 0.9766 | Test Acc: 55.94%\n",
      "Epoch 20/30 | Loss: 0.9597 | Test Acc: 56.46%\n",
      "Epoch 21/30 | Loss: 0.9425 | Test Acc: 56.32%\n",
      "Epoch 22/30 | Loss: 0.9226 | Test Acc: 56.38%\n",
      "Epoch 23/30 | Loss: 0.9052 | Test Acc: 56.53%\n",
      "Epoch 24/30 | Loss: 0.8879 | Test Acc: 56.35%\n",
      "Epoch 25/30 | Loss: 0.8702 | Test Acc: 56.53%\n",
      "Epoch 26/30 | Loss: 0.8569 | Test Acc: 56.06%\n",
      "Epoch 27/30 | Loss: 0.8391 | Test Acc: 56.62%\n",
      "Epoch 28/30 | Loss: 0.8294 | Test Acc: 55.63%\n",
      "Epoch 29/30 | Loss: 0.8152 | Test Acc: 56.62%\n",
      "Epoch 30/30 | Loss: 0.8002 | Test Acc: 56.97%\n",
      "   -> Final Result: Acc = 56.97%\n",
      "\n",
      "--- Training TOEPLITZ on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.8611 | Test Acc: 41.06%\n",
      "Epoch 2/30 | Loss: 1.5760 | Test Acc: 44.74%\n",
      "Epoch 3/30 | Loss: 1.4838 | Test Acc: 48.11%\n",
      "Epoch 4/30 | Loss: 1.4247 | Test Acc: 50.29%\n",
      "Epoch 5/30 | Loss: 1.3777 | Test Acc: 49.65%\n",
      "Epoch 6/30 | Loss: 1.3317 | Test Acc: 52.14%\n",
      "Epoch 7/30 | Loss: 1.2967 | Test Acc: 51.41%\n",
      "Epoch 8/30 | Loss: 1.2608 | Test Acc: 54.56%\n",
      "Epoch 9/30 | Loss: 1.2269 | Test Acc: 55.17%\n",
      "Epoch 10/30 | Loss: 1.1945 | Test Acc: 56.34%\n",
      "Epoch 11/30 | Loss: 1.1704 | Test Acc: 55.42%\n",
      "Epoch 12/30 | Loss: 1.1432 | Test Acc: 56.87%\n",
      "Epoch 13/30 | Loss: 1.1180 | Test Acc: 56.51%\n",
      "Epoch 14/30 | Loss: 1.0943 | Test Acc: 57.41%\n",
      "Epoch 15/30 | Loss: 1.0747 | Test Acc: 56.72%\n",
      "Epoch 16/30 | Loss: 1.0524 | Test Acc: 57.90%\n",
      "Epoch 17/30 | Loss: 1.0366 | Test Acc: 58.77%\n",
      "Epoch 18/30 | Loss: 1.0189 | Test Acc: 58.56%\n",
      "Epoch 19/30 | Loss: 1.0012 | Test Acc: 57.53%\n",
      "Epoch 20/30 | Loss: 0.9876 | Test Acc: 58.17%\n",
      "Epoch 21/30 | Loss: 0.9658 | Test Acc: 58.89%\n",
      "Epoch 22/30 | Loss: 0.9500 | Test Acc: 58.47%\n",
      "Epoch 23/30 | Loss: 0.9373 | Test Acc: 59.60%\n",
      "Epoch 24/30 | Loss: 0.9180 | Test Acc: 58.90%\n",
      "Epoch 25/30 | Loss: 0.8986 | Test Acc: 60.06%\n",
      "Epoch 26/30 | Loss: 0.8871 | Test Acc: 59.08%\n",
      "Epoch 27/30 | Loss: 0.8743 | Test Acc: 59.81%\n",
      "Epoch 28/30 | Loss: 0.8643 | Test Acc: 59.67%\n",
      "Epoch 29/30 | Loss: 0.8476 | Test Acc: 59.20%\n",
      "Epoch 30/30 | Loss: 0.8394 | Test Acc: 59.47%\n",
      "   -> Final Result: Acc = 59.47%\n",
      "\n",
      "--- Training M_ALPHA on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.8639 | Test Acc: 40.97%\n",
      "Epoch 2/30 | Loss: 1.5694 | Test Acc: 44.53%\n",
      "Epoch 3/30 | Loss: 1.4743 | Test Acc: 47.76%\n",
      "Epoch 4/30 | Loss: 1.4171 | Test Acc: 50.75%\n",
      "Epoch 5/30 | Loss: 1.3757 | Test Acc: 50.16%\n",
      "Epoch 6/30 | Loss: 1.3314 | Test Acc: 51.82%\n",
      "Epoch 7/30 | Loss: 1.3036 | Test Acc: 50.55%\n",
      "Epoch 8/30 | Loss: 1.2774 | Test Acc: 53.45%\n",
      "Epoch 9/30 | Loss: 1.2447 | Test Acc: 53.46%\n",
      "Epoch 10/30 | Loss: 1.2207 | Test Acc: 53.70%\n",
      "Epoch 11/30 | Loss: 1.1975 | Test Acc: 53.19%\n",
      "Epoch 12/30 | Loss: 1.1718 | Test Acc: 55.71%\n",
      "Epoch 13/30 | Loss: 1.1498 | Test Acc: 54.04%\n",
      "Epoch 14/30 | Loss: 1.1279 | Test Acc: 55.45%\n",
      "Epoch 15/30 | Loss: 1.1072 | Test Acc: 54.82%\n",
      "Epoch 16/30 | Loss: 1.0860 | Test Acc: 56.51%\n",
      "Epoch 17/30 | Loss: 1.0692 | Test Acc: 57.29%\n",
      "Epoch 18/30 | Loss: 1.0541 | Test Acc: 57.52%\n",
      "Epoch 19/30 | Loss: 1.0393 | Test Acc: 55.30%\n",
      "Epoch 20/30 | Loss: 1.0231 | Test Acc: 56.88%\n",
      "Epoch 21/30 | Loss: 1.0064 | Test Acc: 56.74%\n",
      "Epoch 22/30 | Loss: 0.9902 | Test Acc: 57.06%\n",
      "Epoch 23/30 | Loss: 0.9752 | Test Acc: 58.32%\n",
      "Epoch 24/30 | Loss: 0.9595 | Test Acc: 57.54%\n",
      "Epoch 25/30 | Loss: 0.9420 | Test Acc: 58.00%\n",
      "Epoch 26/30 | Loss: 0.9315 | Test Acc: 57.30%\n",
      "Epoch 27/30 | Loss: 0.9230 | Test Acc: 58.51%\n",
      "Epoch 28/30 | Loss: 0.9068 | Test Acc: 58.39%\n",
      "Epoch 29/30 | Loss: 0.8977 | Test Acc: 58.74%\n",
      "Epoch 30/30 | Loss: 0.8872 | Test Acc: 58.06%\n",
      "   -> Final Result: Acc = 58.06%\n",
      "\n",
      "--- Training GRF on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.9005 | Test Acc: 40.65%\n",
      "Epoch 2/30 | Loss: 1.6029 | Test Acc: 43.95%\n",
      "Epoch 3/30 | Loss: 1.5059 | Test Acc: 47.43%\n",
      "Epoch 4/30 | Loss: 1.4471 | Test Acc: 49.85%\n",
      "Epoch 5/30 | Loss: 1.4028 | Test Acc: 48.41%\n",
      "Epoch 6/30 | Loss: 1.3603 | Test Acc: 50.86%\n",
      "Epoch 7/30 | Loss: 1.3293 | Test Acc: 49.14%\n",
      "Epoch 8/30 | Loss: 1.2987 | Test Acc: 53.47%\n",
      "Epoch 9/30 | Loss: 1.2648 | Test Acc: 53.24%\n",
      "Epoch 10/30 | Loss: 1.2348 | Test Acc: 54.17%\n",
      "Epoch 11/30 | Loss: 1.2101 | Test Acc: 54.77%\n",
      "Epoch 12/30 | Loss: 1.1793 | Test Acc: 55.46%\n",
      "Epoch 13/30 | Loss: 1.1554 | Test Acc: 55.20%\n",
      "Epoch 14/30 | Loss: 1.1271 | Test Acc: 56.93%\n",
      "Epoch 15/30 | Loss: 1.1075 | Test Acc: 55.90%\n",
      "Epoch 16/30 | Loss: 1.0858 | Test Acc: 58.05%\n",
      "Epoch 17/30 | Loss: 1.0656 | Test Acc: 58.03%\n",
      "Epoch 18/30 | Loss: 1.0496 | Test Acc: 58.63%\n",
      "Epoch 19/30 | Loss: 1.0310 | Test Acc: 57.99%\n",
      "Epoch 20/30 | Loss: 1.0159 | Test Acc: 59.09%\n",
      "Epoch 21/30 | Loss: 0.9915 | Test Acc: 58.59%\n",
      "Epoch 22/30 | Loss: 0.9800 | Test Acc: 59.36%\n",
      "Epoch 23/30 | Loss: 0.9660 | Test Acc: 59.66%\n",
      "Epoch 24/30 | Loss: 0.9507 | Test Acc: 59.47%\n",
      "Epoch 25/30 | Loss: 0.9274 | Test Acc: 60.08%\n",
      "Epoch 26/30 | Loss: 0.9128 | Test Acc: 60.41%\n",
      "Epoch 27/30 | Loss: 0.9041 | Test Acc: 60.27%\n",
      "Epoch 28/30 | Loss: 0.8944 | Test Acc: 60.81%\n",
      "Epoch 29/30 | Loss: 0.8784 | Test Acc: 60.69%\n",
      "Epoch 30/30 | Loss: 0.8686 | Test Acc: 60.67%\n",
      "   -> Final Result: Acc = 60.67%\n",
      "\n",
      "--- Training LINEAR on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.8522 | Test Acc: 41.80%\n",
      "Epoch 2/30 | Loss: 1.5686 | Test Acc: 45.14%\n",
      "Epoch 3/30 | Loss: 1.4779 | Test Acc: 48.36%\n",
      "Epoch 4/30 | Loss: 1.4318 | Test Acc: 50.06%\n",
      "Epoch 5/30 | Loss: 1.3939 | Test Acc: 49.86%\n",
      "Epoch 6/30 | Loss: 1.3484 | Test Acc: 51.61%\n",
      "Epoch 7/30 | Loss: 1.3272 | Test Acc: 50.97%\n",
      "Epoch 8/30 | Loss: 1.2980 | Test Acc: 52.21%\n",
      "Epoch 9/30 | Loss: 1.2919 | Test Acc: 52.79%\n",
      "Epoch 10/30 | Loss: 1.2551 | Test Acc: 54.21%\n",
      "Epoch 11/30 | Loss: 1.2460 | Test Acc: 53.83%\n",
      "Epoch 12/30 | Loss: 1.2068 | Test Acc: 54.76%\n",
      "Epoch 13/30 | Loss: 1.1846 | Test Acc: 53.88%\n",
      "Epoch 14/30 | Loss: 1.1709 | Test Acc: 55.68%\n",
      "Epoch 15/30 | Loss: 1.1679 | Test Acc: 55.41%\n",
      "Epoch 16/30 | Loss: 1.1829 | Test Acc: 53.43%\n",
      "Epoch 17/30 | Loss: 1.1558 | Test Acc: 56.70%\n",
      "Epoch 18/30 | Loss: 1.1189 | Test Acc: 56.39%\n",
      "Epoch 19/30 | Loss: 1.0984 | Test Acc: 55.64%\n",
      "Epoch 20/30 | Loss: 1.0838 | Test Acc: 57.32%\n",
      "Epoch 21/30 | Loss: 1.0755 | Test Acc: 57.50%\n",
      "Epoch 22/30 | Loss: 1.0521 | Test Acc: 56.94%\n",
      "Epoch 23/30 | Loss: 1.0875 | Test Acc: 57.83%\n",
      "Epoch 24/30 | Loss: 1.0357 | Test Acc: 57.65%\n",
      "Epoch 25/30 | Loss: 1.0348 | Test Acc: 57.81%\n",
      "Epoch 26/30 | Loss: 1.0080 | Test Acc: 59.04%\n",
      "Epoch 27/30 | Loss: 0.9981 | Test Acc: 59.23%\n",
      "Epoch 28/30 | Loss: 0.9846 | Test Acc: 59.37%\n",
      "Epoch 29/30 | Loss: 0.9785 | Test Acc: 58.90%\n",
      "Epoch 30/30 | Loss: 0.9530 | Test Acc: 59.30%\n",
      "   -> Final Result: Acc = 59.30%\n",
      "\n",
      "[TABLE 1 COMPLETE RESULT - CIFAR10]\n",
      "Method                    Accuracy   Type\n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          56.97      Exact Dense\n",
      "Toeplitz-masked Linear    59.47      Structure Bias\n",
      "M_alpha(G)-masked         58.06      Exact Topo\n",
      "GRF-masked Linear         60.67      Stochastic Topo\n",
      "Unmasked Linear           59.30      Baseline\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 30\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = 'CIFAR10'  # Options: 'CIFAR10', 'CIFAR100'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "# TODO Rename title in output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c0ecf",
   "metadata": {},
   "source": [
    "## FashionMNIST - 10 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77d526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“œ REPLICATING TABLE 1 (COMPLETE) on FashionMNIST\n",
      "   Goal: Softmax > (GRF ~= M_alpha ~= Toeplitz) > Linear\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.7680 | Test Acc: 81.47%\n",
      "Epoch 2/10 | Loss: 0.4416 | Test Acc: 84.12%\n",
      "Epoch 3/10 | Loss: 0.3950 | Test Acc: 84.84%\n",
      "Epoch 4/10 | Loss: 0.3719 | Test Acc: 85.01%\n",
      "Epoch 5/10 | Loss: 0.3532 | Test Acc: 85.91%\n",
      "Epoch 6/10 | Loss: 0.3406 | Test Acc: 86.38%\n",
      "Epoch 7/10 | Loss: 0.3297 | Test Acc: 86.99%\n",
      "Epoch 8/10 | Loss: 0.3135 | Test Acc: 86.35%\n",
      "Epoch 9/10 | Loss: 0.3036 | Test Acc: 86.64%\n",
      "Epoch 10/10 | Loss: 0.2939 | Test Acc: 86.54%\n",
      "   -> Final Result: Acc = 86.54%\n",
      "\n",
      "--- Training TOEPLITZ on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.7523 | Test Acc: 79.02%\n",
      "Epoch 2/10 | Loss: 0.4711 | Test Acc: 83.88%\n",
      "Epoch 3/10 | Loss: 0.4150 | Test Acc: 84.11%\n",
      "Epoch 4/10 | Loss: 0.3874 | Test Acc: 84.94%\n",
      "Epoch 5/10 | Loss: 0.3630 | Test Acc: 86.31%\n",
      "Epoch 6/10 | Loss: 0.3471 | Test Acc: 86.65%\n",
      "Epoch 7/10 | Loss: 0.3329 | Test Acc: 87.53%\n",
      "Epoch 8/10 | Loss: 0.3166 | Test Acc: 87.74%\n",
      "Epoch 9/10 | Loss: 0.3076 | Test Acc: 86.89%\n",
      "Epoch 10/10 | Loss: 0.2954 | Test Acc: 88.03%\n",
      "   -> Final Result: Acc = 88.03%\n",
      "\n",
      "--- Training M_ALPHA on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.7378 | Test Acc: 81.74%\n",
      "Epoch 2/10 | Loss: 0.4457 | Test Acc: 84.17%\n",
      "Epoch 3/10 | Loss: 0.4027 | Test Acc: 85.38%\n",
      "Epoch 4/10 | Loss: 0.3748 | Test Acc: 86.18%\n",
      "Epoch 5/10 | Loss: 0.3553 | Test Acc: 86.75%\n",
      "Epoch 6/10 | Loss: 0.3447 | Test Acc: 86.56%\n",
      "Epoch 7/10 | Loss: 0.3276 | Test Acc: 87.07%\n",
      "Epoch 8/10 | Loss: 0.3178 | Test Acc: 87.56%\n",
      "Epoch 9/10 | Loss: 0.3095 | Test Acc: 87.01%\n",
      "Epoch 10/10 | Loss: 0.2972 | Test Acc: 87.21%\n",
      "   -> Final Result: Acc = 87.21%\n",
      "\n",
      "--- Training GRF on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.7980 | Test Acc: 79.34%\n",
      "Epoch 2/10 | Loss: 0.4896 | Test Acc: 83.06%\n",
      "Epoch 3/10 | Loss: 0.4295 | Test Acc: 83.62%\n",
      "Epoch 4/10 | Loss: 0.3969 | Test Acc: 84.24%\n",
      "Epoch 5/10 | Loss: 0.3680 | Test Acc: 86.26%\n",
      "Epoch 6/10 | Loss: 0.3519 | Test Acc: 86.56%\n",
      "Epoch 7/10 | Loss: 0.3350 | Test Acc: 87.35%\n",
      "Epoch 8/10 | Loss: 0.3204 | Test Acc: 87.60%\n",
      "Epoch 9/10 | Loss: 0.3130 | Test Acc: 87.14%\n",
      "Epoch 10/10 | Loss: 0.3006 | Test Acc: 86.99%\n",
      "   -> Final Result: Acc = 86.99%\n",
      "\n",
      "--- Training LINEAR on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.7491 | Test Acc: 79.58%\n",
      "Epoch 2/10 | Loss: 0.4746 | Test Acc: 83.90%\n",
      "Epoch 3/10 | Loss: 0.4311 | Test Acc: 83.58%\n",
      "Epoch 4/10 | Loss: 0.3954 | Test Acc: 84.78%\n",
      "Epoch 5/10 | Loss: 0.3721 | Test Acc: 85.87%\n",
      "Epoch 6/10 | Loss: 0.3618 | Test Acc: 85.97%\n",
      "Epoch 7/10 | Loss: 0.3525 | Test Acc: 85.99%\n",
      "Epoch 8/10 | Loss: 0.3609 | Test Acc: 86.04%\n",
      "Epoch 9/10 | Loss: 0.3377 | Test Acc: 86.44%\n",
      "Epoch 10/10 | Loss: 0.3199 | Test Acc: 86.36%\n",
      "   -> Final Result: Acc = 86.36%\n",
      "\n",
      "[TABLE 1 COMPLETE RESULT - FashionMNIST]\n",
      "Method                    Accuracy   Type\n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          86.54      Exact Dense\n",
      "Toeplitz-masked Linear    88.03      Structure Bias\n",
      "M_alpha(G)-masked         87.21      Exact Topo\n",
      "GRF-masked Linear         86.99      Stochastic Topo\n",
      "Unmasked Linear           86.36      Baseline\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = 'FashionMNIST'  # Options: 'CIFAR10', 'CIFAR100', 'FashionMNIST', 'MNIST'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "# TODO Rename title in output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c2e7f",
   "metadata": {},
   "source": [
    "## CIFAR10 - 15 EPOCHS - different parameters (batch_size, image_size, patch_size, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0cb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ðŸ“œ REPLICATING TABLE 1 (COMPLETE) on CIFAR10\n",
      "   Goal: Softmax > (GRF ~= M_alpha ~= Toeplitz) > Linear\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8122 | Test Acc: 42.97%\n",
      "Epoch 2/15 | Loss: 1.5424 | Test Acc: 46.78%\n",
      "Epoch 3/15 | Loss: 1.4487 | Test Acc: 48.50%\n",
      "Epoch 4/15 | Loss: 1.3898 | Test Acc: 48.28%\n",
      "Epoch 5/15 | Loss: 1.3388 | Test Acc: 50.47%\n",
      "Epoch 6/15 | Loss: 1.2919 | Test Acc: 51.53%\n",
      "Epoch 7/15 | Loss: 1.2522 | Test Acc: 51.05%\n",
      "Epoch 8/15 | Loss: 1.2118 | Test Acc: 52.70%\n",
      "Epoch 9/15 | Loss: 1.1751 | Test Acc: 52.38%\n",
      "Epoch 10/15 | Loss: 1.1389 | Test Acc: 52.82%\n",
      "Epoch 11/15 | Loss: 1.1018 | Test Acc: 53.00%\n",
      "Epoch 12/15 | Loss: 1.0690 | Test Acc: 54.38%\n",
      "Epoch 13/15 | Loss: 1.0312 | Test Acc: 54.34%\n",
      "Epoch 14/15 | Loss: 0.9940 | Test Acc: 54.78%\n",
      "Epoch 15/15 | Loss: 0.9608 | Test Acc: 54.49%\n",
      "   -> Final Result: Acc = 54.49%\n",
      "\n",
      "--- Training TOEPLITZ on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8271 | Test Acc: 42.75%\n",
      "Epoch 2/15 | Loss: 1.5111 | Test Acc: 48.30%\n",
      "Epoch 3/15 | Loss: 1.3978 | Test Acc: 49.93%\n",
      "Epoch 4/15 | Loss: 1.3177 | Test Acc: 52.28%\n",
      "Epoch 5/15 | Loss: 1.2544 | Test Acc: 55.04%\n",
      "Epoch 6/15 | Loss: 1.1926 | Test Acc: 55.46%\n",
      "Epoch 7/15 | Loss: 1.1465 | Test Acc: 54.63%\n",
      "Epoch 8/15 | Loss: 1.1003 | Test Acc: 55.66%\n",
      "Epoch 9/15 | Loss: 1.0605 | Test Acc: 57.52%\n",
      "Epoch 10/15 | Loss: 1.0129 | Test Acc: 58.22%\n",
      "Epoch 11/15 | Loss: 0.9741 | Test Acc: 58.46%\n",
      "Epoch 12/15 | Loss: 0.9352 | Test Acc: 58.29%\n",
      "Epoch 13/15 | Loss: 0.9057 | Test Acc: 58.46%\n",
      "Epoch 14/15 | Loss: 0.8681 | Test Acc: 57.60%\n",
      "Epoch 15/15 | Loss: 0.8457 | Test Acc: 60.27%\n",
      "   -> Final Result: Acc = 60.27%\n",
      "\n",
      "--- Training M_ALPHA on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8531 | Test Acc: 41.76%\n",
      "Epoch 2/15 | Loss: 1.5389 | Test Acc: 46.90%\n",
      "Epoch 3/15 | Loss: 1.4364 | Test Acc: 48.25%\n",
      "Epoch 4/15 | Loss: 1.3728 | Test Acc: 51.86%\n",
      "Epoch 5/15 | Loss: 1.3083 | Test Acc: 52.53%\n",
      "Epoch 6/15 | Loss: 1.2587 | Test Acc: 54.72%\n",
      "Epoch 7/15 | Loss: 1.2118 | Test Acc: 53.34%\n",
      "Epoch 8/15 | Loss: 1.1707 | Test Acc: 57.06%\n",
      "Epoch 9/15 | Loss: 1.1317 | Test Acc: 55.54%\n",
      "Epoch 10/15 | Loss: 1.0956 | Test Acc: 56.88%\n",
      "Epoch 11/15 | Loss: 1.0645 | Test Acc: 57.30%\n",
      "Epoch 12/15 | Loss: 1.0383 | Test Acc: 57.63%\n",
      "Epoch 13/15 | Loss: 1.0132 | Test Acc: 58.50%\n",
      "Epoch 14/15 | Loss: 0.9831 | Test Acc: 59.08%\n",
      "Epoch 15/15 | Loss: 0.9545 | Test Acc: 61.03%\n",
      "   -> Final Result: Acc = 61.03%\n",
      "\n",
      "--- Training GRF on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8805 | Test Acc: 40.18%\n",
      "Epoch 2/15 | Loss: 1.5597 | Test Acc: 44.59%\n",
      "Epoch 3/15 | Loss: 1.4434 | Test Acc: 48.19%\n",
      "Epoch 4/15 | Loss: 1.3694 | Test Acc: 50.91%\n",
      "Epoch 5/15 | Loss: 1.3093 | Test Acc: 52.92%\n",
      "Epoch 6/15 | Loss: 1.2699 | Test Acc: 53.21%\n",
      "Epoch 7/15 | Loss: 1.2076 | Test Acc: 51.93%\n",
      "Epoch 8/15 | Loss: 1.1678 | Test Acc: 57.63%\n",
      "Epoch 9/15 | Loss: 1.1238 | Test Acc: 56.14%\n",
      "Epoch 10/15 | Loss: 1.0868 | Test Acc: 57.33%\n",
      "Epoch 11/15 | Loss: 1.0487 | Test Acc: 58.49%\n",
      "Epoch 12/15 | Loss: 1.0191 | Test Acc: 57.22%\n",
      "Epoch 13/15 | Loss: 0.9825 | Test Acc: 58.93%\n",
      "Epoch 14/15 | Loss: 0.9588 | Test Acc: 58.11%\n",
      "Epoch 15/15 | Loss: 0.9324 | Test Acc: 60.27%\n",
      "   -> Final Result: Acc = 60.27%\n",
      "\n",
      "--- Training LINEAR on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8074 | Test Acc: 41.92%\n",
      "Epoch 2/15 | Loss: 1.4922 | Test Acc: 47.62%\n",
      "Epoch 3/15 | Loss: 1.3748 | Test Acc: 49.00%\n",
      "Epoch 4/15 | Loss: 1.2974 | Test Acc: 52.43%\n",
      "Epoch 5/15 | Loss: 1.2316 | Test Acc: 54.83%\n",
      "Epoch 6/15 | Loss: 1.1698 | Test Acc: 55.55%\n",
      "Epoch 7/15 | Loss: 1.1182 | Test Acc: 55.38%\n",
      "Epoch 8/15 | Loss: 1.0674 | Test Acc: 58.06%\n",
      "Epoch 9/15 | Loss: 1.0303 | Test Acc: 57.32%\n",
      "Epoch 10/15 | Loss: 0.9850 | Test Acc: 58.68%\n",
      "Epoch 11/15 | Loss: 0.9457 | Test Acc: 58.18%\n",
      "Epoch 12/15 | Loss: 0.9078 | Test Acc: 59.71%\n",
      "Epoch 13/15 | Loss: 0.8752 | Test Acc: 60.53%\n",
      "Epoch 14/15 | Loss: 0.8455 | Test Acc: 59.37%\n",
      "Epoch 15/15 | Loss: 0.8251 | Test Acc: 60.53%\n",
      "   -> Final Result: Acc = 60.53%\n",
      "\n",
      "[TABLE 1 COMPLETE RESULT - CIFAR10]\n",
      "Method                    Accuracy   Type\n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          54.49      Exact Dense\n",
      "Toeplitz-masked Linear    60.27      Structure Bias\n",
      "M_alpha(G)-masked         61.03      Exact Topo\n",
      "GRF-masked Linear         60.27      Stochastic Topo\n",
      "Unmasked Linear           60.53      Baseline\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 15\n",
    "IMAGE_SIZE = 42\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 4\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "N_WALKS = 100\n",
    "P_HALT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = \"CIFAR10\"  # Options: 'CIFAR10', 'CIFAR100', 'FashionMNIST', 'MNIST'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(\n",
    "    model_type, dataset_name=\"cifar10\", n_walks=50, p_halt=0.1, manipulate_images=False\n",
    "):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(\n",
    "        dataset_name,\n",
    "        BATCH_SIZE,\n",
    "        manipulate_images=manipulate_images,\n",
    "        resize_image=IMAGE_SIZE,\n",
    "    )\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc  # Store last accuracy\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "\n",
    "# TODO Rename title in output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME, N_WALKS, P_HALT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topo_masking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
