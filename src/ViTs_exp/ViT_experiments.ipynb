{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T22:07:02.830647Z",
     "iopub.status.busy": "2025-12-10T22:07:02.830375Z",
     "iopub.status.idle": "2025-12-10T22:07:12.332482Z",
     "shell.execute_reply": "2025-12-10T22:07:12.331940Z",
     "shell.execute_reply.started": "2025-12-10T22:07:02.830627Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T22:07:13.965098Z",
     "iopub.status.busy": "2025-12-10T22:07:13.964222Z",
     "iopub.status.idle": "2025-12-10T22:07:13.996786Z",
     "shell.execute_reply": "2025-12-10T22:07:13.995941Z",
     "shell.execute_reply.started": "2025-12-10T22:07:13.965069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- ATTENTION MODULES ---\n",
    "\n",
    "class SoftmaxAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "        kv = k.transpose(-2, -1) @ v\n",
    "        z = 1 / (q @ k.sum(dim=-2, keepdim=True).transpose(-2, -1) + self.eps)\n",
    "        out = (q @ kv) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class GRFExactAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_patches, n_walks, p_halt, device):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "        self.register_buffer('mask', self._generate_grf_mask(num_patches, n_walks, p_halt, device))\n",
    "\n",
    "    def _generate_grf_mask(self, N, n_walks, p_halt, device):\n",
    "        side = int(np.sqrt(N))\n",
    "        G = nx.grid_2d_graph(side, side)\n",
    "        mapping = {node: i for i, node in enumerate(sorted(list(G.nodes())))}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "        mask = torch.zeros(N, N)\n",
    "        for start_node in range(N):\n",
    "            for _ in range(n_walks):\n",
    "                curr = start_node\n",
    "                while True:\n",
    "                    mask[start_node, curr] += 1.0\n",
    "                    if np.random.rand() < p_halt: break\n",
    "                    neighbors = sorted(list(G.neighbors(curr)))\n",
    "                    if not neighbors: break\n",
    "                    curr = np.random.choice(neighbors)\n",
    "            mask[start_node] /= max(n_walks, 1)\n",
    "        return mask.to(device)\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "\n",
    "        q_graph = (q.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        k_graph = (k.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        q = q + 0.1 * q_graph\n",
    "        k = k + 0.1 * k_graph\n",
    "\n",
    "        linear_kernel = q @ k.transpose(-2, -1)\n",
    "        masked_kernel = linear_kernel * self.mask.unsqueeze(0).unsqueeze(0)\n",
    "        z = 1 / (masked_kernel.sum(dim=-1, keepdim=True) + self.eps)\n",
    "        out = (masked_kernel @ v) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class MAlphaAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_patches, device, order=5, decay=0.5):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "        self.register_buffer('mask', self._generate_exact_mask(num_patches, order, decay, device))\n",
    "\n",
    "    def _generate_exact_mask(self, N, order, decay, device):\n",
    "        side = int(np.sqrt(N))\n",
    "        G = nx.grid_2d_graph(side, side)\n",
    "        mapping = {node: i for i, node in enumerate(sorted(list(G.nodes())))}\n",
    "        G = nx.relabel_nodes(G, mapping)\n",
    "        A = nx.to_numpy_array(G)\n",
    "        D_inv = np.diag(1.0 / np.maximum(A.sum(axis=1), 1))\n",
    "        W = D_inv @ A\n",
    "        M = np.eye(N)\n",
    "        W_k = np.eye(N)\n",
    "        coeff = 1.0\n",
    "        for _ in range(order):\n",
    "            W_k = W_k @ W\n",
    "            coeff *= decay\n",
    "            M += coeff * W_k\n",
    "        M = M / M.sum(axis=1, keepdims=True)\n",
    "        return torch.tensor(M, dtype=torch.float32).to(device)\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "        q_graph = (q.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        k_graph = (k.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        q = q + 0.1 * q_graph\n",
    "        k = k + 0.1 * k_graph\n",
    "        linear_kernel = q @ k.transpose(-2, -1)\n",
    "        masked_kernel = linear_kernel * self.mask.unsqueeze(0).unsqueeze(0)\n",
    "        z = 1 / (masked_kernel.sum(dim=-1, keepdim=True) + self.eps)\n",
    "        out = (masked_kernel @ v) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class ToeplitzAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_patches, device, decay=0.8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        self.eps = 1e-6\n",
    "        self.register_buffer('mask', self._generate_toeplitz_mask(num_patches, decay, device))\n",
    "\n",
    "    def _generate_toeplitz_mask(self, N, decay, device):\n",
    "        side = int(np.sqrt(N))\n",
    "        mask = np.zeros((N, N))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                xi, yi = i // side, i % side\n",
    "                xj, yj = j // side, j % side\n",
    "                dist = abs(xi - xj) + abs(yi - yj)\n",
    "                mask[i, j] = decay ** dist\n",
    "        mask = mask / mask.sum(axis=1, keepdims=True)\n",
    "        return torch.tensor(mask, dtype=torch.float32).to(device)\n",
    "\n",
    "    def feature_map(self, x):\n",
    "        return torch.nn.functional.relu(x) + self.eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.reshape(B, N, self.num_heads, -1).transpose(1, 2), qkv)\n",
    "        q = self.feature_map(q)\n",
    "        k = self.feature_map(k)\n",
    "        q_graph = (q.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        k_graph = (k.transpose(-2, -1) @ self.mask).transpose(-2, -1)\n",
    "        q = q + 0.1 * q_graph\n",
    "        k = k + 0.1 * k_graph\n",
    "        linear_kernel = q @ k.transpose(-2, -1)\n",
    "        masked_kernel = linear_kernel * self.mask.unsqueeze(0).unsqueeze(0)\n",
    "        z = 1 / (masked_kernel.sum(dim=-1, keepdim=True) + self.eps)\n",
    "        out = (masked_kernel @ v) * z\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        return self.to_out(out)\n",
    "\n",
    "# --- 4. MODEL ---\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, patch_size, image_size, dim, depth, num_heads, dropout, mlp_dim, device, channels=3, attention_type='softmax', n_walks=50, p_halt=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "        self.patch_embed = nn.Linear(patch_dim, dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            if attention_type == 'softmax':\n",
    "                attn = SoftmaxAttention(dim, num_heads)\n",
    "            elif attention_type == 'linear':\n",
    "                attn = LinearAttention(dim, num_heads)\n",
    "            elif attention_type == 'grf':\n",
    "                attn = GRFExactAttention(dim, num_heads, num_patches, n_walks, p_halt, device)\n",
    "            elif attention_type == 'm_alpha':\n",
    "                attn = MAlphaAttention(dim, num_heads, num_patches, device)\n",
    "            elif attention_type == 'toeplitz':\n",
    "                attn = ToeplitzAttention(dim, num_heads, num_patches, device)\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                nn.LayerNorm(dim),\n",
    "                attn,\n",
    "                nn.LayerNorm(dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(dim, mlp_dim), nn.GELU(), nn.Dropout(dropout),\n",
    "                    nn.Linear(mlp_dim, dim), nn.Dropout(dropout)\n",
    "                )\n",
    "            ]))\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))\n",
    "\n",
    "    def forward(self, img):\n",
    "        p = self.patch_size\n",
    "        x = img.unfold(2, p, p).unfold(3, p, p).reshape(img.shape[0], -1, self.channels * p * p)\n",
    "        x = self.patch_embed(x)\n",
    "        B, N, _ = x.shape\n",
    "        x += self.pos_embed[:, :N]\n",
    "        for norm1, attn, norm2, mlp in self.layers:\n",
    "            x = x + attn(norm1(x))\n",
    "            x = x + mlp(norm2(x))\n",
    "        return self.mlp_head(x.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T22:07:20.505690Z",
     "iopub.status.busy": "2025-12-10T22:07:20.505061Z",
     "iopub.status.idle": "2025-12-10T22:07:20.577473Z",
     "shell.execute_reply": "2025-12-10T22:07:20.576699Z",
     "shell.execute_reply.started": "2025-12-10T22:07:20.505653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "def get_dataloaders(dataset_name, batch_size, resize_image=32, manipulate_images=False):\n",
    "    transform_compose_list = [\n",
    "            transforms.Resize((resize_image, resize_image)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    "\n",
    "    if manipulate_images:\n",
    "        transform_compose_list = [\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(resize_image, padding=4),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2)\n",
    "        ] + transform_compose_list\n",
    "        \n",
    "    if 'mnist' in dataset_name.lower():\n",
    "        # Resize to 32x32 to match patch logic easily\n",
    "        transform_compose_list = transform_compose_list[:-1] + [transforms.Normalize((0.5,), (0.5,))]\n",
    "\n",
    "    transform = transforms.Compose(transform_compose_list)\n",
    "        \n",
    "    if dataset_name.lower() == 'cifar10':\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        channels = 3\n",
    "    elif dataset_name.lower() == 'cifar100':\n",
    "        trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 100\n",
    "        channels = 3\n",
    "    elif dataset_name.lower() == 'mnist':\n",
    "        trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        channels = 1\n",
    "    elif dataset_name.lower() == 'fashionmnist':\n",
    "        trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "        num_classes = 10\n",
    "        channels = 1\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return trainloader, testloader, num_classes, channels\n",
    "\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 15\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE, manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "def replicate_table_1_complete(dataset_name, manipulate_images=False):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"5.2 Visual transformer training on {dataset_name}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    acc_softmax = train_and_evaluate('softmax', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_toeplitz = train_and_evaluate('toeplitz', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_m_alpha = train_and_evaluate('m_alpha', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_grf = train_and_evaluate('grf', dataset_name=dataset_name, n_walks=50, p_halt=0.1, manipulate_images=manipulate_images)\n",
    "    acc_linear = train_and_evaluate('linear', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "\n",
    "    print(f\"\\nCOMPLETE RESULT - {dataset_name}\")\n",
    "    print(f\"{'Method':<25} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Unmasked Softmax':<25} {acc_softmax:<10.2f} \")\n",
    "    print(f\"{'Toeplitz-masked Linear':<25} {acc_toeplitz:<10.2f}\")\n",
    "    print(f\"{'M_alpha(G)-masked':<25} {acc_m_alpha:<10.2f} \")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'GRF-masked Linear':<25} {acc_grf:<10.2f}\")\n",
    "    print(f\"{'Unmasked Linear':<25} {acc_linear:<10.2f}\")\n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 - 15 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "5.2 Visual transformer training on cifar10\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on CIFAR10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:04<00:00, 42.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 1.8666 | Test Acc: 41.48%\n",
      "Epoch 2/15 | Loss: 1.5630 | Test Acc: 45.51%\n",
      "Epoch 3/15 | Loss: 1.4720 | Test Acc: 47.96%\n",
      "Epoch 4/15 | Loss: 1.4135 | Test Acc: 49.56%\n",
      "Epoch 5/15 | Loss: 1.3679 | Test Acc: 50.59%\n",
      "Epoch 6/15 | Loss: 1.3241 | Test Acc: 51.24%\n",
      "Epoch 7/15 | Loss: 1.2897 | Test Acc: 52.09%\n",
      "Epoch 8/15 | Loss: 1.2538 | Test Acc: 53.14%\n",
      "Epoch 9/15 | Loss: 1.2244 | Test Acc: 52.59%\n",
      "Epoch 10/15 | Loss: 1.1910 | Test Acc: 53.89%\n",
      "Epoch 11/15 | Loss: 1.1664 | Test Acc: 53.69%\n",
      "Epoch 12/15 | Loss: 1.1357 | Test Acc: 55.49%\n",
      "Epoch 13/15 | Loss: 1.1118 | Test Acc: 55.40%\n",
      "Epoch 14/15 | Loss: 1.0927 | Test Acc: 55.33%\n",
      "Epoch 15/15 | Loss: 1.0623 | Test Acc: 56.11%\n",
      "   -> Final Result: Acc = 56.11%\n",
      "\n",
      "--- Training TOEPLITZ on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8765 | Test Acc: 41.40%\n",
      "Epoch 2/15 | Loss: 1.5699 | Test Acc: 43.49%\n",
      "Epoch 3/15 | Loss: 1.4728 | Test Acc: 47.59%\n",
      "Epoch 4/15 | Loss: 1.4046 | Test Acc: 49.48%\n",
      "Epoch 5/15 | Loss: 1.3598 | Test Acc: 51.16%\n",
      "Epoch 6/15 | Loss: 1.3149 | Test Acc: 52.43%\n",
      "Epoch 7/15 | Loss: 1.2782 | Test Acc: 53.04%\n",
      "Epoch 8/15 | Loss: 1.2457 | Test Acc: 52.85%\n",
      "Epoch 9/15 | Loss: 1.2171 | Test Acc: 55.11%\n",
      "Epoch 10/15 | Loss: 1.1865 | Test Acc: 55.08%\n",
      "Epoch 11/15 | Loss: 1.1586 | Test Acc: 55.89%\n",
      "Epoch 12/15 | Loss: 1.1349 | Test Acc: 56.09%\n",
      "Epoch 13/15 | Loss: 1.1062 | Test Acc: 56.78%\n",
      "Epoch 14/15 | Loss: 1.0830 | Test Acc: 56.34%\n",
      "Epoch 15/15 | Loss: 1.0600 | Test Acc: 57.25%\n",
      "   -> Final Result: Acc = 57.25%\n",
      "\n",
      "--- Training M_ALPHA on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8690 | Test Acc: 39.05%\n",
      "Epoch 2/15 | Loss: 1.5756 | Test Acc: 45.79%\n",
      "Epoch 3/15 | Loss: 1.4786 | Test Acc: 48.73%\n",
      "Epoch 4/15 | Loss: 1.4204 | Test Acc: 48.45%\n",
      "Epoch 5/15 | Loss: 1.3719 | Test Acc: 50.95%\n",
      "Epoch 6/15 | Loss: 1.3311 | Test Acc: 51.40%\n",
      "Epoch 7/15 | Loss: 1.2963 | Test Acc: 51.70%\n",
      "Epoch 8/15 | Loss: 1.2662 | Test Acc: 53.63%\n",
      "Epoch 9/15 | Loss: 1.2373 | Test Acc: 53.77%\n",
      "Epoch 10/15 | Loss: 1.2110 | Test Acc: 54.71%\n",
      "Epoch 11/15 | Loss: 1.1833 | Test Acc: 54.09%\n",
      "Epoch 12/15 | Loss: 1.1615 | Test Acc: 55.29%\n",
      "Epoch 13/15 | Loss: 1.1350 | Test Acc: 55.65%\n",
      "Epoch 14/15 | Loss: 1.1150 | Test Acc: 55.12%\n",
      "Epoch 15/15 | Loss: 1.0966 | Test Acc: 56.83%\n",
      "   -> Final Result: Acc = 56.83%\n",
      "\n",
      "--- Training GRF on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.9163 | Test Acc: 38.71%\n",
      "Epoch 2/15 | Loss: 1.6101 | Test Acc: 44.44%\n",
      "Epoch 3/15 | Loss: 1.5097 | Test Acc: 46.58%\n",
      "Epoch 4/15 | Loss: 1.4493 | Test Acc: 48.66%\n",
      "Epoch 5/15 | Loss: 1.4019 | Test Acc: 49.39%\n",
      "Epoch 6/15 | Loss: 1.3615 | Test Acc: 50.74%\n",
      "Epoch 7/15 | Loss: 1.3279 | Test Acc: 52.25%\n",
      "Epoch 8/15 | Loss: 1.2947 | Test Acc: 53.00%\n",
      "Epoch 9/15 | Loss: 1.2670 | Test Acc: 53.11%\n",
      "Epoch 10/15 | Loss: 1.2387 | Test Acc: 55.10%\n",
      "Epoch 11/15 | Loss: 1.2135 | Test Acc: 55.79%\n",
      "Epoch 12/15 | Loss: 1.1907 | Test Acc: 55.99%\n",
      "Epoch 13/15 | Loss: 1.1592 | Test Acc: 55.40%\n",
      "Epoch 14/15 | Loss: 1.1425 | Test Acc: 53.61%\n",
      "Epoch 15/15 | Loss: 1.1189 | Test Acc: 56.69%\n",
      "   -> Final Result: Acc = 56.69%\n",
      "\n",
      "--- Training LINEAR on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8475 | Test Acc: 41.21%\n",
      "Epoch 2/15 | Loss: 1.5659 | Test Acc: 45.04%\n",
      "Epoch 3/15 | Loss: 1.4703 | Test Acc: 47.87%\n",
      "Epoch 4/15 | Loss: 1.4303 | Test Acc: 48.29%\n",
      "Epoch 5/15 | Loss: 1.3871 | Test Acc: 49.17%\n",
      "Epoch 6/15 | Loss: 1.3472 | Test Acc: 49.82%\n",
      "Epoch 7/15 | Loss: 1.3398 | Test Acc: 49.62%\n",
      "Epoch 8/15 | Loss: 1.3103 | Test Acc: 52.27%\n",
      "Epoch 9/15 | Loss: 1.2798 | Test Acc: 53.74%\n",
      "Epoch 10/15 | Loss: 1.2483 | Test Acc: 54.55%\n",
      "Epoch 11/15 | Loss: 1.2169 | Test Acc: 54.32%\n",
      "Epoch 12/15 | Loss: 1.2672 | Test Acc: 53.71%\n",
      "Epoch 13/15 | Loss: 1.2133 | Test Acc: 55.85%\n",
      "Epoch 14/15 | Loss: 1.1847 | Test Acc: 55.38%\n",
      "Epoch 15/15 | Loss: 1.1581 | Test Acc: 56.69%\n",
      "   -> Final Result: Acc = 56.69%\n",
      "\n",
      "COMPLETE RESULT - cifar10\n",
      "Method                    Accuracy  \n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          56.11      \n",
      "Toeplitz-masked Linear    57.25     \n",
      "M_alpha(G)-masked         56.83      \n",
      "--------------------------------------------------\n",
      "GRF-masked Linear         56.69     \n",
      "Unmasked Linear           56.69     \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 15\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "DATASET_NAME = 'cifar10'\n",
    "\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR100 - 30 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Dataset: CIFAR100\n",
      "\n",
      "======================================================================\n",
      "5.2 Visual transformer training on CIFAR100\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on CIFAR100 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:02<00:00, 76.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Loss: 4.2431 | Test Acc: 10.79%\n",
      "Epoch 2/30 | Loss: 3.6941 | Test Acc: 15.60%\n",
      "Epoch 3/30 | Loss: 3.5025 | Test Acc: 18.18%\n",
      "Epoch 4/30 | Loss: 3.3678 | Test Acc: 19.69%\n",
      "Epoch 5/30 | Loss: 3.2686 | Test Acc: 20.41%\n",
      "Epoch 6/30 | Loss: 3.1789 | Test Acc: 22.40%\n",
      "Epoch 7/30 | Loss: 3.1008 | Test Acc: 23.15%\n",
      "Epoch 8/30 | Loss: 3.0389 | Test Acc: 23.65%\n",
      "Epoch 9/30 | Loss: 2.9835 | Test Acc: 23.89%\n",
      "Epoch 10/30 | Loss: 2.9298 | Test Acc: 25.24%\n",
      "Epoch 11/30 | Loss: 2.8821 | Test Acc: 26.12%\n",
      "Epoch 12/30 | Loss: 2.8379 | Test Acc: 26.12%\n",
      "Epoch 13/30 | Loss: 2.8026 | Test Acc: 25.98%\n",
      "Epoch 14/30 | Loss: 2.7620 | Test Acc: 26.47%\n",
      "Epoch 15/30 | Loss: 2.7242 | Test Acc: 26.71%\n",
      "Epoch 16/30 | Loss: 2.6934 | Test Acc: 27.37%\n",
      "Epoch 17/30 | Loss: 2.6640 | Test Acc: 27.02%\n",
      "Epoch 18/30 | Loss: 2.6305 | Test Acc: 27.32%\n",
      "Epoch 19/30 | Loss: 2.6065 | Test Acc: 27.49%\n",
      "Epoch 20/30 | Loss: 2.5741 | Test Acc: 27.65%\n",
      "Epoch 21/30 | Loss: 2.5537 | Test Acc: 28.24%\n",
      "Epoch 22/30 | Loss: 2.5239 | Test Acc: 28.15%\n",
      "Epoch 23/30 | Loss: 2.5043 | Test Acc: 27.79%\n",
      "Epoch 24/30 | Loss: 2.4780 | Test Acc: 27.96%\n",
      "Epoch 25/30 | Loss: 2.4510 | Test Acc: 28.10%\n",
      "Epoch 26/30 | Loss: 2.4316 | Test Acc: 28.32%\n",
      "Epoch 27/30 | Loss: 2.4191 | Test Acc: 28.27%\n",
      "Epoch 28/30 | Loss: 2.3918 | Test Acc: 28.30%\n",
      "Epoch 29/30 | Loss: 2.3741 | Test Acc: 29.16%\n",
      "Epoch 30/30 | Loss: 2.3506 | Test Acc: 28.67%\n",
      "   -> Final Result: Acc = 28.67%\n",
      "\n",
      "--- Training TOEPLITZ on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.2541 | Test Acc: 10.32%\n",
      "Epoch 2/30 | Loss: 3.7514 | Test Acc: 14.29%\n",
      "Epoch 3/30 | Loss: 3.5277 | Test Acc: 17.75%\n",
      "Epoch 4/30 | Loss: 3.3684 | Test Acc: 19.73%\n",
      "Epoch 5/30 | Loss: 3.2489 | Test Acc: 21.28%\n",
      "Epoch 6/30 | Loss: 3.1526 | Test Acc: 22.93%\n",
      "Epoch 7/30 | Loss: 3.0770 | Test Acc: 23.92%\n",
      "Epoch 8/30 | Loss: 3.0039 | Test Acc: 25.15%\n",
      "Epoch 9/30 | Loss: 2.9447 | Test Acc: 25.49%\n",
      "Epoch 10/30 | Loss: 2.8889 | Test Acc: 26.34%\n",
      "Epoch 11/30 | Loss: 2.8374 | Test Acc: 27.39%\n",
      "Epoch 12/30 | Loss: 2.7900 | Test Acc: 27.00%\n",
      "Epoch 13/30 | Loss: 2.7465 | Test Acc: 27.68%\n",
      "Epoch 14/30 | Loss: 2.7017 | Test Acc: 28.04%\n",
      "Epoch 15/30 | Loss: 2.6632 | Test Acc: 28.18%\n",
      "Epoch 16/30 | Loss: 2.6276 | Test Acc: 29.36%\n",
      "Epoch 17/30 | Loss: 2.5913 | Test Acc: 28.95%\n",
      "Epoch 18/30 | Loss: 2.5644 | Test Acc: 30.03%\n",
      "Epoch 19/30 | Loss: 2.5254 | Test Acc: 29.79%\n",
      "Epoch 20/30 | Loss: 2.4936 | Test Acc: 30.24%\n",
      "Epoch 21/30 | Loss: 2.4692 | Test Acc: 30.80%\n",
      "Epoch 22/30 | Loss: 2.4399 | Test Acc: 30.49%\n",
      "Epoch 23/30 | Loss: 2.4165 | Test Acc: 31.12%\n",
      "Epoch 24/30 | Loss: 2.3862 | Test Acc: 30.63%\n",
      "Epoch 25/30 | Loss: 2.3689 | Test Acc: 31.48%\n",
      "Epoch 26/30 | Loss: 2.3457 | Test Acc: 30.88%\n",
      "Epoch 27/30 | Loss: 2.3266 | Test Acc: 31.14%\n",
      "Epoch 28/30 | Loss: 2.3020 | Test Acc: 31.47%\n",
      "Epoch 29/30 | Loss: 2.2860 | Test Acc: 31.95%\n",
      "Epoch 30/30 | Loss: 2.2626 | Test Acc: 31.80%\n",
      "   -> Final Result: Acc = 31.80%\n",
      "\n",
      "--- Training M_ALPHA on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.2556 | Test Acc: 9.83%\n",
      "Epoch 2/30 | Loss: 3.7532 | Test Acc: 14.88%\n",
      "Epoch 3/30 | Loss: 3.5475 | Test Acc: 16.92%\n",
      "Epoch 4/30 | Loss: 3.4160 | Test Acc: 19.52%\n",
      "Epoch 5/30 | Loss: 3.3165 | Test Acc: 20.76%\n",
      "Epoch 6/30 | Loss: 3.2397 | Test Acc: 21.56%\n",
      "Epoch 7/30 | Loss: 3.1810 | Test Acc: 22.45%\n",
      "Epoch 8/30 | Loss: 3.1215 | Test Acc: 23.53%\n",
      "Epoch 9/30 | Loss: 3.0718 | Test Acc: 23.95%\n",
      "Epoch 10/30 | Loss: 3.0251 | Test Acc: 24.77%\n",
      "Epoch 11/30 | Loss: 2.9783 | Test Acc: 25.64%\n",
      "Epoch 12/30 | Loss: 2.9351 | Test Acc: 25.44%\n",
      "Epoch 13/30 | Loss: 2.8988 | Test Acc: 25.24%\n",
      "Epoch 14/30 | Loss: 2.8613 | Test Acc: 25.99%\n",
      "Epoch 15/30 | Loss: 2.8281 | Test Acc: 26.54%\n",
      "Epoch 16/30 | Loss: 2.7951 | Test Acc: 26.51%\n",
      "Epoch 17/30 | Loss: 2.7644 | Test Acc: 26.97%\n",
      "Epoch 18/30 | Loss: 2.7287 | Test Acc: 27.60%\n",
      "Epoch 19/30 | Loss: 2.7006 | Test Acc: 27.80%\n",
      "Epoch 20/30 | Loss: 2.6723 | Test Acc: 27.38%\n",
      "Epoch 21/30 | Loss: 2.6419 | Test Acc: 28.21%\n",
      "Epoch 22/30 | Loss: 2.6160 | Test Acc: 28.38%\n",
      "Epoch 23/30 | Loss: 2.5896 | Test Acc: 29.06%\n",
      "Epoch 24/30 | Loss: 2.5640 | Test Acc: 29.04%\n",
      "Epoch 25/30 | Loss: 2.5433 | Test Acc: 29.34%\n",
      "Epoch 26/30 | Loss: 2.5140 | Test Acc: 29.05%\n",
      "Epoch 27/30 | Loss: 2.4984 | Test Acc: 30.24%\n",
      "Epoch 28/30 | Loss: 2.4736 | Test Acc: 29.55%\n",
      "Epoch 29/30 | Loss: 2.4554 | Test Acc: 30.23%\n",
      "Epoch 30/30 | Loss: 2.4309 | Test Acc: 30.11%\n",
      "   -> Final Result: Acc = 30.11%\n",
      "\n",
      "--- Training GRF on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.3080 | Test Acc: 7.52%\n",
      "Epoch 2/30 | Loss: 3.8322 | Test Acc: 13.58%\n",
      "Epoch 3/30 | Loss: 3.6015 | Test Acc: 15.72%\n",
      "Epoch 4/30 | Loss: 3.4697 | Test Acc: 18.40%\n",
      "Epoch 5/30 | Loss: 3.3643 | Test Acc: 19.64%\n",
      "Epoch 6/30 | Loss: 3.2747 | Test Acc: 20.19%\n",
      "Epoch 7/30 | Loss: 3.1916 | Test Acc: 21.58%\n",
      "Epoch 8/30 | Loss: 3.1209 | Test Acc: 23.17%\n",
      "Epoch 9/30 | Loss: 3.0550 | Test Acc: 23.61%\n",
      "Epoch 10/30 | Loss: 2.9980 | Test Acc: 25.36%\n",
      "Epoch 11/30 | Loss: 2.9441 | Test Acc: 25.36%\n",
      "Epoch 12/30 | Loss: 2.9029 | Test Acc: 26.35%\n",
      "Epoch 13/30 | Loss: 2.8579 | Test Acc: 26.60%\n",
      "Epoch 14/30 | Loss: 2.8248 | Test Acc: 28.03%\n",
      "Epoch 15/30 | Loss: 2.7821 | Test Acc: 28.56%\n",
      "Epoch 16/30 | Loss: 2.7479 | Test Acc: 28.38%\n",
      "Epoch 17/30 | Loss: 2.7183 | Test Acc: 28.37%\n",
      "Epoch 18/30 | Loss: 2.6831 | Test Acc: 29.18%\n",
      "Epoch 19/30 | Loss: 2.6491 | Test Acc: 30.15%\n",
      "Epoch 20/30 | Loss: 2.6236 | Test Acc: 29.82%\n",
      "Epoch 21/30 | Loss: 2.5953 | Test Acc: 30.19%\n",
      "Epoch 22/30 | Loss: 2.5619 | Test Acc: 30.39%\n",
      "Epoch 23/30 | Loss: 2.5472 | Test Acc: 30.36%\n",
      "Epoch 24/30 | Loss: 2.5166 | Test Acc: 31.07%\n",
      "Epoch 25/30 | Loss: 2.4934 | Test Acc: 30.25%\n",
      "Epoch 26/30 | Loss: 2.4736 | Test Acc: 31.06%\n",
      "Epoch 27/30 | Loss: 2.4402 | Test Acc: 30.93%\n",
      "Epoch 28/30 | Loss: 2.4277 | Test Acc: 31.78%\n",
      "Epoch 29/30 | Loss: 2.4062 | Test Acc: 31.67%\n",
      "Epoch 30/30 | Loss: 2.3843 | Test Acc: 31.99%\n",
      "   -> Final Result: Acc = 31.99%\n",
      "\n",
      "--- Training LINEAR on CIFAR100 ---\n",
      "Epoch 1/30 | Loss: 4.1890 | Test Acc: 11.29%\n",
      "Epoch 2/30 | Loss: 3.6814 | Test Acc: 15.52%\n",
      "Epoch 3/30 | Loss: 3.4685 | Test Acc: 17.57%\n",
      "Epoch 4/30 | Loss: 3.3369 | Test Acc: 20.12%\n",
      "Epoch 5/30 | Loss: 3.2312 | Test Acc: 21.53%\n",
      "Epoch 6/30 | Loss: 3.1514 | Test Acc: 23.32%\n",
      "Epoch 7/30 | Loss: 3.0803 | Test Acc: 23.89%\n",
      "Epoch 8/30 | Loss: 3.0076 | Test Acc: 24.55%\n",
      "Epoch 9/30 | Loss: 2.9538 | Test Acc: 25.76%\n",
      "Epoch 10/30 | Loss: 2.9052 | Test Acc: 26.13%\n",
      "Epoch 11/30 | Loss: 2.9200 | Test Acc: 24.90%\n",
      "Epoch 12/30 | Loss: 2.8527 | Test Acc: 26.80%\n",
      "Epoch 13/30 | Loss: 2.7931 | Test Acc: 27.24%\n",
      "Epoch 14/30 | Loss: 2.7536 | Test Acc: 27.84%\n",
      "Epoch 15/30 | Loss: 2.7159 | Test Acc: 28.41%\n",
      "Epoch 16/30 | Loss: 2.6892 | Test Acc: 27.98%\n",
      "Epoch 17/30 | Loss: 2.6564 | Test Acc: 29.50%\n",
      "Epoch 18/30 | Loss: 2.6648 | Test Acc: 28.52%\n",
      "Epoch 19/30 | Loss: 2.6149 | Test Acc: 29.75%\n",
      "Epoch 20/30 | Loss: 2.5966 | Test Acc: 29.24%\n",
      "Epoch 21/30 | Loss: 2.5583 | Test Acc: 29.65%\n",
      "Epoch 22/30 | Loss: 2.5304 | Test Acc: 30.20%\n",
      "Epoch 23/30 | Loss: 2.5097 | Test Acc: 30.73%\n",
      "Epoch 24/30 | Loss: 2.4961 | Test Acc: 29.77%\n",
      "Epoch 25/30 | Loss: 2.5126 | Test Acc: 30.16%\n",
      "Epoch 26/30 | Loss: 2.4826 | Test Acc: 28.91%\n",
      "Epoch 27/30 | Loss: 2.4745 | Test Acc: 30.77%\n",
      "Epoch 28/30 | Loss: 2.4060 | Test Acc: 31.79%\n",
      "Epoch 29/30 | Loss: 2.3987 | Test Acc: 31.49%\n",
      "Epoch 30/30 | Loss: 2.3622 | Test Acc: 31.59%\n",
      "   -> Final Result: Acc = 31.59%\n",
      "\n",
      "COMPLETE RESULT - CIFAR100\n",
      "Method                    Accuracy  \n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          28.67      \n",
      "Toeplitz-masked Linear    31.80     \n",
      "M_alpha(G)-masked         30.11      \n",
      "--------------------------------------------------\n",
      "GRF-masked Linear         31.99     \n",
      "Unmasked Linear           31.59     \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 30\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = 'CIFAR100'  # Options: 'CIFAR10', 'CIFAR100'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "# TODO Rename title in output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 - 30 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Dataset: CIFAR10\n",
      "\n",
      "======================================================================\n",
      "5.2 Visual transformer training on CIFAR10\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.9151 | Test Acc: 41.59%\n",
      "Epoch 2/30 | Loss: 1.5766 | Test Acc: 46.01%\n",
      "Epoch 3/30 | Loss: 1.4707 | Test Acc: 48.28%\n",
      "Epoch 4/30 | Loss: 1.4069 | Test Acc: 48.28%\n",
      "Epoch 5/30 | Loss: 1.3553 | Test Acc: 51.22%\n",
      "Epoch 6/30 | Loss: 1.3190 | Test Acc: 51.52%\n",
      "Epoch 7/30 | Loss: 1.2815 | Test Acc: 52.23%\n",
      "Epoch 8/30 | Loss: 1.2509 | Test Acc: 52.95%\n",
      "Epoch 9/30 | Loss: 1.2222 | Test Acc: 52.23%\n",
      "Epoch 10/30 | Loss: 1.1965 | Test Acc: 54.76%\n",
      "Epoch 11/30 | Loss: 1.1643 | Test Acc: 54.48%\n",
      "Epoch 12/30 | Loss: 1.1359 | Test Acc: 54.84%\n",
      "Epoch 13/30 | Loss: 1.1124 | Test Acc: 55.10%\n",
      "Epoch 14/30 | Loss: 1.0890 | Test Acc: 56.20%\n",
      "Epoch 15/30 | Loss: 1.0642 | Test Acc: 55.21%\n",
      "Epoch 16/30 | Loss: 1.0398 | Test Acc: 55.20%\n",
      "Epoch 17/30 | Loss: 1.0212 | Test Acc: 56.55%\n",
      "Epoch 18/30 | Loss: 0.9947 | Test Acc: 57.13%\n",
      "Epoch 19/30 | Loss: 0.9782 | Test Acc: 56.57%\n",
      "Epoch 20/30 | Loss: 0.9558 | Test Acc: 57.30%\n",
      "Epoch 21/30 | Loss: 0.9343 | Test Acc: 56.92%\n",
      "Epoch 22/30 | Loss: 0.9229 | Test Acc: 56.55%\n",
      "Epoch 23/30 | Loss: 0.9027 | Test Acc: 55.94%\n",
      "Epoch 24/30 | Loss: 0.8843 | Test Acc: 55.82%\n",
      "Epoch 25/30 | Loss: 0.8661 | Test Acc: 57.08%\n",
      "Epoch 26/30 | Loss: 0.8567 | Test Acc: 56.85%\n",
      "Epoch 27/30 | Loss: 0.8413 | Test Acc: 57.24%\n",
      "Epoch 28/30 | Loss: 0.8297 | Test Acc: 57.37%\n",
      "Epoch 29/30 | Loss: 0.8114 | Test Acc: 56.84%\n",
      "Epoch 30/30 | Loss: 0.7990 | Test Acc: 57.31%\n",
      "   -> Final Result: Acc = 57.31%\n",
      "\n",
      "--- Training TOEPLITZ on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.8646 | Test Acc: 41.10%\n",
      "Epoch 2/30 | Loss: 1.5724 | Test Acc: 46.21%\n",
      "Epoch 3/30 | Loss: 1.4727 | Test Acc: 48.36%\n",
      "Epoch 4/30 | Loss: 1.4077 | Test Acc: 50.38%\n",
      "Epoch 5/30 | Loss: 1.3544 | Test Acc: 51.34%\n",
      "Epoch 6/30 | Loss: 1.3103 | Test Acc: 53.19%\n",
      "Epoch 7/30 | Loss: 1.2754 | Test Acc: 53.99%\n",
      "Epoch 8/30 | Loss: 1.2411 | Test Acc: 53.76%\n",
      "Epoch 9/30 | Loss: 1.2085 | Test Acc: 54.50%\n",
      "Epoch 10/30 | Loss: 1.1851 | Test Acc: 54.86%\n",
      "Epoch 11/30 | Loss: 1.1575 | Test Acc: 55.71%\n",
      "Epoch 12/30 | Loss: 1.1360 | Test Acc: 56.52%\n",
      "Epoch 13/30 | Loss: 1.1055 | Test Acc: 56.76%\n",
      "Epoch 14/30 | Loss: 1.0872 | Test Acc: 57.16%\n",
      "Epoch 15/30 | Loss: 1.0674 | Test Acc: 57.64%\n",
      "Epoch 16/30 | Loss: 1.0474 | Test Acc: 57.19%\n",
      "Epoch 17/30 | Loss: 1.0265 | Test Acc: 57.17%\n",
      "Epoch 18/30 | Loss: 1.0084 | Test Acc: 57.76%\n",
      "Epoch 19/30 | Loss: 0.9899 | Test Acc: 58.46%\n",
      "Epoch 20/30 | Loss: 0.9710 | Test Acc: 58.32%\n",
      "Epoch 21/30 | Loss: 0.9549 | Test Acc: 58.81%\n",
      "Epoch 22/30 | Loss: 0.9384 | Test Acc: 58.60%\n",
      "Epoch 23/30 | Loss: 0.9257 | Test Acc: 58.65%\n",
      "Epoch 24/30 | Loss: 0.9071 | Test Acc: 58.52%\n",
      "Epoch 25/30 | Loss: 0.8957 | Test Acc: 58.55%\n",
      "Epoch 26/30 | Loss: 0.8831 | Test Acc: 59.52%\n",
      "Epoch 27/30 | Loss: 0.8691 | Test Acc: 59.31%\n",
      "Epoch 28/30 | Loss: 0.8541 | Test Acc: 59.37%\n",
      "Epoch 29/30 | Loss: 0.8396 | Test Acc: 60.34%\n",
      "Epoch 30/30 | Loss: 0.8274 | Test Acc: 60.59%\n",
      "   -> Final Result: Acc = 60.59%\n",
      "\n",
      "--- Training M_ALPHA on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.8820 | Test Acc: 40.63%\n",
      "Epoch 2/30 | Loss: 1.5749 | Test Acc: 45.90%\n",
      "Epoch 3/30 | Loss: 1.4785 | Test Acc: 48.63%\n",
      "Epoch 4/30 | Loss: 1.4215 | Test Acc: 49.83%\n",
      "Epoch 5/30 | Loss: 1.3699 | Test Acc: 51.28%\n",
      "Epoch 6/30 | Loss: 1.3310 | Test Acc: 50.60%\n",
      "Epoch 7/30 | Loss: 1.3051 | Test Acc: 50.93%\n",
      "Epoch 8/30 | Loss: 1.2739 | Test Acc: 51.46%\n",
      "Epoch 9/30 | Loss: 1.2383 | Test Acc: 54.72%\n",
      "Epoch 10/30 | Loss: 1.2118 | Test Acc: 54.95%\n",
      "Epoch 11/30 | Loss: 1.1828 | Test Acc: 54.07%\n",
      "Epoch 12/30 | Loss: 1.1597 | Test Acc: 53.61%\n",
      "Epoch 13/30 | Loss: 1.1384 | Test Acc: 56.33%\n",
      "Epoch 14/30 | Loss: 1.1173 | Test Acc: 56.78%\n",
      "Epoch 15/30 | Loss: 1.1019 | Test Acc: 56.78%\n",
      "Epoch 16/30 | Loss: 1.0739 | Test Acc: 57.48%\n",
      "Epoch 17/30 | Loss: 1.0591 | Test Acc: 57.19%\n",
      "Epoch 18/30 | Loss: 1.0422 | Test Acc: 56.74%\n",
      "Epoch 19/30 | Loss: 1.0245 | Test Acc: 55.97%\n",
      "Epoch 20/30 | Loss: 1.0108 | Test Acc: 57.60%\n",
      "Epoch 21/30 | Loss: 0.9973 | Test Acc: 57.81%\n",
      "Epoch 22/30 | Loss: 0.9854 | Test Acc: 59.16%\n",
      "Epoch 23/30 | Loss: 0.9626 | Test Acc: 58.76%\n",
      "Epoch 24/30 | Loss: 0.9489 | Test Acc: 57.60%\n",
      "Epoch 25/30 | Loss: 0.9396 | Test Acc: 57.97%\n",
      "Epoch 26/30 | Loss: 0.9266 | Test Acc: 58.81%\n",
      "Epoch 27/30 | Loss: 0.9076 | Test Acc: 58.52%\n",
      "Epoch 28/30 | Loss: 0.9071 | Test Acc: 59.10%\n",
      "Epoch 29/30 | Loss: 0.8901 | Test Acc: 59.08%\n",
      "Epoch 30/30 | Loss: 0.8818 | Test Acc: 58.02%\n",
      "   -> Final Result: Acc = 58.02%\n",
      "\n",
      "--- Training GRF on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.9241 | Test Acc: 38.69%\n",
      "Epoch 2/30 | Loss: 1.5968 | Test Acc: 44.77%\n",
      "Epoch 3/30 | Loss: 1.4928 | Test Acc: 48.39%\n",
      "Epoch 4/30 | Loss: 1.4386 | Test Acc: 48.30%\n",
      "Epoch 5/30 | Loss: 1.3963 | Test Acc: 50.44%\n",
      "Epoch 6/30 | Loss: 1.3512 | Test Acc: 51.79%\n",
      "Epoch 7/30 | Loss: 1.3151 | Test Acc: 52.01%\n",
      "Epoch 8/30 | Loss: 1.2869 | Test Acc: 52.16%\n",
      "Epoch 9/30 | Loss: 1.2573 | Test Acc: 54.19%\n",
      "Epoch 10/30 | Loss: 1.2217 | Test Acc: 55.34%\n",
      "Epoch 11/30 | Loss: 1.1943 | Test Acc: 55.93%\n",
      "Epoch 12/30 | Loss: 1.1660 | Test Acc: 56.50%\n",
      "Epoch 13/30 | Loss: 1.1401 | Test Acc: 56.46%\n",
      "Epoch 14/30 | Loss: 1.1159 | Test Acc: 57.32%\n",
      "Epoch 15/30 | Loss: 1.0931 | Test Acc: 58.09%\n",
      "Epoch 16/30 | Loss: 1.0667 | Test Acc: 57.38%\n",
      "Epoch 17/30 | Loss: 1.0551 | Test Acc: 59.17%\n",
      "Epoch 18/30 | Loss: 1.0318 | Test Acc: 58.18%\n",
      "Epoch 19/30 | Loss: 1.0129 | Test Acc: 59.14%\n",
      "Epoch 20/30 | Loss: 1.0051 | Test Acc: 59.06%\n",
      "Epoch 21/30 | Loss: 0.9801 | Test Acc: 59.66%\n",
      "Epoch 22/30 | Loss: 0.9679 | Test Acc: 59.91%\n",
      "Epoch 23/30 | Loss: 0.9553 | Test Acc: 59.07%\n",
      "Epoch 24/30 | Loss: 0.9347 | Test Acc: 59.96%\n",
      "Epoch 25/30 | Loss: 0.9163 | Test Acc: 59.92%\n",
      "Epoch 26/30 | Loss: 0.9121 | Test Acc: 60.11%\n",
      "Epoch 27/30 | Loss: 0.8976 | Test Acc: 59.72%\n",
      "Epoch 28/30 | Loss: 0.8830 | Test Acc: 59.04%\n",
      "Epoch 29/30 | Loss: 0.8778 | Test Acc: 59.07%\n",
      "Epoch 30/30 | Loss: 0.8647 | Test Acc: 60.37%\n",
      "   -> Final Result: Acc = 60.37%\n",
      "\n",
      "--- Training LINEAR on CIFAR10 ---\n",
      "Epoch 1/30 | Loss: 1.8723 | Test Acc: 41.47%\n",
      "Epoch 2/30 | Loss: 1.5809 | Test Acc: 44.46%\n",
      "Epoch 3/30 | Loss: 1.5058 | Test Acc: 44.44%\n",
      "Epoch 4/30 | Loss: 1.4553 | Test Acc: 49.61%\n",
      "Epoch 5/30 | Loss: 1.4043 | Test Acc: 48.56%\n",
      "Epoch 6/30 | Loss: 1.4192 | Test Acc: 49.76%\n",
      "Epoch 7/30 | Loss: 1.3843 | Test Acc: 50.23%\n",
      "Epoch 8/30 | Loss: 1.3682 | Test Acc: 50.75%\n",
      "Epoch 9/30 | Loss: 1.3328 | Test Acc: 51.95%\n",
      "Epoch 10/30 | Loss: 1.2993 | Test Acc: 53.70%\n",
      "Epoch 11/30 | Loss: 1.2809 | Test Acc: 53.39%\n",
      "Epoch 12/30 | Loss: 1.2801 | Test Acc: 52.11%\n",
      "Epoch 13/30 | Loss: 1.2471 | Test Acc: 53.83%\n",
      "Epoch 14/30 | Loss: 1.2244 | Test Acc: 54.53%\n",
      "Epoch 15/30 | Loss: 1.2061 | Test Acc: 55.08%\n",
      "Epoch 16/30 | Loss: 1.2418 | Test Acc: 54.35%\n",
      "Epoch 17/30 | Loss: 1.2284 | Test Acc: 55.34%\n",
      "Epoch 18/30 | Loss: 1.1965 | Test Acc: 50.20%\n",
      "Epoch 19/30 | Loss: 1.1830 | Test Acc: 55.66%\n",
      "Epoch 20/30 | Loss: 1.1684 | Test Acc: 55.57%\n",
      "Epoch 21/30 | Loss: 1.1565 | Test Acc: 54.24%\n",
      "Epoch 22/30 | Loss: 1.1438 | Test Acc: 57.35%\n",
      "Epoch 23/30 | Loss: 1.1002 | Test Acc: 57.06%\n",
      "Epoch 24/30 | Loss: 1.0809 | Test Acc: 58.14%\n",
      "Epoch 25/30 | Loss: 1.0730 | Test Acc: 58.17%\n",
      "Epoch 26/30 | Loss: 1.0684 | Test Acc: 57.63%\n",
      "Epoch 27/30 | Loss: 1.0932 | Test Acc: 57.26%\n",
      "Epoch 28/30 | Loss: 1.0492 | Test Acc: 58.23%\n",
      "Epoch 29/30 | Loss: 1.0190 | Test Acc: 59.01%\n",
      "Epoch 30/30 | Loss: 1.0361 | Test Acc: 58.52%\n",
      "   -> Final Result: Acc = 58.52%\n",
      "\n",
      "COMPLETE RESULT - CIFAR10\n",
      "Method                    Accuracy  \n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          57.31      \n",
      "Toeplitz-masked Linear    60.59     \n",
      "M_alpha(G)-masked         58.02      \n",
      "--------------------------------------------------\n",
      "GRF-masked Linear         60.37     \n",
      "Unmasked Linear           58.52     \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 30\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = 'CIFAR10'  # Options: 'CIFAR10', 'CIFAR100'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "# TODO Rename title in output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FashionMNIST - 10 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Dataset: FashionMNIST\n",
      "\n",
      "======================================================================\n",
      "5.2 Visual transformer training on FashionMNIST\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on FASHIONMNIST ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:01<00:00, 14.0MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 300kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.53MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.8049 | Test Acc: 80.18%\n",
      "Epoch 2/10 | Loss: 0.4555 | Test Acc: 84.50%\n",
      "Epoch 3/10 | Loss: 0.4091 | Test Acc: 84.87%\n",
      "Epoch 4/10 | Loss: 0.3803 | Test Acc: 85.40%\n",
      "Epoch 5/10 | Loss: 0.3616 | Test Acc: 86.37%\n",
      "Epoch 6/10 | Loss: 0.3420 | Test Acc: 86.12%\n",
      "Epoch 7/10 | Loss: 0.3305 | Test Acc: 86.64%\n",
      "Epoch 8/10 | Loss: 0.3205 | Test Acc: 87.07%\n",
      "Epoch 9/10 | Loss: 0.3080 | Test Acc: 87.49%\n",
      "Epoch 10/10 | Loss: 0.2999 | Test Acc: 87.49%\n",
      "   -> Final Result: Acc = 87.49%\n",
      "\n",
      "--- Training TOEPLITZ on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.8267 | Test Acc: 80.97%\n",
      "Epoch 2/10 | Loss: 0.4768 | Test Acc: 83.00%\n",
      "Epoch 3/10 | Loss: 0.4220 | Test Acc: 84.06%\n",
      "Epoch 4/10 | Loss: 0.3903 | Test Acc: 84.90%\n",
      "Epoch 5/10 | Loss: 0.3683 | Test Acc: 86.25%\n",
      "Epoch 6/10 | Loss: 0.3511 | Test Acc: 86.53%\n",
      "Epoch 7/10 | Loss: 0.3347 | Test Acc: 87.00%\n",
      "Epoch 8/10 | Loss: 0.3236 | Test Acc: 85.69%\n",
      "Epoch 9/10 | Loss: 0.3145 | Test Acc: 86.91%\n",
      "Epoch 10/10 | Loss: 0.3041 | Test Acc: 87.92%\n",
      "   -> Final Result: Acc = 87.92%\n",
      "\n",
      "--- Training M_ALPHA on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.7555 | Test Acc: 78.47%\n",
      "Epoch 2/10 | Loss: 0.4504 | Test Acc: 83.16%\n",
      "Epoch 3/10 | Loss: 0.4076 | Test Acc: 85.67%\n",
      "Epoch 4/10 | Loss: 0.3786 | Test Acc: 85.55%\n",
      "Epoch 5/10 | Loss: 0.3583 | Test Acc: 85.57%\n",
      "Epoch 6/10 | Loss: 0.3451 | Test Acc: 87.07%\n",
      "Epoch 7/10 | Loss: 0.3340 | Test Acc: 87.25%\n",
      "Epoch 8/10 | Loss: 0.3205 | Test Acc: 86.70%\n",
      "Epoch 9/10 | Loss: 0.3142 | Test Acc: 87.18%\n",
      "Epoch 10/10 | Loss: 0.3023 | Test Acc: 86.77%\n",
      "   -> Final Result: Acc = 86.77%\n",
      "\n",
      "--- Training GRF on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.8345 | Test Acc: 80.16%\n",
      "Epoch 2/10 | Loss: 0.4941 | Test Acc: 83.34%\n",
      "Epoch 3/10 | Loss: 0.4331 | Test Acc: 84.22%\n",
      "Epoch 4/10 | Loss: 0.4053 | Test Acc: 85.54%\n",
      "Epoch 5/10 | Loss: 0.3832 | Test Acc: 85.00%\n",
      "Epoch 6/10 | Loss: 0.3614 | Test Acc: 85.94%\n",
      "Epoch 7/10 | Loss: 0.3480 | Test Acc: 86.57%\n",
      "Epoch 8/10 | Loss: 0.3344 | Test Acc: 87.11%\n",
      "Epoch 9/10 | Loss: 0.3228 | Test Acc: 87.43%\n",
      "Epoch 10/10 | Loss: 0.3159 | Test Acc: 87.11%\n",
      "   -> Final Result: Acc = 87.11%\n",
      "\n",
      "--- Training LINEAR on FASHIONMNIST ---\n",
      "Epoch 1/10 | Loss: 0.7659 | Test Acc: 81.51%\n",
      "Epoch 2/10 | Loss: 0.4797 | Test Acc: 83.69%\n",
      "Epoch 3/10 | Loss: 0.4282 | Test Acc: 82.31%\n",
      "Epoch 4/10 | Loss: 0.3948 | Test Acc: 85.71%\n",
      "Epoch 5/10 | Loss: 0.3864 | Test Acc: 85.99%\n",
      "Epoch 6/10 | Loss: 0.3605 | Test Acc: 85.75%\n",
      "Epoch 7/10 | Loss: 0.3449 | Test Acc: 86.51%\n",
      "Epoch 8/10 | Loss: 0.3344 | Test Acc: 86.60%\n",
      "Epoch 9/10 | Loss: 0.3229 | Test Acc: 87.20%\n",
      "Epoch 10/10 | Loss: 0.3136 | Test Acc: 87.41%\n",
      "   -> Final Result: Acc = 87.41%\n",
      "\n",
      "COMPLETE RESULT - FashionMNIST\n",
      "Method                    Accuracy  \n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          87.49      \n",
      "Toeplitz-masked Linear    87.92     \n",
      "M_alpha(G)-masked         86.77      \n",
      "--------------------------------------------------\n",
      "GRF-masked Linear         87.11     \n",
      "Unmasked Linear           87.41     \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10\n",
    "IMAGE_SIZE = 32\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 2\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = 'FashionMNIST'  # Options: 'CIFAR10', 'CIFAR100', 'FashionMNIST', 'MNIST'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(model_type, dataset_name='cifar10', n_walks=50, p_halt=0.1, manipulate_images=False):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(dataset_name, BATCH_SIZE,manipulate_images=manipulate_images)\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc # Store last accuracy\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\")\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "# TODO Rename title in output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 - 15 EPOCHS - different parameters (batch_size, image_size, patch_size, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T22:07:28.390686Z",
     "iopub.status.busy": "2025-12-10T22:07:28.390023Z",
     "iopub.status.idle": "2025-12-10T22:41:30.082249Z",
     "shell.execute_reply": "2025-12-10T22:41:30.081589Z",
     "shell.execute_reply.started": "2025-12-10T22:07:28.390663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device: cuda\n",
      "Dataset: CIFAR10\n",
      "\n",
      "======================================================================\n",
      "5.2 Visual transformer training on CIFAR10\n",
      "======================================================================\n",
      "\n",
      "--- Training SOFTMAX on CIFAR10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:05<00:00, 28.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | Loss: 1.8496 | Test Acc: 42.40%\n",
      "Epoch 2/15 | Loss: 1.5206 | Test Acc: 46.95%\n",
      "Epoch 3/15 | Loss: 1.4078 | Test Acc: 50.05%\n",
      "Epoch 4/15 | Loss: 1.3361 | Test Acc: 49.56%\n",
      "Epoch 5/15 | Loss: 1.2800 | Test Acc: 52.41%\n",
      "Epoch 6/15 | Loss: 1.2350 | Test Acc: 53.77%\n",
      "Epoch 7/15 | Loss: 1.1895 | Test Acc: 52.02%\n",
      "Epoch 8/15 | Loss: 1.1520 | Test Acc: 54.61%\n",
      "Epoch 9/15 | Loss: 1.1141 | Test Acc: 55.36%\n",
      "Epoch 10/15 | Loss: 1.0784 | Test Acc: 55.44%\n",
      "Epoch 11/15 | Loss: 1.0444 | Test Acc: 55.81%\n",
      "Epoch 12/15 | Loss: 1.0096 | Test Acc: 56.54%\n",
      "Epoch 13/15 | Loss: 0.9797 | Test Acc: 56.33%\n",
      "Epoch 14/15 | Loss: 0.9469 | Test Acc: 56.55%\n",
      "Epoch 15/15 | Loss: 0.9225 | Test Acc: 56.42%\n",
      "   -> Final Result: Acc = 56.42%\n",
      "\n",
      "--- Training TOEPLITZ on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8154 | Test Acc: 43.01%\n",
      "Epoch 2/15 | Loss: 1.5184 | Test Acc: 47.63%\n",
      "Epoch 3/15 | Loss: 1.4132 | Test Acc: 49.49%\n",
      "Epoch 4/15 | Loss: 1.3396 | Test Acc: 51.25%\n",
      "Epoch 5/15 | Loss: 1.2820 | Test Acc: 54.27%\n",
      "Epoch 6/15 | Loss: 1.2316 | Test Acc: 54.02%\n",
      "Epoch 7/15 | Loss: 1.1849 | Test Acc: 56.40%\n",
      "Epoch 8/15 | Loss: 1.1401 | Test Acc: 57.39%\n",
      "Epoch 9/15 | Loss: 1.1037 | Test Acc: 56.99%\n",
      "Epoch 10/15 | Loss: 1.0693 | Test Acc: 57.68%\n",
      "Epoch 11/15 | Loss: 1.0393 | Test Acc: 58.07%\n",
      "Epoch 12/15 | Loss: 1.0069 | Test Acc: 57.07%\n",
      "Epoch 13/15 | Loss: 0.9774 | Test Acc: 59.07%\n",
      "Epoch 14/15 | Loss: 0.9551 | Test Acc: 59.26%\n",
      "Epoch 15/15 | Loss: 0.9199 | Test Acc: 57.90%\n",
      "   -> Final Result: Acc = 57.90%\n",
      "\n",
      "--- Training M_ALPHA on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8397 | Test Acc: 42.41%\n",
      "Epoch 2/15 | Loss: 1.5375 | Test Acc: 46.83%\n",
      "Epoch 3/15 | Loss: 1.4361 | Test Acc: 50.91%\n",
      "Epoch 4/15 | Loss: 1.3640 | Test Acc: 47.79%\n",
      "Epoch 5/15 | Loss: 1.3113 | Test Acc: 52.66%\n",
      "Epoch 6/15 | Loss: 1.2711 | Test Acc: 53.09%\n",
      "Epoch 7/15 | Loss: 1.2257 | Test Acc: 54.68%\n",
      "Epoch 8/15 | Loss: 1.1937 | Test Acc: 54.26%\n",
      "Epoch 9/15 | Loss: 1.1644 | Test Acc: 54.40%\n",
      "Epoch 10/15 | Loss: 1.1282 | Test Acc: 57.09%\n",
      "Epoch 11/15 | Loss: 1.0985 | Test Acc: 57.20%\n",
      "Epoch 12/15 | Loss: 1.0684 | Test Acc: 57.02%\n",
      "Epoch 13/15 | Loss: 1.0472 | Test Acc: 58.28%\n",
      "Epoch 14/15 | Loss: 1.0120 | Test Acc: 58.83%\n",
      "Epoch 15/15 | Loss: 0.9903 | Test Acc: 58.95%\n",
      "   -> Final Result: Acc = 58.95%\n",
      "\n",
      "--- Training GRF on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8559 | Test Acc: 40.18%\n",
      "Epoch 2/15 | Loss: 1.5532 | Test Acc: 46.04%\n",
      "Epoch 3/15 | Loss: 1.4456 | Test Acc: 49.78%\n",
      "Epoch 4/15 | Loss: 1.3716 | Test Acc: 50.96%\n",
      "Epoch 5/15 | Loss: 1.3120 | Test Acc: 53.50%\n",
      "Epoch 6/15 | Loss: 1.2592 | Test Acc: 54.55%\n",
      "Epoch 7/15 | Loss: 1.2169 | Test Acc: 55.89%\n",
      "Epoch 8/15 | Loss: 1.1742 | Test Acc: 56.82%\n",
      "Epoch 9/15 | Loss: 1.1382 | Test Acc: 55.11%\n",
      "Epoch 10/15 | Loss: 1.1065 | Test Acc: 57.09%\n",
      "Epoch 11/15 | Loss: 1.0737 | Test Acc: 57.24%\n",
      "Epoch 12/15 | Loss: 1.0428 | Test Acc: 58.42%\n",
      "Epoch 13/15 | Loss: 1.0165 | Test Acc: 57.12%\n",
      "Epoch 14/15 | Loss: 0.9901 | Test Acc: 58.70%\n",
      "Epoch 15/15 | Loss: 0.9590 | Test Acc: 58.93%\n",
      "   -> Final Result: Acc = 58.93%\n",
      "\n",
      "--- Training LINEAR on CIFAR10 ---\n",
      "Epoch 1/15 | Loss: 1.8134 | Test Acc: 42.36%\n",
      "Epoch 2/15 | Loss: 1.5152 | Test Acc: 47.23%\n",
      "Epoch 3/15 | Loss: 1.4215 | Test Acc: 50.62%\n",
      "Epoch 4/15 | Loss: 1.3586 | Test Acc: 51.89%\n",
      "Epoch 5/15 | Loss: 1.2864 | Test Acc: 52.76%\n",
      "Epoch 6/15 | Loss: 1.2407 | Test Acc: 51.27%\n",
      "Epoch 7/15 | Loss: 1.2008 | Test Acc: 54.97%\n",
      "Epoch 8/15 | Loss: 1.1655 | Test Acc: 55.55%\n",
      "Epoch 9/15 | Loss: 1.1296 | Test Acc: 56.20%\n",
      "Epoch 10/15 | Loss: 1.1004 | Test Acc: 56.84%\n",
      "Epoch 11/15 | Loss: 1.0954 | Test Acc: 57.38%\n",
      "Epoch 12/15 | Loss: 1.0824 | Test Acc: 58.40%\n",
      "Epoch 13/15 | Loss: 1.0588 | Test Acc: 59.06%\n",
      "Epoch 14/15 | Loss: 1.0098 | Test Acc: 59.50%\n",
      "Epoch 15/15 | Loss: 0.9816 | Test Acc: 59.58%\n",
      "   -> Final Result: Acc = 59.58%\n",
      "\n",
      "COMPLETE RESULT - CIFAR10\n",
      "Method                    Accuracy  \n",
      "--------------------------------------------------\n",
      "Unmasked Softmax          56.42      \n",
      "Toeplitz-masked Linear    57.90     \n",
      "M_alpha(G)-masked         58.95      \n",
      "--------------------------------------------------\n",
      "GRF-masked Linear         58.93     \n",
      "Unmasked Linear           59.58     \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- 1. CONFIGURATION ---\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 15\n",
    "IMAGE_SIZE = 42\n",
    "PATCH_SIZE = 4\n",
    "DIM = 64\n",
    "DEPTH = 4\n",
    "NUM_HEADS = 4\n",
    "MLP_DIM = 128\n",
    "DROPOUT = 0.1\n",
    "N_WALKS = 100\n",
    "P_HALT = 0.1\n",
    "\n",
    "# --- DATASET SELECTION ---\n",
    "# Switch to 'CIFAR100' for a harder task\n",
    "DATASET_NAME = \"CIFAR10\"  # Options: 'CIFAR10', 'CIFAR100', 'FashionMNIST', 'MNIST'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using Device: {DEVICE}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "\n",
    "\n",
    "# --- 5. TRAINING UTILS ---\n",
    "def train_and_evaluate(\n",
    "    model_type, dataset_name=\"cifar10\", n_walks=50, p_halt=0.1, manipulate_images=False\n",
    "):\n",
    "    print(f\"\\n--- Training {model_type.upper()} on {dataset_name.upper()} ---\")\n",
    "\n",
    "    trainloader, testloader, num_classes, channels = get_dataloaders(\n",
    "        dataset_name,\n",
    "        BATCH_SIZE,\n",
    "        manipulate_images=manipulate_images,\n",
    "        resize_image=IMAGE_SIZE,\n",
    "    )\n",
    "\n",
    "    model = ViT(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        dim=DIM,\n",
    "        depth=DEPTH,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        mlp_dim=MLP_DIM,\n",
    "        device=DEVICE,\n",
    "        channels=channels,\n",
    "        attention_type=model_type,\n",
    "        n_walks=n_walks,\n",
    "        p_halt=p_halt,\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- TRAINING LOOP WITH PER-EPOCH LOGGING ---\n",
    "    final_acc = 0.0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Evaluate after every epoch\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_acc = 100 * correct / total\n",
    "        final_acc = epoch_acc  # Store last accuracy\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_train_loss:.4f} | Test Acc: {epoch_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"   -> Final Result: Acc = {final_acc:.2f}%\")\n",
    "    return final_acc\n",
    "\n",
    "\n",
    "# TODO Rename title in output\n",
    "\n",
    "def replicate_table_1_complete(dataset_name, n_walks, p_halt, manipulate_images=False):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"5.2 Visual transformer training on {dataset_name}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    acc_softmax = train_and_evaluate('softmax', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_toeplitz = train_and_evaluate('toeplitz', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_m_alpha = train_and_evaluate('m_alpha', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "    acc_grf = train_and_evaluate('grf', dataset_name=dataset_name, n_walks=n_walks, p_halt=p_halt, manipulate_images=manipulate_images)\n",
    "    acc_linear = train_and_evaluate('linear', dataset_name=dataset_name, manipulate_images=manipulate_images)\n",
    "\n",
    "    print(f\"\\nCOMPLETE RESULT - {dataset_name}\")\n",
    "    print(f\"{'Method':<25} {'Accuracy':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Unmasked Softmax':<25} {acc_softmax:<10.2f} \")\n",
    "    print(f\"{'Toeplitz-masked Linear':<25} {acc_toeplitz:<10.2f}\")\n",
    "    print(f\"{'M_alpha(G)-masked':<25} {acc_m_alpha:<10.2f} \")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'GRF-masked Linear':<25} {acc_grf:<10.2f}\")\n",
    "    print(f\"{'Unmasked Linear':<25} {acc_linear:<10.2f}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    replicate_table_1_complete(DATASET_NAME, N_WALKS, P_HALT)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
