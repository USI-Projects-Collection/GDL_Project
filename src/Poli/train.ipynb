{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxl9dgG0gEOI",
        "outputId": "8d53c4ec-8694-4f4c-ebc3-5350e9ce4c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# os.chdir('/content/drive/MyDrive/gdl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gdrR-qsgE-0",
        "outputId": "72a4846a-180b-4cb1-b9da-ee7f222b3f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Start Training: BASELINE\n",
            "Ep 1: Train Loss: 0.435942, Val Loss: 0.117731\n",
            "Ep 2: Train Loss: 0.081333, Val Loss: 0.064164\n",
            "Ep 3: Train Loss: 0.058039, Val Loss: 0.056372\n",
            "Ep 4: Train Loss: 0.051227, Val Loss: 0.050842\n",
            "Ep 5: Train Loss: 0.046237, Val Loss: 0.045099\n",
            "Ep 6: Train Loss: 0.042346, Val Loss: 0.041847\n",
            "Ep 7: Train Loss: 0.038286, Val Loss: 0.036727\n",
            "Ep 8: Train Loss: 0.034563, Val Loss: 0.033371\n",
            "Ep 9: Train Loss: 0.030055, Val Loss: 0.029248\n",
            "Ep 10: Train Loss: 0.028208, Val Loss: 0.031879\n",
            "Ep 11: Train Loss: 0.025277, Val Loss: 0.026574\n",
            "Ep 12: Train Loss: 0.024223, Val Loss: 0.026323\n",
            "Ep 13: Train Loss: 0.024496, Val Loss: 0.024218\n",
            "Ep 14: Train Loss: 0.023349, Val Loss: 0.027794\n",
            "Ep 15: Train Loss: 0.023980, Val Loss: 0.023757\n",
            "Ep 16: Train Loss: 0.022008, Val Loss: 0.023401\n",
            "Ep 17: Train Loss: 0.021762, Val Loss: 0.023962\n",
            "Ep 18: Train Loss: 0.020996, Val Loss: 0.021346\n",
            "Ep 19: Train Loss: 0.021132, Val Loss: 0.023190\n",
            "Ep 20: Train Loss: 0.021778, Val Loss: 0.021515\n",
            "Ep 21: Train Loss: 0.020319, Val Loss: 0.021373\n",
            "Ep 22: Train Loss: 0.020655, Val Loss: 0.023033\n",
            "Ep 23: Train Loss: 0.021630, Val Loss: 0.023268\n",
            "Ep 24: Train Loss: 0.021042, Val Loss: 0.020868\n",
            "Ep 25: Train Loss: 0.019959, Val Loss: 0.021713\n",
            "Ep 26: Train Loss: 0.020761, Val Loss: 0.021980\n",
            "Ep 27: Train Loss: 0.020974, Val Loss: 0.021369\n",
            "Ep 28: Train Loss: 0.021119, Val Loss: 0.022121\n",
            "Ep 29: Train Loss: 0.020681, Val Loss: 0.020762\n",
            "Ep 30: Train Loss: 0.020007, Val Loss: 0.020874\n",
            "Ep 31: Train Loss: 0.020849, Val Loss: 0.021824\n",
            "Ep 32: Train Loss: 0.020539, Val Loss: 0.022645\n",
            "Ep 33: Train Loss: 0.019473, Val Loss: 0.024641\n",
            "Ep 34: Train Loss: 0.020400, Val Loss: 0.021178\n",
            "Ep 35: Train Loss: 0.020390, Val Loss: 0.023317\n",
            "Ep 36: Train Loss: 0.019670, Val Loss: 0.020218\n",
            "Ep 37: Train Loss: 0.019928, Val Loss: 0.020188\n",
            "Ep 38: Train Loss: 0.019840, Val Loss: 0.023245\n",
            "Ep 39: Train Loss: 0.020151, Val Loss: 0.020351\n",
            "Ep 40: Train Loss: 0.019418, Val Loss: 0.020314\n",
            "Ep 41: Train Loss: 0.019820, Val Loss: 0.021900\n",
            "Ep 42: Train Loss: 0.019728, Val Loss: 0.021802\n",
            "Ep 43: Train Loss: 0.018638, Val Loss: 0.019588\n",
            "Ep 44: Train Loss: 0.018153, Val Loss: 0.018888\n",
            "Ep 45: Train Loss: 0.018949, Val Loss: 0.020254\n",
            "Ep 46: Train Loss: 0.020420, Val Loss: 0.023129\n",
            "Ep 47: Train Loss: 0.020500, Val Loss: 0.024276\n",
            "Ep 48: Train Loss: 0.020140, Val Loss: 0.020915\n",
            "Ep 49: Train Loss: 0.020390, Val Loss: 0.020873\n",
            "Ep 50: Train Loss: 0.019279, Val Loss: 0.021657\n",
            "Ep 51: Train Loss: 0.021992, Val Loss: 0.026380\n",
            "Ep 52: Train Loss: 0.019457, Val Loss: 0.019677\n",
            "Ep 53: Train Loss: 0.018788, Val Loss: 0.020318\n",
            "Ep 54: Train Loss: 0.018886, Val Loss: 0.023027\n",
            "Ep 55: Train Loss: 0.019159, Val Loss: 0.019537\n",
            "Ep 56: Train Loss: 0.018492, Val Loss: 0.020843\n",
            "Ep 57: Train Loss: 0.018830, Val Loss: 0.023390\n",
            "Ep 58: Train Loss: 0.018422, Val Loss: 0.018914\n",
            "Ep 59: Train Loss: 0.017809, Val Loss: 0.019016\n",
            "Ep 60: Train Loss: 0.019182, Val Loss: 0.019698\n",
            "Ep 61: Train Loss: 0.018934, Val Loss: 0.019860\n",
            "Ep 62: Train Loss: 0.018593, Val Loss: 0.018699\n",
            "Ep 63: Train Loss: 0.018899, Val Loss: 0.021902\n",
            "Ep 64: Train Loss: 0.020547, Val Loss: 0.021685\n",
            "Ep 65: Train Loss: 0.020061, Val Loss: 0.019178\n",
            "Ep 66: Train Loss: 0.018122, Val Loss: 0.019039\n",
            "Ep 67: Train Loss: 0.017351, Val Loss: 0.019542\n",
            "Ep 68: Train Loss: 0.018032, Val Loss: 0.021214\n",
            "Ep 69: Train Loss: 0.017971, Val Loss: 0.018533\n",
            "Ep 70: Train Loss: 0.017565, Val Loss: 0.017892\n",
            "Ep 71: Train Loss: 0.017638, Val Loss: 0.019570\n",
            "Ep 72: Train Loss: 0.019318, Val Loss: 0.022005\n",
            "Ep 73: Train Loss: 0.019058, Val Loss: 0.018663\n",
            "Ep 74: Train Loss: 0.019181, Val Loss: 0.021502\n",
            "Ep 75: Train Loss: 0.019579, Val Loss: 0.021321\n",
            "Ep 76: Train Loss: 0.017823, Val Loss: 0.020796\n",
            "Ep 77: Train Loss: 0.019008, Val Loss: 0.021017\n",
            "Ep 78: Train Loss: 0.018241, Val Loss: 0.018071\n",
            "Ep 79: Train Loss: 0.017873, Val Loss: 0.018721\n",
            "Ep 80: Train Loss: 0.017357, Val Loss: 0.018057\n",
            "Ep 81: Train Loss: 0.016992, Val Loss: 0.017661\n",
            "Ep 82: Train Loss: 0.017111, Val Loss: 0.019129\n",
            "Ep 83: Train Loss: 0.017753, Val Loss: 0.019677\n",
            "Ep 84: Train Loss: 0.017343, Val Loss: 0.018072\n",
            "Ep 85: Train Loss: 0.016949, Val Loss: 0.017997\n",
            "Ep 86: Train Loss: 0.018076, Val Loss: 0.019443\n",
            "Ep 87: Train Loss: 0.018256, Val Loss: 0.020946\n",
            "Ep 88: Train Loss: 0.018578, Val Loss: 0.019064\n",
            "Ep 89: Train Loss: 0.017118, Val Loss: 0.017555\n",
            "Ep 90: Train Loss: 0.016935, Val Loss: 0.021351\n",
            "Ep 91: Train Loss: 0.017152, Val Loss: 0.019286\n",
            "Ep 92: Train Loss: 0.017174, Val Loss: 0.019835\n",
            "Ep 93: Train Loss: 0.017066, Val Loss: 0.021387\n",
            "Ep 94: Train Loss: 0.017222, Val Loss: 0.018542\n",
            "Ep 95: Train Loss: 0.018870, Val Loss: 0.019267\n",
            "Ep 96: Train Loss: 0.016453, Val Loss: 0.018673\n",
            "Ep 97: Train Loss: 0.016412, Val Loss: 0.018469\n",
            "Ep 98: Train Loss: 0.016375, Val Loss: 0.017617\n",
            "Ep 99: Train Loss: 0.016289, Val Loss: 0.018084\n",
            "Ep 100: Train Loss: 0.017246, Val Loss: 0.016806\n",
            "Ep 101: Train Loss: 0.016596, Val Loss: 0.019907\n",
            "Ep 102: Train Loss: 0.016844, Val Loss: 0.017762\n",
            "Ep 103: Train Loss: 0.017020, Val Loss: 0.017963\n",
            "Ep 104: Train Loss: 0.016795, Val Loss: 0.018396\n",
            "Ep 105: Train Loss: 0.016901, Val Loss: 0.018661\n",
            "Ep 106: Train Loss: 0.016185, Val Loss: 0.018214\n",
            "Ep 107: Train Loss: 0.015971, Val Loss: 0.017250\n",
            "Ep 108: Train Loss: 0.016466, Val Loss: 0.018399\n",
            "Ep 109: Train Loss: 0.017270, Val Loss: 0.018029\n",
            "Ep 110: Train Loss: 0.016668, Val Loss: 0.018346\n",
            "Ep 111: Train Loss: 0.016995, Val Loss: 0.017317\n",
            "Ep 112: Train Loss: 0.016570, Val Loss: 0.017441\n",
            "Ep 113: Train Loss: 0.016070, Val Loss: 0.017558\n",
            "Ep 114: Train Loss: 0.015862, Val Loss: 0.018198\n",
            "Ep 115: Train Loss: 0.016071, Val Loss: 0.019896\n",
            "Ep 116: Train Loss: 0.016663, Val Loss: 0.018085\n",
            "Ep 117: Train Loss: 0.016744, Val Loss: 0.020642\n",
            "Ep 118: Train Loss: 0.016816, Val Loss: 0.017344\n",
            "Ep 119: Train Loss: 0.016314, Val Loss: 0.017155\n",
            "Ep 120: Train Loss: 0.015694, Val Loss: 0.017644\n",
            "Ep 121: Train Loss: 0.017188, Val Loss: 0.020115\n",
            "Ep 122: Train Loss: 0.017059, Val Loss: 0.017398\n",
            "Ep 123: Train Loss: 0.016467, Val Loss: 0.021253\n",
            "Ep 124: Train Loss: 0.017109, Val Loss: 0.017522\n",
            "Ep 125: Train Loss: 0.016022, Val Loss: 0.017390\n",
            "Ep 126: Train Loss: 0.016218, Val Loss: 0.018492\n",
            "Ep 127: Train Loss: 0.016626, Val Loss: 0.017486\n",
            "Ep 128: Train Loss: 0.016757, Val Loss: 0.018035\n",
            "Ep 129: Train Loss: 0.015116, Val Loss: 0.017621\n",
            "Ep 130: Train Loss: 0.016198, Val Loss: 0.018364\n",
            "Ep 131: Train Loss: 0.016603, Val Loss: 0.017742\n",
            "Ep 132: Train Loss: 0.016232, Val Loss: 0.018172\n",
            "Ep 133: Train Loss: 0.017145, Val Loss: 0.021027\n",
            "Ep 134: Train Loss: 0.017027, Val Loss: 0.018863\n",
            "Ep 135: Train Loss: 0.016494, Val Loss: 0.017304\n",
            "Ep 136: Train Loss: 0.017039, Val Loss: 0.017912\n",
            "Ep 137: Train Loss: 0.016958, Val Loss: 0.018267\n",
            "Ep 138: Train Loss: 0.016009, Val Loss: 0.017759\n",
            "Ep 139: Train Loss: 0.016395, Val Loss: 0.017270\n",
            "Ep 140: Train Loss: 0.016383, Val Loss: 0.017682\n",
            "Ep 141: Train Loss: 0.015572, Val Loss: 0.017062\n",
            "Ep 142: Train Loss: 0.015533, Val Loss: 0.018089\n",
            "Ep 143: Train Loss: 0.016369, Val Loss: 0.016633\n",
            "Ep 144: Train Loss: 0.016895, Val Loss: 0.018922\n",
            "Ep 145: Train Loss: 0.015569, Val Loss: 0.020106\n",
            "Ep 146: Train Loss: 0.017170, Val Loss: 0.020459\n",
            "Ep 147: Train Loss: 0.016461, Val Loss: 0.017757\n",
            "Ep 148: Train Loss: 0.015912, Val Loss: 0.018394\n",
            "Ep 149: Train Loss: 0.015154, Val Loss: 0.018546\n",
            "Ep 150: Train Loss: 0.015620, Val Loss: 0.016938\n",
            "Ep 151: Train Loss: 0.015837, Val Loss: 0.018120\n",
            "Ep 152: Train Loss: 0.015999, Val Loss: 0.019772\n",
            "Ep 153: Train Loss: 0.015980, Val Loss: 0.016552\n",
            "Ep 154: Train Loss: 0.015947, Val Loss: 0.016850\n",
            "Ep 155: Train Loss: 0.015654, Val Loss: 0.016638\n",
            "Ep 156: Train Loss: 0.016842, Val Loss: 0.016600\n",
            "Ep 157: Train Loss: 0.015796, Val Loss: 0.017296\n",
            "Ep 158: Train Loss: 0.015736, Val Loss: 0.018453\n",
            "Ep 159: Train Loss: 0.016394, Val Loss: 0.017258\n",
            "Ep 160: Train Loss: 0.015853, Val Loss: 0.017193\n",
            "Ep 161: Train Loss: 0.015461, Val Loss: 0.018448\n",
            "Ep 162: Train Loss: 0.016510, Val Loss: 0.016713\n",
            "Ep 163: Train Loss: 0.016444, Val Loss: 0.019476\n",
            "Ep 164: Train Loss: 0.016142, Val Loss: 0.018392\n",
            "Ep 165: Train Loss: 0.016445, Val Loss: 0.017257\n",
            "Ep 166: Train Loss: 0.015769, Val Loss: 0.018071\n",
            "Ep 167: Train Loss: 0.015918, Val Loss: 0.017573\n",
            "Ep 168: Train Loss: 0.015353, Val Loss: 0.017588\n",
            "Ep 169: Train Loss: 0.014647, Val Loss: 0.016014\n",
            "Ep 170: Train Loss: 0.015058, Val Loss: 0.019441\n",
            "Ep 171: Train Loss: 0.015685, Val Loss: 0.017168\n",
            "Ep 172: Train Loss: 0.015213, Val Loss: 0.016098\n",
            "Ep 173: Train Loss: 0.015510, Val Loss: 0.016381\n",
            "Ep 174: Train Loss: 0.015466, Val Loss: 0.016304\n",
            "Ep 175: Train Loss: 0.015671, Val Loss: 0.018055\n",
            "Ep 176: Train Loss: 0.015720, Val Loss: 0.016968\n",
            "Ep 177: Train Loss: 0.015136, Val Loss: 0.017548\n",
            "Ep 178: Train Loss: 0.015367, Val Loss: 0.018791\n",
            "Ep 179: Train Loss: 0.016759, Val Loss: 0.016632\n",
            "Ep 180: Train Loss: 0.016507, Val Loss: 0.016851\n",
            "Ep 181: Train Loss: 0.015183, Val Loss: 0.016652\n",
            "Ep 182: Train Loss: 0.015150, Val Loss: 0.017799\n",
            "Ep 183: Train Loss: 0.015183, Val Loss: 0.016919\n",
            "Ep 184: Train Loss: 0.014937, Val Loss: 0.015645\n",
            "Ep 185: Train Loss: 0.014753, Val Loss: 0.016586\n",
            "Ep 186: Train Loss: 0.014911, Val Loss: 0.017425\n",
            "Ep 187: Train Loss: 0.015133, Val Loss: 0.017362\n",
            "Ep 188: Train Loss: 0.015047, Val Loss: 0.016587\n",
            "Ep 189: Train Loss: 0.015085, Val Loss: 0.017805\n",
            "Ep 190: Train Loss: 0.014791, Val Loss: 0.016856\n",
            "Ep 191: Train Loss: 0.015596, Val Loss: 0.017456\n",
            "Ep 192: Train Loss: 0.015022, Val Loss: 0.016615\n",
            "Ep 193: Train Loss: 0.014598, Val Loss: 0.015718\n",
            "Ep 194: Train Loss: 0.014220, Val Loss: 0.015534\n",
            "Ep 195: Train Loss: 0.014422, Val Loss: 0.016237\n",
            "Ep 196: Train Loss: 0.015599, Val Loss: 0.020445\n",
            "Ep 197: Train Loss: 0.015620, Val Loss: 0.016847\n",
            "Ep 198: Train Loss: 0.015690, Val Loss: 0.016122\n",
            "Ep 199: Train Loss: 0.015208, Val Loss: 0.021053\n",
            "Ep 200: Train Loss: 0.015385, Val Loss: 0.016655\n",
            "Ep 201: Train Loss: 0.014669, Val Loss: 0.016990\n",
            "Ep 202: Train Loss: 0.014819, Val Loss: 0.018155\n",
            "Ep 203: Train Loss: 0.014954, Val Loss: 0.016801\n",
            "Ep 204: Train Loss: 0.015614, Val Loss: 0.015771\n",
            "Ep 205: Train Loss: 0.014631, Val Loss: 0.016117\n",
            "Ep 206: Train Loss: 0.015095, Val Loss: 0.016844\n",
            "Ep 207: Train Loss: 0.014855, Val Loss: 0.017728\n",
            "Ep 208: Train Loss: 0.015213, Val Loss: 0.017934\n",
            "Ep 209: Train Loss: 0.015649, Val Loss: 0.015568\n",
            "Ep 210: Train Loss: 0.013842, Val Loss: 0.017523\n",
            "Ep 211: Train Loss: 0.014749, Val Loss: 0.016563\n",
            "Ep 212: Train Loss: 0.015119, Val Loss: 0.017357\n",
            "Ep 213: Train Loss: 0.014322, Val Loss: 0.016663\n",
            "Ep 214: Train Loss: 0.014306, Val Loss: 0.016108\n",
            "Ep 215: Train Loss: 0.014840, Val Loss: 0.017056\n",
            "Ep 216: Train Loss: 0.015001, Val Loss: 0.015661\n",
            "Ep 217: Train Loss: 0.014402, Val Loss: 0.015698\n",
            "Ep 218: Train Loss: 0.014522, Val Loss: 0.017216\n",
            "Ep 219: Train Loss: 0.014416, Val Loss: 0.017979\n",
            "Ep 220: Train Loss: 0.014926, Val Loss: 0.018223\n",
            "Ep 221: Train Loss: 0.014727, Val Loss: 0.016112\n",
            "Ep 222: Train Loss: 0.014390, Val Loss: 0.015046\n",
            "Ep 223: Train Loss: 0.014322, Val Loss: 0.018164\n",
            "Ep 224: Train Loss: 0.014485, Val Loss: 0.016155\n",
            "Ep 225: Train Loss: 0.015578, Val Loss: 0.016937\n",
            "Ep 226: Train Loss: 0.014461, Val Loss: 0.015110\n",
            "Ep 227: Train Loss: 0.014381, Val Loss: 0.017787\n",
            "Ep 228: Train Loss: 0.014962, Val Loss: 0.016172\n",
            "Ep 229: Train Loss: 0.014491, Val Loss: 0.017277\n",
            "Ep 230: Train Loss: 0.014342, Val Loss: 0.015666\n",
            "Ep 231: Train Loss: 0.014666, Val Loss: 0.015662\n",
            "Ep 232: Train Loss: 0.015317, Val Loss: 0.021836\n",
            "Ep 233: Train Loss: 0.015783, Val Loss: 0.016033\n",
            "Ep 234: Train Loss: 0.014502, Val Loss: 0.017987\n",
            "Ep 235: Train Loss: 0.014899, Val Loss: 0.017242\n",
            "Ep 236: Train Loss: 0.015629, Val Loss: 0.015396\n",
            "Ep 237: Train Loss: 0.015457, Val Loss: 0.016687\n",
            "Ep 238: Train Loss: 0.014720, Val Loss: 0.016870\n",
            "Ep 239: Train Loss: 0.014480, Val Loss: 0.016639\n",
            "Ep 240: Train Loss: 0.014684, Val Loss: 0.016643\n",
            "Ep 241: Train Loss: 0.013961, Val Loss: 0.016697\n",
            "Ep 242: Train Loss: 0.014389, Val Loss: 0.017533\n",
            "Ep 243: Train Loss: 0.014289, Val Loss: 0.015702\n",
            "Ep 244: Train Loss: 0.014874, Val Loss: 0.016143\n",
            "Ep 245: Train Loss: 0.014033, Val Loss: 0.017117\n",
            "Ep 246: Train Loss: 0.014357, Val Loss: 0.015657\n",
            "Ep 247: Train Loss: 0.015027, Val Loss: 0.017362\n",
            "Ep 248: Train Loss: 0.013642, Val Loss: 0.015379\n",
            "Ep 249: Train Loss: 0.013867, Val Loss: 0.016930\n",
            "Ep 250: Train Loss: 0.014305, Val Loss: 0.016172\n",
            "Ep 251: Train Loss: 0.014174, Val Loss: 0.016917\n",
            "Ep 252: Train Loss: 0.014565, Val Loss: 0.016445\n",
            "Ep 253: Train Loss: 0.013673, Val Loss: 0.015971\n",
            "Ep 254: Train Loss: 0.013720, Val Loss: 0.016946\n",
            "Ep 255: Train Loss: 0.013976, Val Loss: 0.016495\n",
            "Ep 256: Train Loss: 0.014768, Val Loss: 0.016288\n",
            "Ep 257: Train Loss: 0.013724, Val Loss: 0.017158\n",
            "Ep 258: Train Loss: 0.013522, Val Loss: 0.016118\n",
            "Ep 259: Train Loss: 0.013765, Val Loss: 0.015535\n",
            "Ep 260: Train Loss: 0.013504, Val Loss: 0.015701\n",
            "Ep 261: Train Loss: 0.013901, Val Loss: 0.016109\n",
            "Ep 262: Train Loss: 0.014080, Val Loss: 0.017799\n",
            "Ep 263: Train Loss: 0.014858, Val Loss: 0.016183\n",
            "Ep 264: Train Loss: 0.014562, Val Loss: 0.015567\n",
            "Ep 265: Train Loss: 0.013491, Val Loss: 0.016372\n",
            "Ep 266: Train Loss: 0.014548, Val Loss: 0.018968\n",
            "Ep 267: Train Loss: 0.014223, Val Loss: 0.015887\n",
            "Ep 268: Train Loss: 0.013706, Val Loss: 0.016489\n",
            "Ep 269: Train Loss: 0.014118, Val Loss: 0.016748\n",
            "Ep 270: Train Loss: 0.013504, Val Loss: 0.016184\n",
            "Ep 271: Train Loss: 0.013841, Val Loss: 0.017797\n",
            "Ep 272: Train Loss: 0.013891, Val Loss: 0.015993\n",
            "Ep 273: Train Loss: 0.013758, Val Loss: 0.015719\n",
            "Ep 274: Train Loss: 0.013019, Val Loss: 0.016515\n",
            "Ep 275: Train Loss: 0.013427, Val Loss: 0.015880\n",
            "Ep 276: Train Loss: 0.013489, Val Loss: 0.016829\n",
            "Ep 277: Train Loss: 0.014146, Val Loss: 0.017024\n",
            "Ep 278: Train Loss: 0.014288, Val Loss: 0.018983\n",
            "Ep 279: Train Loss: 0.013642, Val Loss: 0.015311\n",
            "Ep 280: Train Loss: 0.013330, Val Loss: 0.016862\n",
            "Ep 281: Train Loss: 0.014172, Val Loss: 0.017396\n",
            "Ep 282: Train Loss: 0.014042, Val Loss: 0.016568\n",
            "Ep 283: Train Loss: 0.013183, Val Loss: 0.017092\n",
            "Ep 284: Train Loss: 0.013216, Val Loss: 0.015751\n",
            "Ep 285: Train Loss: 0.013540, Val Loss: 0.016726\n",
            "Ep 286: Train Loss: 0.012982, Val Loss: 0.015959\n",
            "Ep 287: Train Loss: 0.013131, Val Loss: 0.015330\n",
            "Ep 288: Train Loss: 0.013440, Val Loss: 0.018965\n",
            "Ep 289: Train Loss: 0.013676, Val Loss: 0.016504\n",
            "Ep 290: Train Loss: 0.013850, Val Loss: 0.015437\n",
            "Ep 291: Train Loss: 0.012818, Val Loss: 0.015465\n",
            "Ep 292: Train Loss: 0.012522, Val Loss: 0.016861\n",
            "Ep 293: Train Loss: 0.013754, Val Loss: 0.018182\n",
            "Ep 294: Train Loss: 0.013465, Val Loss: 0.017317\n",
            "Ep 295: Train Loss: 0.013897, Val Loss: 0.015721\n",
            "Ep 296: Train Loss: 0.013339, Val Loss: 0.016657\n",
            "Ep 297: Train Loss: 0.013404, Val Loss: 0.016519\n",
            "Ep 298: Train Loss: 0.013777, Val Loss: 0.016884\n",
            "Ep 299: Train Loss: 0.013537, Val Loss: 0.016402\n",
            "Ep 300: Train Loss: 0.013085, Val Loss: 0.015049\n",
            "Ep 301: Train Loss: 0.013419, Val Loss: 0.018154\n",
            "Ep 302: Train Loss: 0.013138, Val Loss: 0.016270\n",
            "Ep 303: Train Loss: 0.012523, Val Loss: 0.016226\n",
            "Ep 304: Train Loss: 0.013731, Val Loss: 0.017646\n",
            "Ep 305: Train Loss: 0.012675, Val Loss: 0.016233\n",
            "Ep 306: Train Loss: 0.013661, Val Loss: 0.016146\n",
            "Ep 307: Train Loss: 0.012998, Val Loss: 0.016088\n",
            "Ep 308: Train Loss: 0.012838, Val Loss: 0.015647\n",
            "Ep 309: Train Loss: 0.013036, Val Loss: 0.015413\n",
            "Ep 310: Train Loss: 0.013028, Val Loss: 0.015407\n",
            "Ep 311: Train Loss: 0.012473, Val Loss: 0.016135\n",
            "Ep 312: Train Loss: 0.012481, Val Loss: 0.015987\n",
            "Ep 313: Train Loss: 0.013125, Val Loss: 0.015771\n",
            "Ep 314: Train Loss: 0.012920, Val Loss: 0.015566\n",
            "Ep 315: Train Loss: 0.012880, Val Loss: 0.016195\n",
            "Ep 316: Train Loss: 0.012835, Val Loss: 0.015802\n",
            "Ep 317: Train Loss: 0.012385, Val Loss: 0.015310\n",
            "Ep 318: Train Loss: 0.013748, Val Loss: 0.017681\n",
            "Ep 319: Train Loss: 0.013794, Val Loss: 0.015791\n",
            "Ep 320: Train Loss: 0.012432, Val Loss: 0.015204\n",
            "Ep 321: Train Loss: 0.012628, Val Loss: 0.015365\n",
            "Ep 322: Train Loss: 0.012366, Val Loss: 0.016841\n",
            "Ep 323: Train Loss: 0.013252, Val Loss: 0.015869\n",
            "Ep 324: Train Loss: 0.013279, Val Loss: 0.016077\n",
            "Ep 325: Train Loss: 0.013087, Val Loss: 0.015849\n",
            "Ep 326: Train Loss: 0.012835, Val Loss: 0.018046\n",
            "Ep 327: Train Loss: 0.013762, Val Loss: 0.017306\n",
            "Ep 328: Train Loss: 0.013025, Val Loss: 0.018269\n",
            "Ep 329: Train Loss: 0.012816, Val Loss: 0.017134\n",
            "Ep 330: Train Loss: 0.012766, Val Loss: 0.016640\n",
            "Ep 331: Train Loss: 0.012968, Val Loss: 0.016816\n",
            "Ep 332: Train Loss: 0.012580, Val Loss: 0.015491\n",
            "Ep 333: Train Loss: 0.012890, Val Loss: 0.017329\n",
            "Ep 334: Train Loss: 0.012712, Val Loss: 0.015070\n",
            "Ep 335: Train Loss: 0.012896, Val Loss: 0.017463\n",
            "Ep 336: Train Loss: 0.012591, Val Loss: 0.016345\n",
            "Ep 337: Train Loss: 0.012856, Val Loss: 0.016816\n",
            "Ep 338: Train Loss: 0.012670, Val Loss: 0.015372\n",
            "Ep 339: Train Loss: 0.012446, Val Loss: 0.015442\n",
            "Ep 340: Train Loss: 0.012256, Val Loss: 0.016123\n",
            "Ep 341: Train Loss: 0.012229, Val Loss: 0.015169\n",
            "Ep 342: Train Loss: 0.012133, Val Loss: 0.015204\n",
            "Ep 343: Train Loss: 0.012947, Val Loss: 0.016656\n",
            "Ep 344: Train Loss: 0.012616, Val Loss: 0.016239\n",
            "Ep 345: Train Loss: 0.012514, Val Loss: 0.015805\n",
            "Ep 346: Train Loss: 0.012172, Val Loss: 0.017358\n",
            "Ep 347: Train Loss: 0.012152, Val Loss: 0.015029\n",
            "Ep 348: Train Loss: 0.011998, Val Loss: 0.016611\n",
            "Ep 349: Train Loss: 0.012153, Val Loss: 0.015687\n",
            "Ep 350: Train Loss: 0.012004, Val Loss: 0.019847\n",
            "Ep 351: Train Loss: 0.012941, Val Loss: 0.018391\n",
            "Ep 352: Train Loss: 0.013145, Val Loss: 0.017181\n",
            "Ep 353: Train Loss: 0.012603, Val Loss: 0.015291\n",
            "Ep 354: Train Loss: 0.012203, Val Loss: 0.016601\n",
            "Ep 355: Train Loss: 0.012323, Val Loss: 0.016128\n",
            "Ep 356: Train Loss: 0.012643, Val Loss: 0.016386\n",
            "Ep 357: Train Loss: 0.012104, Val Loss: 0.016029\n",
            "Ep 358: Train Loss: 0.011665, Val Loss: 0.015694\n",
            "Ep 359: Train Loss: 0.011808, Val Loss: 0.015622\n",
            "Ep 360: Train Loss: 0.011933, Val Loss: 0.015889\n",
            "Ep 361: Train Loss: 0.012160, Val Loss: 0.016140\n",
            "Ep 362: Train Loss: 0.011772, Val Loss: 0.015538\n",
            "Ep 363: Train Loss: 0.011743, Val Loss: 0.015591\n",
            "Ep 364: Train Loss: 0.012959, Val Loss: 0.019126\n",
            "Ep 365: Train Loss: 0.012244, Val Loss: 0.016784\n",
            "Ep 366: Train Loss: 0.011708, Val Loss: 0.016921\n",
            "Ep 367: Train Loss: 0.011680, Val Loss: 0.016143\n",
            "Ep 368: Train Loss: 0.012419, Val Loss: 0.016269\n",
            "Ep 369: Train Loss: 0.012269, Val Loss: 0.015639\n",
            "Ep 370: Train Loss: 0.012085, Val Loss: 0.015351\n",
            "Ep 371: Train Loss: 0.012624, Val Loss: 0.017348\n",
            "Ep 372: Train Loss: 0.012357, Val Loss: 0.015115\n",
            "Ep 373: Train Loss: 0.011536, Val Loss: 0.015837\n",
            "Ep 374: Train Loss: 0.012088, Val Loss: 0.018563\n",
            "Ep 375: Train Loss: 0.013108, Val Loss: 0.017036\n",
            "Ep 376: Train Loss: 0.012291, Val Loss: 0.016931\n",
            "Ep 377: Train Loss: 0.012133, Val Loss: 0.015480\n",
            "Ep 378: Train Loss: 0.011836, Val Loss: 0.015543\n",
            "Ep 379: Train Loss: 0.012095, Val Loss: 0.014663\n",
            "Ep 380: Train Loss: 0.012144, Val Loss: 0.015016\n",
            "Ep 381: Train Loss: 0.011800, Val Loss: 0.016088\n",
            "Ep 382: Train Loss: 0.011747, Val Loss: 0.016275\n",
            "Ep 383: Train Loss: 0.011642, Val Loss: 0.015139\n",
            "Ep 384: Train Loss: 0.011716, Val Loss: 0.017145\n",
            "Ep 385: Train Loss: 0.011759, Val Loss: 0.017576\n",
            "Ep 386: Train Loss: 0.012908, Val Loss: 0.015953\n",
            "Ep 387: Train Loss: 0.012558, Val Loss: 0.016234\n",
            "Ep 388: Train Loss: 0.012267, Val Loss: 0.017249\n",
            "Ep 389: Train Loss: 0.013302, Val Loss: 0.016560\n",
            "Ep 390: Train Loss: 0.011661, Val Loss: 0.018859\n",
            "Ep 391: Train Loss: 0.012814, Val Loss: 0.016009\n",
            "Ep 392: Train Loss: 0.011661, Val Loss: 0.015865\n",
            "Ep 393: Train Loss: 0.012083, Val Loss: 0.015291\n",
            "Ep 394: Train Loss: 0.011432, Val Loss: 0.016067\n",
            "Ep 395: Train Loss: 0.011807, Val Loss: 0.014916\n",
            "Ep 396: Train Loss: 0.011913, Val Loss: 0.015893\n",
            "Ep 397: Train Loss: 0.011897, Val Loss: 0.015777\n",
            "Ep 398: Train Loss: 0.011823, Val Loss: 0.015536\n",
            "Ep 399: Train Loss: 0.011962, Val Loss: 0.015456\n",
            "Ep 400: Train Loss: 0.012700, Val Loss: 0.015959\n",
            "Ep 401: Train Loss: 0.012217, Val Loss: 0.017322\n",
            "Ep 402: Train Loss: 0.012087, Val Loss: 0.017252\n",
            "Ep 403: Train Loss: 0.012762, Val Loss: 0.015036\n",
            "Ep 404: Train Loss: 0.011500, Val Loss: 0.015312\n",
            "Ep 405: Train Loss: 0.011180, Val Loss: 0.015457\n",
            "Ep 406: Train Loss: 0.011886, Val Loss: 0.014906\n",
            "Ep 407: Train Loss: 0.011804, Val Loss: 0.015206\n",
            "Ep 408: Train Loss: 0.011764, Val Loss: 0.015559\n",
            "Ep 409: Train Loss: 0.011544, Val Loss: 0.015500\n",
            "Ep 410: Train Loss: 0.011766, Val Loss: 0.014521\n",
            "Ep 411: Train Loss: 0.011474, Val Loss: 0.015283\n",
            "Ep 412: Train Loss: 0.011412, Val Loss: 0.014331\n",
            "Ep 413: Train Loss: 0.011337, Val Loss: 0.015089\n",
            "Ep 414: Train Loss: 0.011251, Val Loss: 0.014397\n",
            "Ep 415: Train Loss: 0.011439, Val Loss: 0.014768\n",
            "Ep 416: Train Loss: 0.011316, Val Loss: 0.015926\n",
            "Ep 417: Train Loss: 0.012032, Val Loss: 0.015846\n",
            "Ep 418: Train Loss: 0.011687, Val Loss: 0.016332\n",
            "Ep 419: Train Loss: 0.011820, Val Loss: 0.015371\n",
            "Ep 420: Train Loss: 0.011352, Val Loss: 0.016248\n",
            "Ep 421: Train Loss: 0.011471, Val Loss: 0.016154\n",
            "Ep 422: Train Loss: 0.011478, Val Loss: 0.016236\n",
            "Ep 423: Train Loss: 0.012284, Val Loss: 0.015525\n",
            "Ep 424: Train Loss: 0.011827, Val Loss: 0.016129\n",
            "Ep 425: Train Loss: 0.011564, Val Loss: 0.015083\n",
            "Ep 426: Train Loss: 0.012162, Val Loss: 0.015314\n",
            "Ep 427: Train Loss: 0.011973, Val Loss: 0.016771\n",
            "Ep 428: Train Loss: 0.012076, Val Loss: 0.015276\n",
            "Ep 429: Train Loss: 0.011436, Val Loss: 0.015725\n",
            "Ep 430: Train Loss: 0.011222, Val Loss: 0.014484\n",
            "Ep 431: Train Loss: 0.011033, Val Loss: 0.015214\n",
            "Ep 432: Train Loss: 0.011639, Val Loss: 0.014852\n",
            "Ep 433: Train Loss: 0.011666, Val Loss: 0.014669\n",
            "Ep 434: Train Loss: 0.011869, Val Loss: 0.019678\n",
            "Ep 435: Train Loss: 0.012151, Val Loss: 0.014287\n",
            "Ep 436: Train Loss: 0.011945, Val Loss: 0.015252\n",
            "Ep 437: Train Loss: 0.011427, Val Loss: 0.015011\n",
            "Ep 438: Train Loss: 0.011785, Val Loss: 0.015912\n",
            "Ep 439: Train Loss: 0.011776, Val Loss: 0.015671\n",
            "Ep 440: Train Loss: 0.011776, Val Loss: 0.015098\n",
            "Ep 441: Train Loss: 0.011354, Val Loss: 0.014394\n",
            "Ep 442: Train Loss: 0.011781, Val Loss: 0.015184\n",
            "Ep 443: Train Loss: 0.012317, Val Loss: 0.016447\n",
            "Ep 444: Train Loss: 0.011835, Val Loss: 0.016581\n",
            "Ep 445: Train Loss: 0.011648, Val Loss: 0.015116\n",
            "Ep 446: Train Loss: 0.011538, Val Loss: 0.015815\n",
            "Ep 447: Train Loss: 0.011474, Val Loss: 0.016463\n",
            "Ep 448: Train Loss: 0.011400, Val Loss: 0.014801\n",
            "Ep 449: Train Loss: 0.011322, Val Loss: 0.015492\n",
            "Ep 450: Train Loss: 0.012618, Val Loss: 0.017651\n",
            "Ep 451: Train Loss: 0.012421, Val Loss: 0.015490\n",
            "Ep 452: Train Loss: 0.011855, Val Loss: 0.014801\n",
            "Ep 453: Train Loss: 0.011768, Val Loss: 0.014454\n",
            "Ep 454: Train Loss: 0.011128, Val Loss: 0.014615\n",
            "Ep 455: Train Loss: 0.011290, Val Loss: 0.013948\n",
            "Ep 456: Train Loss: 0.012063, Val Loss: 0.015655\n",
            "Ep 457: Train Loss: 0.010987, Val Loss: 0.016319\n",
            "Ep 458: Train Loss: 0.010959, Val Loss: 0.014793\n",
            "Ep 459: Train Loss: 0.012066, Val Loss: 0.015888\n",
            "Ep 460: Train Loss: 0.011073, Val Loss: 0.014806\n",
            "Ep 461: Train Loss: 0.010771, Val Loss: 0.014278\n",
            "Ep 462: Train Loss: 0.012158, Val Loss: 0.017350\n",
            "Ep 463: Train Loss: 0.011295, Val Loss: 0.015147\n",
            "Ep 464: Train Loss: 0.011081, Val Loss: 0.015453\n",
            "Ep 465: Train Loss: 0.011554, Val Loss: 0.015086\n",
            "Ep 466: Train Loss: 0.011080, Val Loss: 0.015112\n",
            "Ep 467: Train Loss: 0.011460, Val Loss: 0.015278\n",
            "Ep 468: Train Loss: 0.011716, Val Loss: 0.015264\n",
            "Ep 469: Train Loss: 0.011853, Val Loss: 0.014252\n",
            "Ep 470: Train Loss: 0.012316, Val Loss: 0.015724\n",
            "Ep 471: Train Loss: 0.011929, Val Loss: 0.014543\n",
            "Ep 472: Train Loss: 0.011463, Val Loss: 0.015408\n",
            "Ep 473: Train Loss: 0.011342, Val Loss: 0.014939\n",
            "Ep 474: Train Loss: 0.011338, Val Loss: 0.015621\n",
            "Ep 475: Train Loss: 0.011390, Val Loss: 0.014447\n",
            "Ep 476: Train Loss: 0.011031, Val Loss: 0.015403\n",
            "Ep 477: Train Loss: 0.010968, Val Loss: 0.016354\n",
            "Ep 478: Train Loss: 0.011314, Val Loss: 0.017049\n",
            "Ep 479: Train Loss: 0.011919, Val Loss: 0.014549\n",
            "Ep 480: Train Loss: 0.011265, Val Loss: 0.015680\n",
            "Ep 481: Train Loss: 0.010877, Val Loss: 0.014417\n",
            "Ep 482: Train Loss: 0.010961, Val Loss: 0.015158\n",
            "Ep 483: Train Loss: 0.011529, Val Loss: 0.014218\n",
            "Ep 484: Train Loss: 0.011017, Val Loss: 0.015507\n",
            "Ep 485: Train Loss: 0.012103, Val Loss: 0.015559\n",
            "Ep 486: Train Loss: 0.012333, Val Loss: 0.015160\n",
            "Ep 487: Train Loss: 0.011027, Val Loss: 0.015733\n",
            "Ep 488: Train Loss: 0.011202, Val Loss: 0.015038\n",
            "Ep 489: Train Loss: 0.011484, Val Loss: 0.014749\n",
            "Ep 490: Train Loss: 0.010985, Val Loss: 0.016717\n",
            "Ep 491: Train Loss: 0.011056, Val Loss: 0.014095\n",
            "Ep 492: Train Loss: 0.011094, Val Loss: 0.015180\n",
            "Ep 493: Train Loss: 0.011567, Val Loss: 0.016606\n",
            "Ep 494: Train Loss: 0.012018, Val Loss: 0.015934\n",
            "Ep 495: Train Loss: 0.011249, Val Loss: 0.015969\n",
            "Ep 496: Train Loss: 0.010984, Val Loss: 0.015815\n",
            "Ep 497: Train Loss: 0.011481, Val Loss: 0.015710\n",
            "Ep 498: Train Loss: 0.011699, Val Loss: 0.014165\n",
            "Ep 499: Train Loss: 0.011210, Val Loss: 0.015200\n",
            "Ep 500: Train Loss: 0.011759, Val Loss: 0.015221\n",
            "Loss plot saved to loss_plot_baseline.png\n",
            "Losses saved to losses_baseline.npz\n",
            "Model saved to model_baseline.pth\n",
            "Start Training: MP\n",
            "Ep 1: Train Loss: 0.237861, Val Loss: 0.076099\n",
            "Ep 2: Train Loss: 0.047404, Val Loss: 0.033373\n",
            "Ep 3: Train Loss: 0.027369, Val Loss: 0.025292\n",
            "Ep 4: Train Loss: 0.021463, Val Loss: 0.022339\n",
            "Ep 5: Train Loss: 0.020386, Val Loss: 0.024889\n",
            "Ep 6: Train Loss: 0.019320, Val Loss: 0.019454\n",
            "Ep 7: Train Loss: 0.017830, Val Loss: 0.019779\n",
            "Ep 8: Train Loss: 0.017666, Val Loss: 0.018980\n",
            "Ep 9: Train Loss: 0.017117, Val Loss: 0.018892\n",
            "Ep 10: Train Loss: 0.017093, Val Loss: 0.017554\n",
            "Ep 11: Train Loss: 0.016646, Val Loss: 0.019211\n",
            "Ep 12: Train Loss: 0.015872, Val Loss: 0.018202\n",
            "Ep 13: Train Loss: 0.016898, Val Loss: 0.017917\n",
            "Ep 14: Train Loss: 0.015876, Val Loss: 0.020710\n",
            "Ep 15: Train Loss: 0.017345, Val Loss: 0.022552\n",
            "Ep 16: Train Loss: 0.018020, Val Loss: 0.019721\n",
            "Ep 17: Train Loss: 0.015841, Val Loss: 0.018190\n",
            "Ep 18: Train Loss: 0.016303, Val Loss: 0.018821\n",
            "Ep 19: Train Loss: 0.017140, Val Loss: 0.018372\n",
            "Ep 20: Train Loss: 0.015828, Val Loss: 0.020756\n",
            "Ep 21: Train Loss: 0.016550, Val Loss: 0.018011\n",
            "Ep 22: Train Loss: 0.016883, Val Loss: 0.017153\n",
            "Ep 23: Train Loss: 0.016842, Val Loss: 0.018721\n",
            "Ep 24: Train Loss: 0.015739, Val Loss: 0.018096\n",
            "Ep 25: Train Loss: 0.015171, Val Loss: 0.018996\n",
            "Ep 26: Train Loss: 0.016043, Val Loss: 0.018746\n",
            "Ep 27: Train Loss: 0.015711, Val Loss: 0.018832\n",
            "Ep 28: Train Loss: 0.016759, Val Loss: 0.018423\n",
            "Ep 29: Train Loss: 0.015234, Val Loss: 0.018480\n",
            "Ep 30: Train Loss: 0.014998, Val Loss: 0.017060\n",
            "Ep 31: Train Loss: 0.015021, Val Loss: 0.018181\n",
            "Ep 32: Train Loss: 0.016717, Val Loss: 0.020136\n",
            "Ep 33: Train Loss: 0.015978, Val Loss: 0.017395\n",
            "Ep 34: Train Loss: 0.015597, Val Loss: 0.016424\n",
            "Ep 35: Train Loss: 0.014797, Val Loss: 0.018990\n",
            "Ep 36: Train Loss: 0.016851, Val Loss: 0.018495\n",
            "Ep 37: Train Loss: 0.015792, Val Loss: 0.016507\n",
            "Ep 38: Train Loss: 0.013900, Val Loss: 0.017175\n",
            "Ep 39: Train Loss: 0.014805, Val Loss: 0.018446\n",
            "Ep 40: Train Loss: 0.014818, Val Loss: 0.020211\n",
            "Ep 41: Train Loss: 0.015842, Val Loss: 0.017158\n",
            "Ep 42: Train Loss: 0.015594, Val Loss: 0.019216\n",
            "Ep 43: Train Loss: 0.014265, Val Loss: 0.016906\n",
            "Ep 44: Train Loss: 0.013950, Val Loss: 0.017410\n",
            "Ep 45: Train Loss: 0.014370, Val Loss: 0.022550\n",
            "Ep 46: Train Loss: 0.015374, Val Loss: 0.017289\n",
            "Ep 47: Train Loss: 0.015341, Val Loss: 0.016527\n",
            "Ep 48: Train Loss: 0.014924, Val Loss: 0.016826\n",
            "Ep 49: Train Loss: 0.014449, Val Loss: 0.017028\n",
            "Ep 50: Train Loss: 0.014455, Val Loss: 0.018097\n",
            "Ep 51: Train Loss: 0.014477, Val Loss: 0.019164\n",
            "Ep 52: Train Loss: 0.014113, Val Loss: 0.017774\n",
            "Ep 53: Train Loss: 0.014793, Val Loss: 0.018251\n",
            "Ep 54: Train Loss: 0.014907, Val Loss: 0.017031\n",
            "Ep 55: Train Loss: 0.015441, Val Loss: 0.019689\n",
            "Ep 56: Train Loss: 0.014380, Val Loss: 0.018102\n",
            "Ep 57: Train Loss: 0.014270, Val Loss: 0.019270\n",
            "Ep 58: Train Loss: 0.014581, Val Loss: 0.018487\n",
            "Ep 59: Train Loss: 0.013705, Val Loss: 0.017285\n",
            "Ep 60: Train Loss: 0.014745, Val Loss: 0.015943\n",
            "Ep 61: Train Loss: 0.013433, Val Loss: 0.017444\n",
            "Ep 62: Train Loss: 0.013552, Val Loss: 0.016641\n",
            "Ep 63: Train Loss: 0.013852, Val Loss: 0.017355\n",
            "Ep 64: Train Loss: 0.014432, Val Loss: 0.018605\n",
            "Ep 65: Train Loss: 0.013120, Val Loss: 0.017498\n",
            "Ep 66: Train Loss: 0.013591, Val Loss: 0.016300\n",
            "Ep 67: Train Loss: 0.013870, Val Loss: 0.018391\n",
            "Ep 68: Train Loss: 0.013558, Val Loss: 0.017737\n",
            "Ep 69: Train Loss: 0.013485, Val Loss: 0.016798\n",
            "Ep 70: Train Loss: 0.013787, Val Loss: 0.018386\n",
            "Ep 71: Train Loss: 0.014160, Val Loss: 0.016788\n",
            "Ep 72: Train Loss: 0.013462, Val Loss: 0.016723\n",
            "Ep 73: Train Loss: 0.013018, Val Loss: 0.016861\n",
            "Ep 74: Train Loss: 0.013076, Val Loss: 0.016979\n",
            "Ep 75: Train Loss: 0.013162, Val Loss: 0.016189\n",
            "Ep 76: Train Loss: 0.013612, Val Loss: 0.016517\n",
            "Ep 77: Train Loss: 0.013299, Val Loss: 0.017035\n",
            "Ep 78: Train Loss: 0.012688, Val Loss: 0.015819\n",
            "Ep 79: Train Loss: 0.012958, Val Loss: 0.019038\n",
            "Ep 80: Train Loss: 0.014396, Val Loss: 0.018522\n",
            "Ep 81: Train Loss: 0.013058, Val Loss: 0.016314\n",
            "Ep 82: Train Loss: 0.013446, Val Loss: 0.018159\n",
            "Ep 83: Train Loss: 0.014431, Val Loss: 0.017253\n",
            "Ep 84: Train Loss: 0.013873, Val Loss: 0.016102\n",
            "Ep 85: Train Loss: 0.012952, Val Loss: 0.015791\n",
            "Ep 86: Train Loss: 0.012231, Val Loss: 0.016820\n",
            "Ep 87: Train Loss: 0.013084, Val Loss: 0.019037\n",
            "Ep 88: Train Loss: 0.013136, Val Loss: 0.017242\n",
            "Ep 89: Train Loss: 0.013765, Val Loss: 0.017038\n",
            "Ep 90: Train Loss: 0.012853, Val Loss: 0.016423\n",
            "Ep 91: Train Loss: 0.012505, Val Loss: 0.018564\n",
            "Ep 92: Train Loss: 0.013894, Val Loss: 0.016665\n",
            "Ep 93: Train Loss: 0.013406, Val Loss: 0.016202\n",
            "Ep 94: Train Loss: 0.012514, Val Loss: 0.018332\n",
            "Ep 95: Train Loss: 0.012434, Val Loss: 0.017423\n",
            "Ep 96: Train Loss: 0.013181, Val Loss: 0.018371\n",
            "Ep 97: Train Loss: 0.012904, Val Loss: 0.017400\n",
            "Ep 98: Train Loss: 0.013168, Val Loss: 0.015459\n",
            "Ep 99: Train Loss: 0.012546, Val Loss: 0.016304\n",
            "Ep 100: Train Loss: 0.012609, Val Loss: 0.016907\n",
            "Ep 101: Train Loss: 0.013549, Val Loss: 0.016491\n",
            "Ep 102: Train Loss: 0.012474, Val Loss: 0.015393\n",
            "Ep 103: Train Loss: 0.013476, Val Loss: 0.017221\n",
            "Ep 104: Train Loss: 0.012738, Val Loss: 0.016966\n",
            "Ep 105: Train Loss: 0.012942, Val Loss: 0.017048\n",
            "Ep 106: Train Loss: 0.012541, Val Loss: 0.016987\n",
            "Ep 107: Train Loss: 0.013018, Val Loss: 0.016265\n",
            "Ep 108: Train Loss: 0.012758, Val Loss: 0.017347\n",
            "Ep 109: Train Loss: 0.013083, Val Loss: 0.017374\n",
            "Ep 110: Train Loss: 0.012934, Val Loss: 0.016636\n",
            "Ep 111: Train Loss: 0.012011, Val Loss: 0.016280\n",
            "Ep 112: Train Loss: 0.011974, Val Loss: 0.015900\n",
            "Ep 113: Train Loss: 0.012195, Val Loss: 0.016644\n",
            "Ep 114: Train Loss: 0.012621, Val Loss: 0.014671\n",
            "Ep 115: Train Loss: 0.012318, Val Loss: 0.016400\n",
            "Ep 116: Train Loss: 0.013133, Val Loss: 0.017341\n",
            "Ep 117: Train Loss: 0.013080, Val Loss: 0.015562\n",
            "Ep 118: Train Loss: 0.012925, Val Loss: 0.015609\n",
            "Ep 119: Train Loss: 0.012593, Val Loss: 0.017227\n",
            "Ep 120: Train Loss: 0.013106, Val Loss: 0.016130\n",
            "Ep 121: Train Loss: 0.011775, Val Loss: 0.017180\n",
            "Ep 122: Train Loss: 0.012720, Val Loss: 0.015477\n",
            "Ep 123: Train Loss: 0.012284, Val Loss: 0.016911\n",
            "Ep 124: Train Loss: 0.013245, Val Loss: 0.016562\n",
            "Ep 125: Train Loss: 0.012115, Val Loss: 0.015035\n",
            "Ep 126: Train Loss: 0.011915, Val Loss: 0.015399\n",
            "Ep 127: Train Loss: 0.012193, Val Loss: 0.015340\n",
            "Ep 128: Train Loss: 0.012256, Val Loss: 0.018316\n",
            "Ep 129: Train Loss: 0.013967, Val Loss: 0.016209\n",
            "Ep 130: Train Loss: 0.012285, Val Loss: 0.014848\n",
            "Ep 131: Train Loss: 0.011461, Val Loss: 0.016163\n",
            "Ep 132: Train Loss: 0.011309, Val Loss: 0.015406\n",
            "Ep 133: Train Loss: 0.012088, Val Loss: 0.016485\n",
            "Ep 134: Train Loss: 0.012823, Val Loss: 0.019145\n",
            "Ep 135: Train Loss: 0.013073, Val Loss: 0.016817\n",
            "Ep 136: Train Loss: 0.011815, Val Loss: 0.015130\n",
            "Ep 137: Train Loss: 0.011748, Val Loss: 0.016434\n",
            "Ep 138: Train Loss: 0.012339, Val Loss: 0.015637\n",
            "Ep 139: Train Loss: 0.012445, Val Loss: 0.015093\n",
            "Ep 140: Train Loss: 0.011673, Val Loss: 0.015959\n",
            "Ep 141: Train Loss: 0.011744, Val Loss: 0.016589\n",
            "Ep 142: Train Loss: 0.011361, Val Loss: 0.017601\n",
            "Ep 143: Train Loss: 0.011922, Val Loss: 0.017069\n",
            "Ep 144: Train Loss: 0.012132, Val Loss: 0.016319\n",
            "Ep 145: Train Loss: 0.011971, Val Loss: 0.015841\n",
            "Ep 146: Train Loss: 0.011909, Val Loss: 0.015504\n",
            "Ep 147: Train Loss: 0.012201, Val Loss: 0.016655\n",
            "Ep 148: Train Loss: 0.011986, Val Loss: 0.015684\n",
            "Ep 149: Train Loss: 0.012907, Val Loss: 0.015306\n",
            "Ep 150: Train Loss: 0.011654, Val Loss: 0.017035\n",
            "Ep 151: Train Loss: 0.011268, Val Loss: 0.015511\n",
            "Ep 152: Train Loss: 0.012301, Val Loss: 0.020102\n",
            "Ep 153: Train Loss: 0.012035, Val Loss: 0.015714\n",
            "Ep 154: Train Loss: 0.012738, Val Loss: 0.016636\n",
            "Ep 155: Train Loss: 0.011272, Val Loss: 0.015603\n",
            "Ep 156: Train Loss: 0.011390, Val Loss: 0.016199\n",
            "Ep 157: Train Loss: 0.011320, Val Loss: 0.015108\n",
            "Ep 158: Train Loss: 0.011296, Val Loss: 0.016004\n",
            "Ep 159: Train Loss: 0.011132, Val Loss: 0.015383\n",
            "Ep 160: Train Loss: 0.011740, Val Loss: 0.018290\n",
            "Ep 161: Train Loss: 0.012051, Val Loss: 0.016867\n",
            "Ep 162: Train Loss: 0.011912, Val Loss: 0.017473\n",
            "Ep 163: Train Loss: 0.012002, Val Loss: 0.016494\n",
            "Ep 164: Train Loss: 0.011722, Val Loss: 0.016545\n",
            "Ep 165: Train Loss: 0.011815, Val Loss: 0.015054\n",
            "Ep 166: Train Loss: 0.011761, Val Loss: 0.014984\n",
            "Ep 167: Train Loss: 0.011697, Val Loss: 0.015231\n",
            "Ep 168: Train Loss: 0.011515, Val Loss: 0.015343\n",
            "Ep 169: Train Loss: 0.011398, Val Loss: 0.014966\n",
            "Ep 170: Train Loss: 0.010969, Val Loss: 0.014958\n",
            "Ep 171: Train Loss: 0.011200, Val Loss: 0.015675\n",
            "Ep 172: Train Loss: 0.011056, Val Loss: 0.014797\n",
            "Ep 173: Train Loss: 0.010814, Val Loss: 0.015427\n",
            "Ep 174: Train Loss: 0.011020, Val Loss: 0.015198\n",
            "Ep 175: Train Loss: 0.011460, Val Loss: 0.016444\n",
            "Ep 176: Train Loss: 0.011197, Val Loss: 0.015373\n",
            "Ep 177: Train Loss: 0.010842, Val Loss: 0.016206\n",
            "Ep 178: Train Loss: 0.011807, Val Loss: 0.016687\n",
            "Ep 179: Train Loss: 0.011724, Val Loss: 0.015927\n",
            "Ep 180: Train Loss: 0.011376, Val Loss: 0.015000\n",
            "Ep 181: Train Loss: 0.011220, Val Loss: 0.016033\n",
            "Ep 182: Train Loss: 0.011080, Val Loss: 0.015349\n",
            "Ep 183: Train Loss: 0.012003, Val Loss: 0.014224\n",
            "Ep 184: Train Loss: 0.011203, Val Loss: 0.016480\n",
            "Ep 185: Train Loss: 0.011423, Val Loss: 0.016128\n",
            "Ep 186: Train Loss: 0.011387, Val Loss: 0.014674\n",
            "Ep 187: Train Loss: 0.011765, Val Loss: 0.016470\n",
            "Ep 188: Train Loss: 0.012181, Val Loss: 0.017004\n",
            "Ep 189: Train Loss: 0.011822, Val Loss: 0.015167\n",
            "Ep 190: Train Loss: 0.010931, Val Loss: 0.015645\n",
            "Ep 191: Train Loss: 0.011189, Val Loss: 0.014487\n",
            "Ep 192: Train Loss: 0.010949, Val Loss: 0.015183\n",
            "Ep 193: Train Loss: 0.010992, Val Loss: 0.015320\n",
            "Ep 194: Train Loss: 0.011210, Val Loss: 0.016528\n",
            "Ep 195: Train Loss: 0.011109, Val Loss: 0.016500\n",
            "Ep 196: Train Loss: 0.011544, Val Loss: 0.014554\n",
            "Ep 197: Train Loss: 0.011062, Val Loss: 0.016408\n",
            "Ep 198: Train Loss: 0.011366, Val Loss: 0.015789\n",
            "Ep 199: Train Loss: 0.011235, Val Loss: 0.015958\n",
            "Ep 200: Train Loss: 0.011527, Val Loss: 0.014177\n",
            "Ep 201: Train Loss: 0.010402, Val Loss: 0.015811\n",
            "Ep 202: Train Loss: 0.010849, Val Loss: 0.015224\n",
            "Ep 203: Train Loss: 0.011098, Val Loss: 0.015812\n",
            "Ep 204: Train Loss: 0.010987, Val Loss: 0.016908\n",
            "Ep 205: Train Loss: 0.011004, Val Loss: 0.015897\n",
            "Ep 206: Train Loss: 0.011324, Val Loss: 0.016887\n",
            "Ep 207: Train Loss: 0.012381, Val Loss: 0.015415\n",
            "Ep 208: Train Loss: 0.011255, Val Loss: 0.015933\n",
            "Ep 209: Train Loss: 0.010902, Val Loss: 0.015404\n",
            "Ep 210: Train Loss: 0.011584, Val Loss: 0.015428\n",
            "Ep 211: Train Loss: 0.011198, Val Loss: 0.014957\n",
            "Ep 212: Train Loss: 0.010476, Val Loss: 0.014491\n",
            "Ep 213: Train Loss: 0.010417, Val Loss: 0.014187\n",
            "Ep 214: Train Loss: 0.010606, Val Loss: 0.017083\n",
            "Ep 215: Train Loss: 0.011607, Val Loss: 0.015086\n",
            "Ep 216: Train Loss: 0.011252, Val Loss: 0.016895\n",
            "Ep 217: Train Loss: 0.011054, Val Loss: 0.015259\n",
            "Ep 218: Train Loss: 0.010690, Val Loss: 0.015792\n",
            "Ep 219: Train Loss: 0.010792, Val Loss: 0.014240\n",
            "Ep 220: Train Loss: 0.010928, Val Loss: 0.015240\n",
            "Ep 221: Train Loss: 0.010574, Val Loss: 0.014998\n",
            "Ep 222: Train Loss: 0.010719, Val Loss: 0.015360\n",
            "Ep 223: Train Loss: 0.010395, Val Loss: 0.015243\n",
            "Ep 224: Train Loss: 0.010552, Val Loss: 0.015843\n",
            "Ep 225: Train Loss: 0.010809, Val Loss: 0.016225\n",
            "Ep 226: Train Loss: 0.010381, Val Loss: 0.014811\n",
            "Ep 227: Train Loss: 0.010431, Val Loss: 0.015123\n",
            "Ep 228: Train Loss: 0.010149, Val Loss: 0.014931\n",
            "Ep 229: Train Loss: 0.010896, Val Loss: 0.016039\n",
            "Ep 230: Train Loss: 0.010551, Val Loss: 0.014912\n",
            "Ep 231: Train Loss: 0.010878, Val Loss: 0.014398\n",
            "Ep 232: Train Loss: 0.010556, Val Loss: 0.016393\n",
            "Ep 233: Train Loss: 0.010925, Val Loss: 0.016457\n",
            "Ep 234: Train Loss: 0.011645, Val Loss: 0.015862\n",
            "Ep 235: Train Loss: 0.011296, Val Loss: 0.016394\n",
            "Ep 236: Train Loss: 0.010994, Val Loss: 0.016856\n",
            "Ep 237: Train Loss: 0.010326, Val Loss: 0.015210\n",
            "Ep 238: Train Loss: 0.010728, Val Loss: 0.015039\n",
            "Ep 239: Train Loss: 0.010483, Val Loss: 0.015460\n",
            "Ep 240: Train Loss: 0.009947, Val Loss: 0.016207\n",
            "Ep 241: Train Loss: 0.010889, Val Loss: 0.016166\n",
            "Ep 242: Train Loss: 0.010676, Val Loss: 0.015893\n",
            "Ep 243: Train Loss: 0.010568, Val Loss: 0.016215\n",
            "Ep 244: Train Loss: 0.010266, Val Loss: 0.015079\n",
            "Ep 245: Train Loss: 0.011049, Val Loss: 0.015369\n",
            "Ep 246: Train Loss: 0.010577, Val Loss: 0.017247\n",
            "Ep 247: Train Loss: 0.011241, Val Loss: 0.014603\n",
            "Ep 248: Train Loss: 0.010989, Val Loss: 0.015242\n",
            "Ep 249: Train Loss: 0.010526, Val Loss: 0.015670\n",
            "Ep 250: Train Loss: 0.010893, Val Loss: 0.016070\n",
            "Ep 251: Train Loss: 0.010789, Val Loss: 0.015786\n",
            "Ep 252: Train Loss: 0.010394, Val Loss: 0.014807\n",
            "Ep 253: Train Loss: 0.010396, Val Loss: 0.016580\n",
            "Ep 254: Train Loss: 0.011201, Val Loss: 0.014820\n",
            "Ep 255: Train Loss: 0.010645, Val Loss: 0.015496\n",
            "Ep 256: Train Loss: 0.010508, Val Loss: 0.016652\n",
            "Ep 257: Train Loss: 0.010288, Val Loss: 0.015330\n",
            "Ep 258: Train Loss: 0.010240, Val Loss: 0.018441\n",
            "Ep 259: Train Loss: 0.010334, Val Loss: 0.014794\n",
            "Ep 260: Train Loss: 0.010022, Val Loss: 0.015134\n",
            "Ep 261: Train Loss: 0.009953, Val Loss: 0.015911\n",
            "Ep 262: Train Loss: 0.010947, Val Loss: 0.015630\n",
            "Ep 263: Train Loss: 0.010602, Val Loss: 0.015133\n",
            "Ep 264: Train Loss: 0.010704, Val Loss: 0.014965\n",
            "Ep 265: Train Loss: 0.009942, Val Loss: 0.015412\n",
            "Ep 266: Train Loss: 0.010198, Val Loss: 0.015830\n",
            "Ep 267: Train Loss: 0.011195, Val Loss: 0.015644\n",
            "Ep 268: Train Loss: 0.010613, Val Loss: 0.018667\n",
            "Ep 269: Train Loss: 0.010513, Val Loss: 0.015560\n",
            "Ep 270: Train Loss: 0.010337, Val Loss: 0.015461\n",
            "Ep 271: Train Loss: 0.010593, Val Loss: 0.015239\n",
            "Ep 272: Train Loss: 0.010157, Val Loss: 0.015530\n",
            "Ep 273: Train Loss: 0.010012, Val Loss: 0.014561\n",
            "Ep 274: Train Loss: 0.009487, Val Loss: 0.013573\n",
            "Ep 275: Train Loss: 0.010085, Val Loss: 0.017982\n",
            "Ep 276: Train Loss: 0.010063, Val Loss: 0.014149\n",
            "Ep 277: Train Loss: 0.009408, Val Loss: 0.015003\n",
            "Ep 278: Train Loss: 0.010274, Val Loss: 0.015195\n",
            "Ep 279: Train Loss: 0.010482, Val Loss: 0.014585\n",
            "Ep 280: Train Loss: 0.009609, Val Loss: 0.014842\n",
            "Ep 281: Train Loss: 0.010047, Val Loss: 0.015996\n",
            "Ep 282: Train Loss: 0.010696, Val Loss: 0.015448\n",
            "Ep 283: Train Loss: 0.009925, Val Loss: 0.016090\n",
            "Ep 284: Train Loss: 0.010267, Val Loss: 0.014878\n",
            "Ep 285: Train Loss: 0.010077, Val Loss: 0.014884\n",
            "Ep 286: Train Loss: 0.009917, Val Loss: 0.014490\n",
            "Ep 287: Train Loss: 0.010176, Val Loss: 0.015175\n",
            "Ep 288: Train Loss: 0.010925, Val Loss: 0.017255\n",
            "Ep 289: Train Loss: 0.010797, Val Loss: 0.016777\n",
            "Ep 290: Train Loss: 0.010162, Val Loss: 0.015025\n",
            "Ep 291: Train Loss: 0.009736, Val Loss: 0.014020\n",
            "Ep 292: Train Loss: 0.009526, Val Loss: 0.014810\n",
            "Ep 293: Train Loss: 0.009876, Val Loss: 0.014571\n",
            "Ep 294: Train Loss: 0.010335, Val Loss: 0.015139\n",
            "Ep 295: Train Loss: 0.010232, Val Loss: 0.016234\n",
            "Ep 296: Train Loss: 0.010085, Val Loss: 0.014442\n",
            "Ep 297: Train Loss: 0.009459, Val Loss: 0.014653\n",
            "Ep 298: Train Loss: 0.009429, Val Loss: 0.015335\n",
            "Ep 299: Train Loss: 0.009894, Val Loss: 0.014158\n",
            "Ep 300: Train Loss: 0.010192, Val Loss: 0.014464\n",
            "Ep 301: Train Loss: 0.010277, Val Loss: 0.014813\n",
            "Ep 302: Train Loss: 0.009466, Val Loss: 0.015028\n",
            "Ep 303: Train Loss: 0.010341, Val Loss: 0.014509\n",
            "Ep 304: Train Loss: 0.009822, Val Loss: 0.013669\n",
            "Ep 305: Train Loss: 0.009413, Val Loss: 0.014462\n",
            "Ep 306: Train Loss: 0.009493, Val Loss: 0.014109\n",
            "Ep 307: Train Loss: 0.009467, Val Loss: 0.014695\n",
            "Ep 308: Train Loss: 0.009724, Val Loss: 0.014957\n",
            "Ep 309: Train Loss: 0.009888, Val Loss: 0.015342\n",
            "Ep 310: Train Loss: 0.009872, Val Loss: 0.014604\n",
            "Ep 311: Train Loss: 0.010018, Val Loss: 0.015454\n",
            "Ep 312: Train Loss: 0.009512, Val Loss: 0.014220\n",
            "Ep 313: Train Loss: 0.009578, Val Loss: 0.014341\n",
            "Ep 314: Train Loss: 0.009566, Val Loss: 0.014550\n",
            "Ep 315: Train Loss: 0.009547, Val Loss: 0.015393\n",
            "Ep 316: Train Loss: 0.009575, Val Loss: 0.014737\n",
            "Ep 317: Train Loss: 0.010108, Val Loss: 0.016410\n",
            "Ep 318: Train Loss: 0.009682, Val Loss: 0.014535\n",
            "Ep 319: Train Loss: 0.009629, Val Loss: 0.014222\n",
            "Ep 320: Train Loss: 0.009507, Val Loss: 0.015345\n",
            "Ep 321: Train Loss: 0.009288, Val Loss: 0.014267\n",
            "Ep 322: Train Loss: 0.009550, Val Loss: 0.016144\n",
            "Ep 323: Train Loss: 0.010242, Val Loss: 0.014164\n",
            "Ep 324: Train Loss: 0.009761, Val Loss: 0.014913\n",
            "Ep 325: Train Loss: 0.009743, Val Loss: 0.014529\n",
            "Ep 326: Train Loss: 0.009443, Val Loss: 0.013607\n",
            "Ep 327: Train Loss: 0.009712, Val Loss: 0.014527\n",
            "Ep 328: Train Loss: 0.009530, Val Loss: 0.015121\n",
            "Ep 329: Train Loss: 0.009356, Val Loss: 0.013933\n",
            "Ep 330: Train Loss: 0.009411, Val Loss: 0.013661\n",
            "Ep 331: Train Loss: 0.009389, Val Loss: 0.014523\n",
            "Ep 332: Train Loss: 0.009064, Val Loss: 0.014653\n",
            "Ep 333: Train Loss: 0.010139, Val Loss: 0.015251\n",
            "Ep 334: Train Loss: 0.009840, Val Loss: 0.014668\n",
            "Ep 335: Train Loss: 0.009661, Val Loss: 0.013969\n",
            "Ep 336: Train Loss: 0.009684, Val Loss: 0.015019\n",
            "Ep 337: Train Loss: 0.009479, Val Loss: 0.014643\n",
            "Ep 338: Train Loss: 0.009523, Val Loss: 0.016799\n",
            "Ep 339: Train Loss: 0.009484, Val Loss: 0.013884\n",
            "Ep 340: Train Loss: 0.009841, Val Loss: 0.015401\n",
            "Ep 341: Train Loss: 0.009725, Val Loss: 0.014361\n",
            "Ep 342: Train Loss: 0.010346, Val Loss: 0.016094\n",
            "Ep 343: Train Loss: 0.009959, Val Loss: 0.015935\n",
            "Ep 344: Train Loss: 0.009355, Val Loss: 0.015391\n",
            "Ep 345: Train Loss: 0.010156, Val Loss: 0.014981\n",
            "Ep 346: Train Loss: 0.009301, Val Loss: 0.014279\n",
            "Ep 347: Train Loss: 0.009657, Val Loss: 0.013501\n",
            "Ep 348: Train Loss: 0.009193, Val Loss: 0.014344\n",
            "Ep 349: Train Loss: 0.009278, Val Loss: 0.014982\n",
            "Ep 350: Train Loss: 0.009881, Val Loss: 0.014600\n",
            "Ep 351: Train Loss: 0.009272, Val Loss: 0.014560\n",
            "Ep 352: Train Loss: 0.009652, Val Loss: 0.013902\n",
            "Ep 353: Train Loss: 0.009238, Val Loss: 0.014168\n",
            "Ep 354: Train Loss: 0.009506, Val Loss: 0.014358\n",
            "Ep 355: Train Loss: 0.009348, Val Loss: 0.015312\n",
            "Ep 356: Train Loss: 0.008896, Val Loss: 0.015175\n",
            "Ep 357: Train Loss: 0.009137, Val Loss: 0.014789\n",
            "Ep 358: Train Loss: 0.009094, Val Loss: 0.014791\n",
            "Ep 359: Train Loss: 0.009042, Val Loss: 0.014943\n",
            "Ep 360: Train Loss: 0.008721, Val Loss: 0.014496\n",
            "Ep 361: Train Loss: 0.009524, Val Loss: 0.013659\n",
            "Ep 362: Train Loss: 0.009127, Val Loss: 0.016106\n",
            "Ep 363: Train Loss: 0.009359, Val Loss: 0.016168\n",
            "Ep 364: Train Loss: 0.009522, Val Loss: 0.015428\n",
            "Ep 365: Train Loss: 0.009399, Val Loss: 0.015508\n",
            "Ep 366: Train Loss: 0.009796, Val Loss: 0.014936\n",
            "Ep 367: Train Loss: 0.010330, Val Loss: 0.014693\n",
            "Ep 368: Train Loss: 0.009877, Val Loss: 0.013801\n",
            "Ep 369: Train Loss: 0.008945, Val Loss: 0.014551\n",
            "Ep 370: Train Loss: 0.008957, Val Loss: 0.013562\n",
            "Ep 371: Train Loss: 0.009376, Val Loss: 0.014789\n",
            "Ep 372: Train Loss: 0.009747, Val Loss: 0.014387\n",
            "Ep 373: Train Loss: 0.008664, Val Loss: 0.014948\n",
            "Ep 374: Train Loss: 0.008937, Val Loss: 0.014049\n",
            "Ep 375: Train Loss: 0.009371, Val Loss: 0.015287\n",
            "Ep 376: Train Loss: 0.009556, Val Loss: 0.013975\n",
            "Ep 377: Train Loss: 0.009055, Val Loss: 0.014986\n",
            "Ep 378: Train Loss: 0.008692, Val Loss: 0.014452\n",
            "Ep 379: Train Loss: 0.009472, Val Loss: 0.013891\n",
            "Ep 380: Train Loss: 0.009473, Val Loss: 0.013683\n",
            "Ep 381: Train Loss: 0.009158, Val Loss: 0.014020\n",
            "Ep 382: Train Loss: 0.009235, Val Loss: 0.015015\n",
            "Ep 383: Train Loss: 0.009340, Val Loss: 0.014497\n",
            "Ep 384: Train Loss: 0.009044, Val Loss: 0.014375\n",
            "Ep 385: Train Loss: 0.009198, Val Loss: 0.013928\n",
            "Ep 386: Train Loss: 0.008889, Val Loss: 0.014087\n",
            "Ep 387: Train Loss: 0.009029, Val Loss: 0.013966\n",
            "Ep 388: Train Loss: 0.009237, Val Loss: 0.016234\n",
            "Ep 389: Train Loss: 0.009250, Val Loss: 0.015055\n",
            "Ep 390: Train Loss: 0.008602, Val Loss: 0.015946\n",
            "Ep 391: Train Loss: 0.008789, Val Loss: 0.013993\n",
            "Ep 392: Train Loss: 0.009071, Val Loss: 0.015067\n",
            "Ep 393: Train Loss: 0.009418, Val Loss: 0.013735\n",
            "Ep 394: Train Loss: 0.008786, Val Loss: 0.013325\n",
            "Ep 395: Train Loss: 0.009117, Val Loss: 0.014774\n",
            "Ep 396: Train Loss: 0.008722, Val Loss: 0.015433\n",
            "Ep 397: Train Loss: 0.008839, Val Loss: 0.013403\n",
            "Ep 398: Train Loss: 0.009031, Val Loss: 0.014516\n",
            "Ep 399: Train Loss: 0.009382, Val Loss: 0.013790\n",
            "Ep 400: Train Loss: 0.008969, Val Loss: 0.013517\n",
            "Ep 401: Train Loss: 0.008723, Val Loss: 0.014565\n",
            "Ep 402: Train Loss: 0.009115, Val Loss: 0.015944\n",
            "Ep 403: Train Loss: 0.009258, Val Loss: 0.013189\n",
            "Ep 404: Train Loss: 0.008629, Val Loss: 0.013341\n",
            "Ep 405: Train Loss: 0.008418, Val Loss: 0.013943\n",
            "Ep 406: Train Loss: 0.008594, Val Loss: 0.013815\n",
            "Ep 407: Train Loss: 0.008725, Val Loss: 0.014540\n",
            "Ep 408: Train Loss: 0.008942, Val Loss: 0.014044\n",
            "Ep 409: Train Loss: 0.009000, Val Loss: 0.014707\n",
            "Ep 410: Train Loss: 0.009540, Val Loss: 0.014523\n",
            "Ep 411: Train Loss: 0.009671, Val Loss: 0.013597\n",
            "Ep 412: Train Loss: 0.008401, Val Loss: 0.015058\n",
            "Ep 413: Train Loss: 0.008959, Val Loss: 0.014331\n",
            "Ep 414: Train Loss: 0.009080, Val Loss: 0.014254\n",
            "Ep 415: Train Loss: 0.009363, Val Loss: 0.013612\n",
            "Ep 416: Train Loss: 0.008898, Val Loss: 0.013843\n",
            "Ep 417: Train Loss: 0.008871, Val Loss: 0.015454\n",
            "Ep 418: Train Loss: 0.008373, Val Loss: 0.014472\n",
            "Ep 419: Train Loss: 0.008811, Val Loss: 0.015495\n",
            "Ep 420: Train Loss: 0.008804, Val Loss: 0.013950\n",
            "Ep 421: Train Loss: 0.008808, Val Loss: 0.014649\n",
            "Ep 422: Train Loss: 0.008982, Val Loss: 0.014784\n",
            "Ep 423: Train Loss: 0.008682, Val Loss: 0.013946\n",
            "Ep 424: Train Loss: 0.008719, Val Loss: 0.015361\n",
            "Ep 425: Train Loss: 0.008796, Val Loss: 0.014615\n",
            "Ep 426: Train Loss: 0.008722, Val Loss: 0.014205\n",
            "Ep 427: Train Loss: 0.008537, Val Loss: 0.013584\n",
            "Ep 428: Train Loss: 0.008690, Val Loss: 0.014215\n",
            "Ep 429: Train Loss: 0.008972, Val Loss: 0.013644\n",
            "Ep 430: Train Loss: 0.008522, Val Loss: 0.015268\n",
            "Ep 431: Train Loss: 0.008593, Val Loss: 0.013794\n",
            "Ep 432: Train Loss: 0.009179, Val Loss: 0.016015\n",
            "Ep 433: Train Loss: 0.009506, Val Loss: 0.014444\n",
            "Ep 434: Train Loss: 0.008892, Val Loss: 0.014357\n",
            "Ep 435: Train Loss: 0.008596, Val Loss: 0.013552\n",
            "Ep 436: Train Loss: 0.008659, Val Loss: 0.014363\n",
            "Ep 437: Train Loss: 0.008743, Val Loss: 0.014343\n",
            "Ep 438: Train Loss: 0.009225, Val Loss: 0.013411\n",
            "Ep 439: Train Loss: 0.009017, Val Loss: 0.014338\n",
            "Ep 440: Train Loss: 0.008513, Val Loss: 0.013526\n",
            "Ep 441: Train Loss: 0.008648, Val Loss: 0.014956\n",
            "Ep 442: Train Loss: 0.008812, Val Loss: 0.012825\n",
            "Ep 443: Train Loss: 0.009311, Val Loss: 0.016425\n",
            "Ep 444: Train Loss: 0.009569, Val Loss: 0.014828\n",
            "Ep 445: Train Loss: 0.008523, Val Loss: 0.014264\n",
            "Ep 446: Train Loss: 0.008539, Val Loss: 0.014312\n",
            "Ep 447: Train Loss: 0.008942, Val Loss: 0.015404\n",
            "Ep 448: Train Loss: 0.008820, Val Loss: 0.013364\n",
            "Ep 449: Train Loss: 0.008802, Val Loss: 0.014843\n",
            "Ep 450: Train Loss: 0.008473, Val Loss: 0.013281\n",
            "Ep 451: Train Loss: 0.008499, Val Loss: 0.013627\n",
            "Ep 452: Train Loss: 0.008547, Val Loss: 0.014749\n",
            "Ep 453: Train Loss: 0.008643, Val Loss: 0.014173\n",
            "Ep 454: Train Loss: 0.008869, Val Loss: 0.015277\n",
            "Ep 455: Train Loss: 0.008444, Val Loss: 0.014001\n",
            "Ep 456: Train Loss: 0.008147, Val Loss: 0.013083\n",
            "Ep 457: Train Loss: 0.008262, Val Loss: 0.014752\n",
            "Ep 458: Train Loss: 0.008556, Val Loss: 0.014145\n",
            "Ep 459: Train Loss: 0.007950, Val Loss: 0.013360\n",
            "Ep 460: Train Loss: 0.008127, Val Loss: 0.013880\n",
            "Ep 461: Train Loss: 0.008283, Val Loss: 0.015078\n",
            "Ep 462: Train Loss: 0.009027, Val Loss: 0.014350\n",
            "Ep 463: Train Loss: 0.008403, Val Loss: 0.014222\n",
            "Ep 464: Train Loss: 0.008509, Val Loss: 0.014122\n",
            "Ep 465: Train Loss: 0.008128, Val Loss: 0.015211\n",
            "Ep 466: Train Loss: 0.008514, Val Loss: 0.014040\n",
            "Ep 467: Train Loss: 0.008671, Val Loss: 0.014219\n",
            "Ep 468: Train Loss: 0.008915, Val Loss: 0.015699\n",
            "Ep 469: Train Loss: 0.008826, Val Loss: 0.014444\n",
            "Ep 470: Train Loss: 0.009154, Val Loss: 0.015278\n",
            "Ep 471: Train Loss: 0.008623, Val Loss: 0.014008\n",
            "Ep 472: Train Loss: 0.008885, Val Loss: 0.013945\n",
            "Ep 473: Train Loss: 0.008702, Val Loss: 0.015260\n",
            "Ep 474: Train Loss: 0.008462, Val Loss: 0.013771\n",
            "Ep 475: Train Loss: 0.008367, Val Loss: 0.012629\n",
            "Ep 476: Train Loss: 0.008579, Val Loss: 0.015115\n",
            "Ep 477: Train Loss: 0.008711, Val Loss: 0.014605\n",
            "Ep 478: Train Loss: 0.008232, Val Loss: 0.013469\n",
            "Ep 479: Train Loss: 0.008743, Val Loss: 0.014356\n",
            "Ep 480: Train Loss: 0.009248, Val Loss: 0.014595\n",
            "Ep 481: Train Loss: 0.008516, Val Loss: 0.014405\n",
            "Ep 482: Train Loss: 0.008776, Val Loss: 0.014701\n",
            "Ep 483: Train Loss: 0.008994, Val Loss: 0.013849\n",
            "Ep 484: Train Loss: 0.009241, Val Loss: 0.014772\n",
            "Ep 485: Train Loss: 0.008113, Val Loss: 0.014339\n",
            "Ep 486: Train Loss: 0.008054, Val Loss: 0.014948\n",
            "Ep 487: Train Loss: 0.008058, Val Loss: 0.013451\n",
            "Ep 488: Train Loss: 0.008328, Val Loss: 0.014177\n",
            "Ep 489: Train Loss: 0.008286, Val Loss: 0.013561\n",
            "Ep 490: Train Loss: 0.008672, Val Loss: 0.014094\n",
            "Ep 491: Train Loss: 0.008245, Val Loss: 0.014168\n",
            "Ep 492: Train Loss: 0.008313, Val Loss: 0.013490\n",
            "Ep 493: Train Loss: 0.008371, Val Loss: 0.014427\n",
            "Ep 494: Train Loss: 0.008336, Val Loss: 0.014886\n",
            "Ep 495: Train Loss: 0.008389, Val Loss: 0.013320\n",
            "Ep 496: Train Loss: 0.008054, Val Loss: 0.013741\n",
            "Ep 497: Train Loss: 0.008763, Val Loss: 0.014285\n",
            "Ep 498: Train Loss: 0.008151, Val Loss: 0.014063\n",
            "Ep 499: Train Loss: 0.008086, Val Loss: 0.014625\n",
            "Ep 500: Train Loss: 0.007977, Val Loss: 0.013330\n",
            "Loss plot saved to loss_plot_mp.png\n",
            "Losses saved to losses_mp.npz\n",
            "Model saved to model_mp.pth\n",
            "Start Training: GRF\n",
            "Ep 1: Train Loss: 0.692412, Val Loss: 0.246802\n",
            "Ep 2: Train Loss: 0.154921, Val Loss: 0.094091\n",
            "Ep 3: Train Loss: 0.070245, Val Loss: 0.054855\n",
            "Ep 4: Train Loss: 0.045640, Val Loss: 0.041238\n",
            "Ep 5: Train Loss: 0.034962, Val Loss: 0.034443\n",
            "Ep 6: Train Loss: 0.030224, Val Loss: 0.031256\n",
            "Ep 7: Train Loss: 0.027649, Val Loss: 0.029131\n",
            "Ep 8: Train Loss: 0.024827, Val Loss: 0.025831\n",
            "Ep 9: Train Loss: 0.023364, Val Loss: 0.024241\n",
            "Ep 10: Train Loss: 0.021786, Val Loss: 0.023442\n",
            "Ep 11: Train Loss: 0.020963, Val Loss: 0.022920\n",
            "Ep 12: Train Loss: 0.020392, Val Loss: 0.021992\n",
            "Ep 13: Train Loss: 0.019242, Val Loss: 0.023248\n",
            "Ep 14: Train Loss: 0.019301, Val Loss: 0.022088\n",
            "Ep 15: Train Loss: 0.020024, Val Loss: 0.020295\n",
            "Ep 16: Train Loss: 0.017890, Val Loss: 0.018956\n",
            "Ep 17: Train Loss: 0.016919, Val Loss: 0.019499\n",
            "Ep 18: Train Loss: 0.017283, Val Loss: 0.018495\n",
            "Ep 19: Train Loss: 0.016280, Val Loss: 0.018320\n",
            "Ep 20: Train Loss: 0.016083, Val Loss: 0.020073\n",
            "Ep 21: Train Loss: 0.016145, Val Loss: 0.017212\n",
            "Ep 22: Train Loss: 0.015807, Val Loss: 0.018880\n",
            "Ep 23: Train Loss: 0.016477, Val Loss: 0.017891\n",
            "Ep 24: Train Loss: 0.016468, Val Loss: 0.016608\n",
            "Ep 25: Train Loss: 0.015640, Val Loss: 0.018895\n",
            "Ep 26: Train Loss: 0.014740, Val Loss: 0.016995\n",
            "Ep 27: Train Loss: 0.015408, Val Loss: 0.016538\n",
            "Ep 28: Train Loss: 0.015253, Val Loss: 0.017121\n",
            "Ep 29: Train Loss: 0.015296, Val Loss: 0.016558\n",
            "Ep 30: Train Loss: 0.014718, Val Loss: 0.017989\n",
            "Ep 31: Train Loss: 0.015264, Val Loss: 0.016697\n",
            "Ep 32: Train Loss: 0.014994, Val Loss: 0.015750\n",
            "Ep 33: Train Loss: 0.014450, Val Loss: 0.017773\n",
            "Ep 34: Train Loss: 0.014516, Val Loss: 0.016406\n",
            "Ep 35: Train Loss: 0.014232, Val Loss: 0.015827\n",
            "Ep 36: Train Loss: 0.013495, Val Loss: 0.017446\n",
            "Ep 37: Train Loss: 0.014540, Val Loss: 0.019083\n",
            "Ep 38: Train Loss: 0.015660, Val Loss: 0.017970\n",
            "Ep 39: Train Loss: 0.013861, Val Loss: 0.017850\n",
            "Ep 40: Train Loss: 0.013794, Val Loss: 0.015812\n",
            "Ep 41: Train Loss: 0.013985, Val Loss: 0.017072\n",
            "Ep 42: Train Loss: 0.013338, Val Loss: 0.016600\n",
            "Ep 43: Train Loss: 0.014044, Val Loss: 0.015975\n",
            "Ep 44: Train Loss: 0.013677, Val Loss: 0.016166\n",
            "Ep 45: Train Loss: 0.013618, Val Loss: 0.017129\n",
            "Ep 46: Train Loss: 0.013443, Val Loss: 0.016295\n",
            "Ep 47: Train Loss: 0.013206, Val Loss: 0.017126\n",
            "Ep 48: Train Loss: 0.013470, Val Loss: 0.016680\n",
            "Ep 49: Train Loss: 0.013399, Val Loss: 0.017650\n",
            "Ep 50: Train Loss: 0.012950, Val Loss: 0.015753\n",
            "Ep 51: Train Loss: 0.013057, Val Loss: 0.016400\n",
            "Ep 52: Train Loss: 0.013206, Val Loss: 0.015689\n",
            "Ep 53: Train Loss: 0.012750, Val Loss: 0.016628\n",
            "Ep 54: Train Loss: 0.013018, Val Loss: 0.015808\n",
            "Ep 55: Train Loss: 0.012410, Val Loss: 0.015826\n",
            "Ep 56: Train Loss: 0.012593, Val Loss: 0.015406\n",
            "Ep 57: Train Loss: 0.012850, Val Loss: 0.016060\n",
            "Ep 58: Train Loss: 0.012597, Val Loss: 0.015949\n",
            "Ep 59: Train Loss: 0.013331, Val Loss: 0.015572\n",
            "Ep 60: Train Loss: 0.013382, Val Loss: 0.017169\n",
            "Ep 61: Train Loss: 0.012756, Val Loss: 0.015781\n",
            "Ep 62: Train Loss: 0.012243, Val Loss: 0.015596\n",
            "Ep 63: Train Loss: 0.012420, Val Loss: 0.015979\n",
            "Ep 64: Train Loss: 0.012192, Val Loss: 0.015024\n",
            "Ep 65: Train Loss: 0.012122, Val Loss: 0.015319\n",
            "Ep 66: Train Loss: 0.012070, Val Loss: 0.016130\n",
            "Ep 67: Train Loss: 0.013353, Val Loss: 0.016558\n",
            "Ep 68: Train Loss: 0.012548, Val Loss: 0.015672\n",
            "Ep 69: Train Loss: 0.012284, Val Loss: 0.014822\n",
            "Ep 70: Train Loss: 0.012508, Val Loss: 0.015951\n",
            "Ep 71: Train Loss: 0.012415, Val Loss: 0.015647\n",
            "Ep 72: Train Loss: 0.011987, Val Loss: 0.014601\n",
            "Ep 73: Train Loss: 0.012499, Val Loss: 0.016327\n",
            "Ep 74: Train Loss: 0.012000, Val Loss: 0.014982\n",
            "Ep 75: Train Loss: 0.011545, Val Loss: 0.015474\n",
            "Ep 76: Train Loss: 0.011840, Val Loss: 0.015283\n",
            "Ep 77: Train Loss: 0.011828, Val Loss: 0.015082\n",
            "Ep 78: Train Loss: 0.012170, Val Loss: 0.014756\n",
            "Ep 79: Train Loss: 0.011754, Val Loss: 0.015407\n",
            "Ep 80: Train Loss: 0.012108, Val Loss: 0.014447\n",
            "Ep 81: Train Loss: 0.011962, Val Loss: 0.015893\n",
            "Ep 82: Train Loss: 0.012616, Val Loss: 0.016081\n",
            "Ep 83: Train Loss: 0.012137, Val Loss: 0.016141\n",
            "Ep 84: Train Loss: 0.011598, Val Loss: 0.014540\n",
            "Ep 85: Train Loss: 0.012529, Val Loss: 0.015642\n",
            "Ep 86: Train Loss: 0.012040, Val Loss: 0.016271\n",
            "Ep 87: Train Loss: 0.011910, Val Loss: 0.013955\n",
            "Ep 88: Train Loss: 0.010918, Val Loss: 0.014122\n",
            "Ep 89: Train Loss: 0.011806, Val Loss: 0.015330\n",
            "Ep 90: Train Loss: 0.011870, Val Loss: 0.014754\n",
            "Ep 91: Train Loss: 0.012012, Val Loss: 0.016521\n",
            "Ep 92: Train Loss: 0.011261, Val Loss: 0.015237\n",
            "Ep 93: Train Loss: 0.011402, Val Loss: 0.015526\n",
            "Ep 94: Train Loss: 0.011022, Val Loss: 0.014163\n",
            "Ep 95: Train Loss: 0.010986, Val Loss: 0.014807\n",
            "Ep 96: Train Loss: 0.011239, Val Loss: 0.015656\n",
            "Ep 97: Train Loss: 0.011034, Val Loss: 0.014885\n",
            "Ep 98: Train Loss: 0.011074, Val Loss: 0.014555\n",
            "Ep 99: Train Loss: 0.011277, Val Loss: 0.015080\n",
            "Ep 100: Train Loss: 0.011361, Val Loss: 0.013453\n",
            "Ep 101: Train Loss: 0.012267, Val Loss: 0.014802\n",
            "Ep 102: Train Loss: 0.011983, Val Loss: 0.015809\n",
            "Ep 103: Train Loss: 0.011424, Val Loss: 0.016184\n",
            "Ep 104: Train Loss: 0.011122, Val Loss: 0.014928\n",
            "Ep 105: Train Loss: 0.010634, Val Loss: 0.013959\n",
            "Ep 106: Train Loss: 0.011283, Val Loss: 0.014434\n",
            "Ep 107: Train Loss: 0.011820, Val Loss: 0.014791\n",
            "Ep 108: Train Loss: 0.011594, Val Loss: 0.013785\n",
            "Ep 109: Train Loss: 0.010785, Val Loss: 0.013141\n",
            "Ep 110: Train Loss: 0.010575, Val Loss: 0.014256\n",
            "Ep 111: Train Loss: 0.010452, Val Loss: 0.013997\n",
            "Ep 112: Train Loss: 0.010624, Val Loss: 0.012875\n",
            "Ep 113: Train Loss: 0.010410, Val Loss: 0.013888\n",
            "Ep 114: Train Loss: 0.010248, Val Loss: 0.014247\n",
            "Ep 115: Train Loss: 0.010511, Val Loss: 0.015206\n",
            "Ep 116: Train Loss: 0.011067, Val Loss: 0.014229\n",
            "Ep 117: Train Loss: 0.011489, Val Loss: 0.015954\n",
            "Ep 118: Train Loss: 0.010856, Val Loss: 0.014057\n",
            "Ep 119: Train Loss: 0.010257, Val Loss: 0.014023\n",
            "Ep 120: Train Loss: 0.011172, Val Loss: 0.015129\n",
            "Ep 121: Train Loss: 0.012130, Val Loss: 0.014676\n",
            "Ep 122: Train Loss: 0.011007, Val Loss: 0.013772\n",
            "Ep 123: Train Loss: 0.010579, Val Loss: 0.013636\n",
            "Ep 124: Train Loss: 0.010403, Val Loss: 0.013278\n",
            "Ep 125: Train Loss: 0.010324, Val Loss: 0.013414\n",
            "Ep 126: Train Loss: 0.010748, Val Loss: 0.013316\n",
            "Ep 127: Train Loss: 0.010767, Val Loss: 0.015029\n",
            "Ep 128: Train Loss: 0.010807, Val Loss: 0.014821\n",
            "Ep 129: Train Loss: 0.010303, Val Loss: 0.014354\n",
            "Ep 130: Train Loss: 0.010607, Val Loss: 0.014237\n",
            "Ep 131: Train Loss: 0.010636, Val Loss: 0.014602\n",
            "Ep 132: Train Loss: 0.010089, Val Loss: 0.016526\n",
            "Ep 133: Train Loss: 0.010989, Val Loss: 0.013308\n",
            "Ep 134: Train Loss: 0.010313, Val Loss: 0.014175\n",
            "Ep 135: Train Loss: 0.010176, Val Loss: 0.013572\n",
            "Ep 136: Train Loss: 0.009758, Val Loss: 0.013905\n",
            "Ep 137: Train Loss: 0.010631, Val Loss: 0.013318\n",
            "Ep 138: Train Loss: 0.010124, Val Loss: 0.013981\n",
            "Ep 139: Train Loss: 0.010211, Val Loss: 0.013924\n",
            "Ep 140: Train Loss: 0.010709, Val Loss: 0.015392\n",
            "Ep 141: Train Loss: 0.010569, Val Loss: 0.014043\n",
            "Ep 142: Train Loss: 0.010354, Val Loss: 0.013351\n",
            "Ep 143: Train Loss: 0.009903, Val Loss: 0.013417\n",
            "Ep 144: Train Loss: 0.009857, Val Loss: 0.014759\n",
            "Ep 145: Train Loss: 0.010490, Val Loss: 0.013110\n",
            "Ep 146: Train Loss: 0.010168, Val Loss: 0.014676\n",
            "Ep 147: Train Loss: 0.010295, Val Loss: 0.013494\n",
            "Ep 148: Train Loss: 0.010252, Val Loss: 0.013171\n",
            "Ep 149: Train Loss: 0.010344, Val Loss: 0.014988\n",
            "Ep 150: Train Loss: 0.009400, Val Loss: 0.013577\n",
            "Ep 151: Train Loss: 0.009704, Val Loss: 0.013796\n",
            "Ep 152: Train Loss: 0.010837, Val Loss: 0.014665\n",
            "Ep 153: Train Loss: 0.010291, Val Loss: 0.013432\n",
            "Ep 154: Train Loss: 0.010195, Val Loss: 0.013645\n",
            "Ep 155: Train Loss: 0.011081, Val Loss: 0.015376\n",
            "Ep 156: Train Loss: 0.010527, Val Loss: 0.014124\n",
            "Ep 157: Train Loss: 0.009910, Val Loss: 0.013378\n",
            "Ep 158: Train Loss: 0.010304, Val Loss: 0.013156\n",
            "Ep 159: Train Loss: 0.010215, Val Loss: 0.013646\n",
            "Ep 160: Train Loss: 0.009421, Val Loss: 0.013014\n",
            "Ep 161: Train Loss: 0.008904, Val Loss: 0.011496\n",
            "Ep 162: Train Loss: 0.009028, Val Loss: 0.015022\n",
            "Ep 163: Train Loss: 0.009297, Val Loss: 0.016188\n",
            "Ep 164: Train Loss: 0.009524, Val Loss: 0.013705\n",
            "Ep 165: Train Loss: 0.009078, Val Loss: 0.014124\n",
            "Ep 166: Train Loss: 0.009343, Val Loss: 0.013050\n",
            "Ep 167: Train Loss: 0.009053, Val Loss: 0.012900\n",
            "Ep 168: Train Loss: 0.009278, Val Loss: 0.016358\n",
            "Ep 169: Train Loss: 0.010446, Val Loss: 0.013513\n",
            "Ep 170: Train Loss: 0.009791, Val Loss: 0.013776\n",
            "Ep 171: Train Loss: 0.009704, Val Loss: 0.013347\n",
            "Ep 172: Train Loss: 0.009007, Val Loss: 0.012840\n",
            "Ep 173: Train Loss: 0.008800, Val Loss: 0.012338\n",
            "Ep 174: Train Loss: 0.009067, Val Loss: 0.013465\n",
            "Ep 175: Train Loss: 0.009963, Val Loss: 0.012903\n",
            "Ep 176: Train Loss: 0.009213, Val Loss: 0.013220\n",
            "Ep 177: Train Loss: 0.008803, Val Loss: 0.012099\n",
            "Ep 178: Train Loss: 0.008797, Val Loss: 0.012560\n",
            "Ep 179: Train Loss: 0.008847, Val Loss: 0.012966\n",
            "Ep 180: Train Loss: 0.008874, Val Loss: 0.012454\n",
            "Ep 181: Train Loss: 0.008695, Val Loss: 0.012998\n",
            "Ep 182: Train Loss: 0.008882, Val Loss: 0.012694\n",
            "Ep 183: Train Loss: 0.008520, Val Loss: 0.012994\n",
            "Ep 184: Train Loss: 0.008331, Val Loss: 0.013429\n",
            "Ep 185: Train Loss: 0.009389, Val Loss: 0.014801\n",
            "Ep 186: Train Loss: 0.010546, Val Loss: 0.012539\n",
            "Ep 187: Train Loss: 0.009258, Val Loss: 0.013223\n",
            "Ep 188: Train Loss: 0.008727, Val Loss: 0.012015\n",
            "Ep 189: Train Loss: 0.008890, Val Loss: 0.013110\n",
            "Ep 190: Train Loss: 0.008962, Val Loss: 0.013352\n",
            "Ep 191: Train Loss: 0.008878, Val Loss: 0.013030\n",
            "Ep 192: Train Loss: 0.010021, Val Loss: 0.013089\n",
            "Ep 193: Train Loss: 0.009487, Val Loss: 0.013591\n",
            "Ep 194: Train Loss: 0.009311, Val Loss: 0.014556\n",
            "Ep 195: Train Loss: 0.009456, Val Loss: 0.013312\n",
            "Ep 196: Train Loss: 0.009173, Val Loss: 0.013169\n",
            "Ep 197: Train Loss: 0.008580, Val Loss: 0.012064\n",
            "Ep 198: Train Loss: 0.009199, Val Loss: 0.012769\n",
            "Ep 199: Train Loss: 0.008666, Val Loss: 0.012921\n",
            "Ep 200: Train Loss: 0.008596, Val Loss: 0.012229\n",
            "Ep 201: Train Loss: 0.008598, Val Loss: 0.012258\n",
            "Ep 202: Train Loss: 0.008454, Val Loss: 0.011973\n",
            "Ep 203: Train Loss: 0.008370, Val Loss: 0.011724\n",
            "Ep 204: Train Loss: 0.008499, Val Loss: 0.012260\n",
            "Ep 205: Train Loss: 0.008456, Val Loss: 0.012558\n",
            "Ep 206: Train Loss: 0.008370, Val Loss: 0.012151\n",
            "Ep 207: Train Loss: 0.008570, Val Loss: 0.013050\n",
            "Ep 208: Train Loss: 0.009017, Val Loss: 0.012027\n",
            "Ep 209: Train Loss: 0.008621, Val Loss: 0.013127\n",
            "Ep 210: Train Loss: 0.008754, Val Loss: 0.012350\n",
            "Ep 211: Train Loss: 0.008965, Val Loss: 0.013736\n",
            "Ep 212: Train Loss: 0.008460, Val Loss: 0.012585\n",
            "Ep 213: Train Loss: 0.008660, Val Loss: 0.012238\n",
            "Ep 214: Train Loss: 0.008458, Val Loss: 0.011810\n",
            "Ep 215: Train Loss: 0.008606, Val Loss: 0.012112\n",
            "Ep 216: Train Loss: 0.008926, Val Loss: 0.012058\n",
            "Ep 217: Train Loss: 0.008796, Val Loss: 0.012754\n",
            "Ep 218: Train Loss: 0.008588, Val Loss: 0.012099\n",
            "Ep 219: Train Loss: 0.008449, Val Loss: 0.011723\n",
            "Ep 220: Train Loss: 0.008138, Val Loss: 0.011273\n",
            "Ep 221: Train Loss: 0.008521, Val Loss: 0.012125\n",
            "Ep 222: Train Loss: 0.008609, Val Loss: 0.012184\n",
            "Ep 223: Train Loss: 0.008644, Val Loss: 0.012032\n",
            "Ep 224: Train Loss: 0.008759, Val Loss: 0.012315\n",
            "Ep 225: Train Loss: 0.009143, Val Loss: 0.012900\n",
            "Ep 226: Train Loss: 0.009711, Val Loss: 0.013836\n",
            "Ep 227: Train Loss: 0.010751, Val Loss: 0.014391\n",
            "Ep 228: Train Loss: 0.009205, Val Loss: 0.012837\n",
            "Ep 229: Train Loss: 0.009072, Val Loss: 0.013236\n",
            "Ep 230: Train Loss: 0.008648, Val Loss: 0.011800\n",
            "Ep 231: Train Loss: 0.008436, Val Loss: 0.011800\n",
            "Ep 232: Train Loss: 0.008462, Val Loss: 0.012109\n",
            "Ep 233: Train Loss: 0.008602, Val Loss: 0.012129\n",
            "Ep 234: Train Loss: 0.008410, Val Loss: 0.012527\n",
            "Ep 235: Train Loss: 0.008264, Val Loss: 0.012347\n",
            "Ep 236: Train Loss: 0.008786, Val Loss: 0.012873\n",
            "Ep 237: Train Loss: 0.008856, Val Loss: 0.011917\n",
            "Ep 238: Train Loss: 0.008615, Val Loss: 0.012228\n",
            "Ep 239: Train Loss: 0.008476, Val Loss: 0.012071\n",
            "Ep 240: Train Loss: 0.008256, Val Loss: 0.012299\n",
            "Ep 241: Train Loss: 0.008689, Val Loss: 0.012386\n",
            "Ep 242: Train Loss: 0.010404, Val Loss: 0.014516\n",
            "Ep 243: Train Loss: 0.010272, Val Loss: 0.013730\n",
            "Ep 244: Train Loss: 0.009211, Val Loss: 0.013362\n",
            "Ep 245: Train Loss: 0.008767, Val Loss: 0.012324\n",
            "Ep 246: Train Loss: 0.008645, Val Loss: 0.012030\n",
            "Ep 247: Train Loss: 0.008107, Val Loss: 0.011947\n",
            "Ep 248: Train Loss: 0.008448, Val Loss: 0.011813\n",
            "Ep 249: Train Loss: 0.008427, Val Loss: 0.012272\n",
            "Ep 250: Train Loss: 0.008457, Val Loss: 0.011723\n",
            "Ep 251: Train Loss: 0.007977, Val Loss: 0.012010\n",
            "Ep 252: Train Loss: 0.008204, Val Loss: 0.012260\n",
            "Ep 253: Train Loss: 0.008075, Val Loss: 0.011652\n",
            "Ep 254: Train Loss: 0.008035, Val Loss: 0.012276\n",
            "Ep 255: Train Loss: 0.007946, Val Loss: 0.011627\n",
            "Ep 256: Train Loss: 0.008746, Val Loss: 0.013028\n",
            "Ep 257: Train Loss: 0.008297, Val Loss: 0.012688\n",
            "Ep 258: Train Loss: 0.008761, Val Loss: 0.012923\n",
            "Ep 259: Train Loss: 0.009517, Val Loss: 0.013531\n",
            "Ep 260: Train Loss: 0.008848, Val Loss: 0.012495\n",
            "Ep 261: Train Loss: 0.008392, Val Loss: 0.012569\n",
            "Ep 262: Train Loss: 0.008333, Val Loss: 0.011949\n",
            "Ep 263: Train Loss: 0.008398, Val Loss: 0.012339\n",
            "Ep 264: Train Loss: 0.007879, Val Loss: 0.011809\n",
            "Ep 265: Train Loss: 0.008093, Val Loss: 0.012627\n",
            "Ep 266: Train Loss: 0.008565, Val Loss: 0.012199\n",
            "Ep 267: Train Loss: 0.008458, Val Loss: 0.011748\n",
            "Ep 268: Train Loss: 0.008424, Val Loss: 0.012244\n",
            "Ep 269: Train Loss: 0.009187, Val Loss: 0.011819\n",
            "Ep 270: Train Loss: 0.008433, Val Loss: 0.012687\n",
            "Ep 271: Train Loss: 0.008905, Val Loss: 0.012133\n",
            "Ep 272: Train Loss: 0.008315, Val Loss: 0.012397\n",
            "Ep 273: Train Loss: 0.008153, Val Loss: 0.011938\n",
            "Ep 274: Train Loss: 0.008146, Val Loss: 0.012017\n",
            "Ep 275: Train Loss: 0.008051, Val Loss: 0.011786\n",
            "Ep 276: Train Loss: 0.008046, Val Loss: 0.011870\n",
            "Ep 277: Train Loss: 0.007904, Val Loss: 0.012254\n",
            "Ep 278: Train Loss: 0.008411, Val Loss: 0.012468\n",
            "Ep 279: Train Loss: 0.008001, Val Loss: 0.011967\n",
            "Ep 280: Train Loss: 0.007677, Val Loss: 0.011468\n",
            "Ep 281: Train Loss: 0.008191, Val Loss: 0.013509\n",
            "Ep 282: Train Loss: 0.008377, Val Loss: 0.012089\n",
            "Ep 283: Train Loss: 0.008115, Val Loss: 0.011242\n",
            "Ep 284: Train Loss: 0.007978, Val Loss: 0.011734\n",
            "Ep 285: Train Loss: 0.007940, Val Loss: 0.012313\n",
            "Ep 286: Train Loss: 0.007857, Val Loss: 0.013038\n",
            "Ep 287: Train Loss: 0.007937, Val Loss: 0.011752\n",
            "Ep 288: Train Loss: 0.007895, Val Loss: 0.012510\n",
            "Ep 289: Train Loss: 0.007597, Val Loss: 0.011266\n",
            "Ep 290: Train Loss: 0.007855, Val Loss: 0.011788\n",
            "Ep 291: Train Loss: 0.007696, Val Loss: 0.012748\n",
            "Ep 292: Train Loss: 0.008866, Val Loss: 0.012269\n",
            "Ep 293: Train Loss: 0.008583, Val Loss: 0.013023\n",
            "Ep 294: Train Loss: 0.008107, Val Loss: 0.011587\n",
            "Ep 295: Train Loss: 0.007884, Val Loss: 0.011721\n",
            "Ep 296: Train Loss: 0.007649, Val Loss: 0.012181\n",
            "Ep 297: Train Loss: 0.008129, Val Loss: 0.011407\n",
            "Ep 298: Train Loss: 0.008247, Val Loss: 0.012524\n",
            "Ep 299: Train Loss: 0.008115, Val Loss: 0.011967\n",
            "Ep 300: Train Loss: 0.008458, Val Loss: 0.012654\n",
            "Ep 301: Train Loss: 0.008520, Val Loss: 0.011344\n",
            "Ep 302: Train Loss: 0.007923, Val Loss: 0.012825\n",
            "Ep 303: Train Loss: 0.007620, Val Loss: 0.012505\n",
            "Ep 304: Train Loss: 0.007910, Val Loss: 0.012865\n",
            "Ep 305: Train Loss: 0.007924, Val Loss: 0.011431\n",
            "Ep 306: Train Loss: 0.007823, Val Loss: 0.012652\n",
            "Ep 307: Train Loss: 0.007934, Val Loss: 0.010981\n",
            "Ep 308: Train Loss: 0.007991, Val Loss: 0.013154\n",
            "Ep 309: Train Loss: 0.007915, Val Loss: 0.012389\n",
            "Ep 310: Train Loss: 0.007573, Val Loss: 0.011688\n",
            "Ep 311: Train Loss: 0.008144, Val Loss: 0.012265\n",
            "Ep 312: Train Loss: 0.008263, Val Loss: 0.012723\n",
            "Ep 313: Train Loss: 0.008755, Val Loss: 0.012376\n",
            "Ep 314: Train Loss: 0.008639, Val Loss: 0.012220\n",
            "Ep 315: Train Loss: 0.008121, Val Loss: 0.012325\n",
            "Ep 316: Train Loss: 0.007924, Val Loss: 0.012041\n",
            "Ep 317: Train Loss: 0.007969, Val Loss: 0.011862\n",
            "Ep 318: Train Loss: 0.008305, Val Loss: 0.012586\n",
            "Ep 319: Train Loss: 0.008443, Val Loss: 0.012990\n",
            "Ep 320: Train Loss: 0.008140, Val Loss: 0.011679\n",
            "Ep 321: Train Loss: 0.007959, Val Loss: 0.012072\n",
            "Ep 322: Train Loss: 0.007567, Val Loss: 0.011011\n",
            "Ep 323: Train Loss: 0.007631, Val Loss: 0.011827\n",
            "Ep 324: Train Loss: 0.007600, Val Loss: 0.011419\n",
            "Ep 325: Train Loss: 0.007906, Val Loss: 0.012317\n",
            "Ep 326: Train Loss: 0.008175, Val Loss: 0.012246\n",
            "Ep 327: Train Loss: 0.007879, Val Loss: 0.011183\n",
            "Ep 328: Train Loss: 0.007764, Val Loss: 0.011081\n",
            "Ep 329: Train Loss: 0.007968, Val Loss: 0.011707\n",
            "Ep 330: Train Loss: 0.008323, Val Loss: 0.011661\n",
            "Ep 331: Train Loss: 0.007820, Val Loss: 0.011594\n",
            "Ep 332: Train Loss: 0.008790, Val Loss: 0.018642\n",
            "Ep 333: Train Loss: 0.013481, Val Loss: 0.016278\n",
            "Ep 334: Train Loss: 0.012119, Val Loss: 0.014356\n",
            "Ep 335: Train Loss: 0.009196, Val Loss: 0.012482\n",
            "Ep 336: Train Loss: 0.009358, Val Loss: 0.013741\n",
            "Ep 337: Train Loss: 0.010121, Val Loss: 0.014108\n",
            "Ep 338: Train Loss: 0.009353, Val Loss: 0.012386\n",
            "Ep 339: Train Loss: 0.009465, Val Loss: 0.012528\n",
            "Ep 340: Train Loss: 0.008797, Val Loss: 0.012655\n",
            "Ep 341: Train Loss: 0.008136, Val Loss: 0.011651\n",
            "Ep 342: Train Loss: 0.007826, Val Loss: 0.012444\n",
            "Ep 343: Train Loss: 0.007974, Val Loss: 0.013759\n",
            "Ep 344: Train Loss: 0.009715, Val Loss: 0.012713\n",
            "Ep 345: Train Loss: 0.008916, Val Loss: 0.013132\n",
            "Ep 346: Train Loss: 0.008013, Val Loss: 0.011281\n",
            "Ep 347: Train Loss: 0.008229, Val Loss: 0.012599\n",
            "Ep 348: Train Loss: 0.008384, Val Loss: 0.012692\n",
            "Ep 349: Train Loss: 0.007791, Val Loss: 0.011896\n",
            "Ep 350: Train Loss: 0.007894, Val Loss: 0.012023\n",
            "Ep 351: Train Loss: 0.007743, Val Loss: 0.011802\n",
            "Ep 352: Train Loss: 0.008096, Val Loss: 0.012791\n",
            "Ep 353: Train Loss: 0.008162, Val Loss: 0.011743\n",
            "Ep 354: Train Loss: 0.008082, Val Loss: 0.012325\n",
            "Ep 355: Train Loss: 0.007676, Val Loss: 0.011376\n",
            "Ep 356: Train Loss: 0.007494, Val Loss: 0.011588\n",
            "Ep 357: Train Loss: 0.007616, Val Loss: 0.011888\n",
            "Ep 358: Train Loss: 0.007702, Val Loss: 0.011318\n",
            "Ep 359: Train Loss: 0.008245, Val Loss: 0.012938\n",
            "Ep 360: Train Loss: 0.008380, Val Loss: 0.011771\n",
            "Ep 361: Train Loss: 0.007877, Val Loss: 0.012381\n",
            "Ep 362: Train Loss: 0.007801, Val Loss: 0.011927\n",
            "Ep 363: Train Loss: 0.007957, Val Loss: 0.011790\n",
            "Ep 364: Train Loss: 0.007654, Val Loss: 0.011301\n",
            "Ep 365: Train Loss: 0.007504, Val Loss: 0.011923\n",
            "Ep 366: Train Loss: 0.007647, Val Loss: 0.011400\n",
            "Ep 367: Train Loss: 0.007735, Val Loss: 0.011949\n",
            "Ep 368: Train Loss: 0.008262, Val Loss: 0.012250\n",
            "Ep 369: Train Loss: 0.008110, Val Loss: 0.011302\n",
            "Ep 370: Train Loss: 0.007586, Val Loss: 0.011720\n",
            "Ep 371: Train Loss: 0.007857, Val Loss: 0.012438\n",
            "Ep 372: Train Loss: 0.008603, Val Loss: 0.012007\n",
            "Ep 373: Train Loss: 0.008091, Val Loss: 0.011909\n",
            "Ep 374: Train Loss: 0.007895, Val Loss: 0.011715\n",
            "Ep 375: Train Loss: 0.007779, Val Loss: 0.011302\n",
            "Ep 376: Train Loss: 0.007746, Val Loss: 0.012157\n",
            "Ep 377: Train Loss: 0.007783, Val Loss: 0.011728\n",
            "Ep 378: Train Loss: 0.007809, Val Loss: 0.011483\n",
            "Ep 379: Train Loss: 0.007437, Val Loss: 0.011662\n",
            "Ep 380: Train Loss: 0.007712, Val Loss: 0.011975\n",
            "Ep 381: Train Loss: 0.007802, Val Loss: 0.011761\n",
            "Ep 382: Train Loss: 0.007556, Val Loss: 0.011263\n",
            "Ep 383: Train Loss: 0.007496, Val Loss: 0.011876\n",
            "Ep 384: Train Loss: 0.007478, Val Loss: 0.011356\n",
            "Ep 385: Train Loss: 0.007670, Val Loss: 0.011689\n",
            "Ep 386: Train Loss: 0.008405, Val Loss: 0.011640\n",
            "Ep 387: Train Loss: 0.007697, Val Loss: 0.011474\n",
            "Ep 388: Train Loss: 0.007918, Val Loss: 0.011923\n",
            "Ep 389: Train Loss: 0.007406, Val Loss: 0.011707\n",
            "Ep 390: Train Loss: 0.007703, Val Loss: 0.011644\n",
            "Ep 391: Train Loss: 0.007484, Val Loss: 0.010798\n",
            "Ep 392: Train Loss: 0.007372, Val Loss: 0.011620\n",
            "Ep 393: Train Loss: 0.007378, Val Loss: 0.011544\n",
            "Ep 394: Train Loss: 0.007699, Val Loss: 0.010959\n",
            "Ep 395: Train Loss: 0.007546, Val Loss: 0.011918\n",
            "Ep 396: Train Loss: 0.007460, Val Loss: 0.012056\n",
            "Ep 397: Train Loss: 0.007904, Val Loss: 0.011677\n",
            "Ep 398: Train Loss: 0.007576, Val Loss: 0.012182\n",
            "Ep 399: Train Loss: 0.008000, Val Loss: 0.011747\n",
            "Ep 400: Train Loss: 0.007975, Val Loss: 0.012514\n",
            "Ep 401: Train Loss: 0.008170, Val Loss: 0.011250\n",
            "Ep 402: Train Loss: 0.008805, Val Loss: 0.013140\n",
            "Ep 403: Train Loss: 0.008996, Val Loss: 0.012344\n",
            "Ep 404: Train Loss: 0.007856, Val Loss: 0.011113\n",
            "Ep 405: Train Loss: 0.007583, Val Loss: 0.011673\n",
            "Ep 406: Train Loss: 0.007504, Val Loss: 0.011433\n",
            "Ep 407: Train Loss: 0.007632, Val Loss: 0.010860\n",
            "Ep 408: Train Loss: 0.007961, Val Loss: 0.012550\n",
            "Ep 409: Train Loss: 0.007857, Val Loss: 0.011544\n",
            "Ep 410: Train Loss: 0.007332, Val Loss: 0.011419\n",
            "Ep 411: Train Loss: 0.007574, Val Loss: 0.011239\n",
            "Ep 412: Train Loss: 0.007373, Val Loss: 0.011452\n",
            "Ep 413: Train Loss: 0.007458, Val Loss: 0.011766\n",
            "Ep 414: Train Loss: 0.007796, Val Loss: 0.011633\n",
            "Ep 415: Train Loss: 0.007764, Val Loss: 0.011831\n",
            "Ep 416: Train Loss: 0.007600, Val Loss: 0.010982\n",
            "Ep 417: Train Loss: 0.007588, Val Loss: 0.011184\n",
            "Ep 418: Train Loss: 0.007415, Val Loss: 0.010726\n",
            "Ep 419: Train Loss: 0.007396, Val Loss: 0.011187\n",
            "Ep 420: Train Loss: 0.007768, Val Loss: 0.010893\n",
            "Ep 421: Train Loss: 0.007778, Val Loss: 0.010737\n",
            "Ep 422: Train Loss: 0.007308, Val Loss: 0.011468\n",
            "Ep 423: Train Loss: 0.007448, Val Loss: 0.010817\n",
            "Ep 424: Train Loss: 0.007072, Val Loss: 0.011260\n",
            "Ep 425: Train Loss: 0.007634, Val Loss: 0.011077\n",
            "Ep 426: Train Loss: 0.007345, Val Loss: 0.011283\n",
            "Ep 427: Train Loss: 0.007418, Val Loss: 0.011602\n",
            "Ep 428: Train Loss: 0.007487, Val Loss: 0.011198\n",
            "Ep 429: Train Loss: 0.007795, Val Loss: 0.011223\n",
            "Ep 430: Train Loss: 0.007705, Val Loss: 0.011435\n",
            "Ep 431: Train Loss: 0.008180, Val Loss: 0.011431\n",
            "Ep 432: Train Loss: 0.009235, Val Loss: 0.013475\n",
            "Ep 433: Train Loss: 0.009130, Val Loss: 0.011340\n",
            "Ep 434: Train Loss: 0.008352, Val Loss: 0.011214\n",
            "Ep 435: Train Loss: 0.007981, Val Loss: 0.011663\n",
            "Ep 436: Train Loss: 0.007582, Val Loss: 0.011359\n",
            "Ep 437: Train Loss: 0.007587, Val Loss: 0.011654\n",
            "Ep 438: Train Loss: 0.008345, Val Loss: 0.012421\n",
            "Ep 439: Train Loss: 0.007842, Val Loss: 0.011115\n",
            "Ep 440: Train Loss: 0.008218, Val Loss: 0.011429\n",
            "Ep 441: Train Loss: 0.007991, Val Loss: 0.011239\n",
            "Ep 442: Train Loss: 0.007651, Val Loss: 0.011428\n",
            "Ep 443: Train Loss: 0.007690, Val Loss: 0.011101\n",
            "Ep 444: Train Loss: 0.007569, Val Loss: 0.011461\n",
            "Ep 445: Train Loss: 0.007518, Val Loss: 0.010724\n",
            "Ep 446: Train Loss: 0.007384, Val Loss: 0.011058\n",
            "Ep 447: Train Loss: 0.008092, Val Loss: 0.011309\n",
            "Ep 448: Train Loss: 0.007774, Val Loss: 0.011896\n",
            "Ep 449: Train Loss: 0.007522, Val Loss: 0.011934\n",
            "Ep 450: Train Loss: 0.007808, Val Loss: 0.011354\n",
            "Ep 451: Train Loss: 0.007456, Val Loss: 0.010618\n",
            "Ep 452: Train Loss: 0.007394, Val Loss: 0.011036\n",
            "Ep 453: Train Loss: 0.007431, Val Loss: 0.011163\n",
            "Ep 454: Train Loss: 0.007684, Val Loss: 0.011841\n",
            "Ep 455: Train Loss: 0.008029, Val Loss: 0.012361\n",
            "Ep 456: Train Loss: 0.007524, Val Loss: 0.012106\n",
            "Ep 457: Train Loss: 0.007456, Val Loss: 0.010919\n",
            "Ep 458: Train Loss: 0.007270, Val Loss: 0.010937\n",
            "Ep 459: Train Loss: 0.007319, Val Loss: 0.010682\n",
            "Ep 460: Train Loss: 0.007298, Val Loss: 0.011240\n",
            "Ep 461: Train Loss: 0.007215, Val Loss: 0.011351\n",
            "Ep 462: Train Loss: 0.007459, Val Loss: 0.010896\n",
            "Ep 463: Train Loss: 0.007557, Val Loss: 0.010894\n",
            "Ep 464: Train Loss: 0.008325, Val Loss: 0.011219\n",
            "Ep 465: Train Loss: 0.007606, Val Loss: 0.011250\n",
            "Ep 466: Train Loss: 0.007256, Val Loss: 0.010865\n",
            "Ep 467: Train Loss: 0.007365, Val Loss: 0.011584\n",
            "Ep 468: Train Loss: 0.007137, Val Loss: 0.010462\n",
            "Ep 469: Train Loss: 0.007161, Val Loss: 0.011002\n",
            "Ep 470: Train Loss: 0.007141, Val Loss: 0.011468\n",
            "Ep 471: Train Loss: 0.007417, Val Loss: 0.011136\n",
            "Ep 472: Train Loss: 0.007573, Val Loss: 0.010760\n",
            "Ep 473: Train Loss: 0.007793, Val Loss: 0.011076\n",
            "Ep 474: Train Loss: 0.007306, Val Loss: 0.010865\n",
            "Ep 475: Train Loss: 0.007329, Val Loss: 0.011225\n",
            "Ep 476: Train Loss: 0.007247, Val Loss: 0.010603\n",
            "Ep 477: Train Loss: 0.007480, Val Loss: 0.010801\n",
            "Ep 478: Train Loss: 0.007197, Val Loss: 0.011470\n",
            "Ep 479: Train Loss: 0.007584, Val Loss: 0.011611\n",
            "Ep 480: Train Loss: 0.007480, Val Loss: 0.011061\n",
            "Ep 481: Train Loss: 0.007142, Val Loss: 0.010723\n",
            "Ep 482: Train Loss: 0.007105, Val Loss: 0.010703\n",
            "Ep 483: Train Loss: 0.007453, Val Loss: 0.011089\n",
            "Ep 484: Train Loss: 0.007166, Val Loss: 0.010981\n",
            "Ep 485: Train Loss: 0.007626, Val Loss: 0.012200\n",
            "Ep 486: Train Loss: 0.007544, Val Loss: 0.011559\n",
            "Ep 487: Train Loss: 0.007157, Val Loss: 0.010534\n",
            "Ep 488: Train Loss: 0.006983, Val Loss: 0.011030\n",
            "Ep 489: Train Loss: 0.007056, Val Loss: 0.010675\n",
            "Ep 490: Train Loss: 0.007162, Val Loss: 0.010731\n",
            "Ep 491: Train Loss: 0.007208, Val Loss: 0.011390\n",
            "Ep 492: Train Loss: 0.007314, Val Loss: 0.011767\n",
            "Ep 493: Train Loss: 0.007549, Val Loss: 0.011453\n",
            "Ep 494: Train Loss: 0.007046, Val Loss: 0.010593\n",
            "Ep 495: Train Loss: 0.007251, Val Loss: 0.010143\n",
            "Ep 496: Train Loss: 0.007606, Val Loss: 0.010798\n",
            "Ep 497: Train Loss: 0.007587, Val Loss: 0.010633\n",
            "Ep 498: Train Loss: 0.007142, Val Loss: 0.010768\n",
            "Ep 499: Train Loss: 0.007668, Val Loss: 0.012455\n",
            "Ep 500: Train Loss: 0.008228, Val Loss: 0.011884\n",
            "Loss plot saved to loss_plot_grf.png\n",
            "Losses saved to losses_grf.npz\n",
            "Model saved to model_grf.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_MODE = 'grf'  # 'grf', 'baseline', 'mp'\n",
        "DATA_PATH = \"./data/\"\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-3 # Learning Rate\n",
        "EPOCHS = 500\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "K_NEIGHBORS = 6\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "# --- DATASET ---\n",
        "class RobotArmDataset(Dataset):\n",
        "    def __init__(self, points_path, knn_path):\n",
        "        if not os.path.exists(points_path):\n",
        "            raise FileNotFoundError(f\"File {points_path} not found.\")\n",
        "        self.points = np.load(points_path).astype(np.float32)\n",
        "        self.knn = np.load(knn_path).astype(np.int64)\n",
        "\n",
        "        # Normalization stats\n",
        "        self.mean = np.mean(self.points, axis=(0, 1))\n",
        "        self.std = np.std(self.points, axis=(0, 1))\n",
        "        self.points = (self.points - self.mean) / (self.std + 1e-6)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.points) - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(self.points[idx]), \\\n",
        "               torch.from_numpy(self.knn[idx]), \\\n",
        "               torch.from_numpy(self.points[idx + 1])\n",
        "\n",
        "# --- MODELS ---\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k = torch.relu(q) + 1e-6, torch.relu(k) + 1e-6\n",
        "        kv = torch.einsum(\"bnd,bne->bde\", k, v)\n",
        "        z = 1 / (torch.einsum(\"bnd,bd->bn\", q, k.sum(dim=1)) + 1e-6)\n",
        "        attn = torch.einsum(\"bnd,bde,bn->bne\", q, kv, z)\n",
        "        return self.to_out(attn)\n",
        "\n",
        "class TopologicalGRFLayer(nn.Module):\n",
        "    def __init__(self, dim, k_neighbors, hops=3):\n",
        "        super().__init__()\n",
        "        self.k = k_neighbors\n",
        "        self.hops = hops\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, knn_idx):\n",
        "        B, N, D = x.shape\n",
        "        # Sparse Matrix Construction\n",
        "        src = torch.arange(N, device=x.device).view(1, N, 1).expand(B, N, self.k)\n",
        "        batch_off = torch.arange(B, device=x.device).view(B, 1, 1) * N\n",
        "        indices = torch.stack([(knn_idx + batch_off).view(-1), (src + batch_off).view(-1)])\n",
        "        values = torch.ones(indices.shape[1], device=x.device)\n",
        "        adj = torch.sparse_coo_tensor(indices, values, (B*N, B*N))\n",
        "\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        v_f, k_f = v.view(B*N, D), k.view(B*N, D)\n",
        "\n",
        "        # Random Walk Diffusion\n",
        "        for _ in range(self.hops):\n",
        "            v_f = torch.sparse.mm(adj, v_f) / (self.k + 1e-6)\n",
        "            k_f = torch.sparse.mm(adj, k_f) / (self.k + 1e-6)\n",
        "\n",
        "        attn = (q * k_f.view(B, N, D)).sum(dim=-1, keepdim=True)\n",
        "        return self.to_out(attn * v_f.view(B, N, D))\n",
        "\n",
        "class SimpleMessagePassing(nn.Module):\n",
        "    def __init__(self, dim, k_neighbors):\n",
        "        super().__init__()\n",
        "        self.k = k_neighbors\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, knn_idx):\n",
        "        B, N, D = x.shape\n",
        "        flat_idx = knn_idx.reshape(B, N * self.k).unsqueeze(-1).expand(-1, -1, D)\n",
        "        neighbors = torch.gather(x, 1, flat_idx.reshape(B, N * self.k, D).long()).reshape(B, N, self.k, D)\n",
        "        return self.proj(neighbors.mean(dim=2))\n",
        "\n",
        "class UnifiedInterlacer(nn.Module):\n",
        "    def __init__(self, mode='grf', input_dim=3, embed_dim=64):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        if mode == 'grf':\n",
        "            self.l1 = TopologicalGRFLayer(embed_dim, K_NEIGHBORS)\n",
        "            self.l3 = TopologicalGRFLayer(embed_dim, K_NEIGHBORS)\n",
        "        elif mode == 'mp':\n",
        "            self.l1 = SimpleMessagePassing(embed_dim, K_NEIGHBORS)\n",
        "            self.l3 = SimpleMessagePassing(embed_dim, K_NEIGHBORS)\n",
        "        else:\n",
        "            self.l1 = nn.Identity()\n",
        "            self.l3 = nn.Identity()\n",
        "\n",
        "        self.l2 = LinearAttention(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, 3)\n",
        "\n",
        "    def forward(self, x, knn):\n",
        "        h = self.embedding(x)\n",
        "        h = h + (self.l1(self.norm1(h), knn) if self.mode != 'baseline' else self.norm1(h))\n",
        "        h = h + self.l2(self.norm2(h))\n",
        "        h = h + (self.l3(self.norm3(h), knn) if self.mode != 'baseline' else self.norm3(h))\n",
        "        return self.head(h)\n",
        "\n",
        "# --- MAIN ---\n",
        "def main():\n",
        "    dataset = RobotArmDataset(\"points.npy\", \"knn_indices.npy\")\n",
        "    train_size = int(len(dataset) * (1 - VAL_SPLIT))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = UnifiedInterlacer(mode=MODEL_MODE).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Track losses\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(f\"Start Training: {MODEL_MODE.upper()}\")\n",
        "    for ep in range(EPOCHS):\n",
        "        # Training\n",
        "        model.train()\n",
        "        epoch_train_losses = []\n",
        "        for x, knn, y in train_loader:\n",
        "            x, knn, y = x.to(DEVICE), knn.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x, knn)\n",
        "            loss = criterion(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_losses.append(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        epoch_val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for x, knn, y in val_loader:\n",
        "                x, knn, y = x.to(DEVICE), knn.to(DEVICE), y.to(DEVICE)\n",
        "                pred = model(x, knn)\n",
        "                loss = criterion(pred, y)\n",
        "                epoch_val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(epoch_train_losses)\n",
        "        val_loss = np.mean(epoch_val_losses)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Ep {ep+1}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    # Plot and save loss curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss', marker='o')\n",
        "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss', marker='s')\n",
        "    plt.title(f'Training and Validation Loss - {MODEL_MODE.upper()}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.savefig(f'loss_plot_{MODEL_MODE}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Loss plot saved to loss_plot_{MODEL_MODE}.png\")\n",
        "\n",
        "    # Save losses to numpy file\n",
        "    np.savez(f'losses_{MODEL_MODE}.npz',\n",
        "             train_losses=np.array(train_losses),\n",
        "             val_losses=np.array(val_losses))\n",
        "    print(f\"Losses saved to losses_{MODEL_MODE}.npz\")\n",
        "\n",
        "    # SAVE MODEL WEIGHTS (include losses in checkpoint)\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'mean': dataset.mean,\n",
        "        'std': dataset.std,\n",
        "        'mode': MODEL_MODE,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses\n",
        "    }, f\"model_{MODEL_MODE}.pth\")\n",
        "    print(f\"Model saved to model_{MODEL_MODE}.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.chdir(DATA_PATH)\n",
        "\n",
        "    MODEL_MODE = 'baseline'\n",
        "    main()\n",
        "    MODEL_MODE = 'mp'\n",
        "    main()\n",
        "    MODEL_MODE = 'grf'\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
