{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14123377,"sourceType":"datasetVersion","datasetId":8998183}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# import os\n# drive.mount('/content/drive')\n\n# os.chdir('/content/drive/MyDrive/gdl')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uxl9dgG0gEOI","outputId":"5fdc0504-57b2-4125-a6fb-f28cd86c5c46","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:05:09.442104Z","iopub.execute_input":"2025-12-12T09:05:09.442797Z","iopub.status.idle":"2025-12-12T09:05:09.445866Z","shell.execute_reply.started":"2025-12-12T09:05:09.442773Z","shell.execute_reply":"2025-12-12T09:05:09.445147Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport gc  # Add garbage collector\n\n# --- CONFIGURATION ---\nMODEL_MODE = 'grf'  # 'grf', 'baseline', 'mp'\nDATA_PATH = \"/kaggle/input/2048-05/data\"\nOUTPUT_PATH = \"/kaggle/working/\"\npoints_file = os.path.join(DATA_PATH, \"points.npy\")\nknn_file = os.path.join(DATA_PATH, \"knn_indices.npy\")\n\nBATCH_SIZE = 16\nLR = 1e-3 # Learning Rate\nEPOCHS = 100\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {DEVICE}\")\nK_NEIGHBORS = 6\nVAL_SPLIT = 0.2\nTRAIN_ROLLOUT_STEPS = 3  # Number of autoregressive steps during training\n\n# --- DYNAMIC KNN FUNCTION ---\ndef compute_knn_torch(points, k):\n    \"\"\"Compute KNN dynamically on GPU - works with batched tensors\"\"\"\n    B, N, D = points.shape\n    # Compute pairwise distances\n    dist_mat = torch.cdist(points, points, p=2)\n    # Get k+1 nearest neighbors (includes self), then exclude self\n    _, knn_indices = torch.topk(dist_mat, k=k+1, dim=-1, largest=False)\n    return knn_indices[:, :, 1:]  # Shape: (B, N, k)\n\n# --- DATASET ---\nclass RobotArmDataset(Dataset):\n    def __init__(self, points_path, knn_path, rollout_steps=1):\n        if not os.path.exists(points_path):\n            raise FileNotFoundError(f\"File {points_path} not found.\")\n        self.points = np.load(points_path).astype(np.float32)\n        self.knn = np.load(knn_path).astype(np.int64)\n        self.rollout_steps = rollout_steps\n\n        # Normalization stats\n        self.mean = np.mean(self.points, axis=(0, 1))\n        self.std = np.std(self.points, axis=(0, 1))\n        self.points = (self.points - self.mean) / (self.std + 1e-6)\n\n    def __len__(self):\n        # Account for needing rollout_steps future frames\n        return len(self.points) - self.rollout_steps\n\n    def __getitem__(self, idx):\n        # Return input, knn, and sequence of rollout_steps targets\n        input_frame = torch.from_numpy(self.points[idx])\n        input_knn = torch.from_numpy(self.knn[idx])\n        # Stack future targets: [t+1, t+2, ..., t+rollout_steps]\n        targets = torch.stack([\n            torch.from_numpy(self.points[idx + i + 1])\n            for i in range(self.rollout_steps)\n        ])  # Shape: (rollout_steps, N, 3)\n        return input_frame, input_knn, targets\n\n# --- MODELS ---\nclass LinearAttention(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.to_out = nn.Linear(dim, dim)\n\n    def forward(self, x):\n        B, N, D = x.shape\n        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n        q, k = torch.relu(q) + 1e-6, torch.relu(k) + 1e-6\n        kv = torch.einsum(\"bnd,bne->bde\", k, v)\n        z = 1 / (torch.einsum(\"bnd,bd->bn\", q, k.sum(dim=1)) + 1e-6)\n        attn = torch.einsum(\"bnd,bde,bn->bne\", q, kv, z)\n        return self.to_out(attn)\n\nclass TopologicalGRFLayer(nn.Module):\n    def __init__(self, dim, k_neighbors, hops=5):\n        super().__init__()\n        self.k = k_neighbors\n        self.hops = hops\n        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.to_out = nn.Linear(dim, dim)\n\n    def forward(self, x, knn_idx):\n        B, N, D = x.shape\n        # Sparse Matrix Construction\n        src = torch.arange(N, device=x.device).view(1, N, 1).expand(B, N, self.k)\n        batch_off = torch.arange(B, device=x.device).view(B, 1, 1) * N\n        indices = torch.stack([(knn_idx + batch_off).view(-1), (src + batch_off).view(-1)])\n        values = torch.ones(indices.shape[1], device=x.device)\n        adj = torch.sparse_coo_tensor(indices, values, (B*N, B*N))\n\n        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n        v_f, k_f = v.view(B*N, D), k.view(B*N, D)\n\n        # Random Walk Diffusion\n        for _ in range(self.hops):\n            v_f = torch.sparse.mm(adj, v_f) / (self.k + 1e-6)\n            k_f = torch.sparse.mm(adj, k_f) / (self.k + 1e-6)\n\n        attn = (q * k_f.view(B, N, D)).sum(dim=-1, keepdim=True)\n        return self.to_out(attn * v_f.view(B, N, D))\n\nclass SimpleMessagePassing(nn.Module):\n    def __init__(self, dim, k_neighbors):\n        super().__init__()\n        self.k = k_neighbors\n        self.proj = nn.Linear(dim, dim)\n\n    def forward(self, x, knn_idx):\n        B, N, D = x.shape\n        flat_idx = knn_idx.reshape(B, N * self.k).unsqueeze(-1).expand(-1, -1, D)\n        neighbors = torch.gather(x, 1, flat_idx.reshape(B, N * self.k, D).long()).reshape(B, N, self.k, D)\n        return self.proj(neighbors.mean(dim=2))\n\nclass UnifiedInterlacer(nn.Module):\n    def __init__(self, mode='grf', input_dim=3, embed_dim=128, num_layers=5):\n        super().__init__()\n        self.mode = mode\n        self.num_layers = num_layers\n        self.embedding = nn.Linear(input_dim, embed_dim)\n\n        # Create layer norms for each layer (2 per block: graph + attention)\n        self.norms = nn.ModuleList([nn.LayerNorm(embed_dim) for _ in range(num_layers * 2)])\n\n        # Create graph layers (GRF, MP, or Identity based on mode)\n        if mode == 'grf':\n            self.graph_layers = nn.ModuleList([TopologicalGRFLayer(embed_dim, K_NEIGHBORS) for _ in range(num_layers)])\n        elif mode == 'mp':\n            self.graph_layers = nn.ModuleList([SimpleMessagePassing(embed_dim, K_NEIGHBORS) for _ in range(num_layers)])\n        else:\n            self.graph_layers = nn.ModuleList([nn.Identity() for _ in range(num_layers)])\n\n        # Create attention layers\n        self.attn_layers = nn.ModuleList([LinearAttention(embed_dim) for _ in range(num_layers)])\n\n        self.head = nn.Linear(embed_dim, 3)\n\n    def forward(self, x, knn):\n        h = self.embedding(x)\n\n        for i in range(self.num_layers):\n            # Graph layer\n            if self.mode != 'baseline':\n                h = h + self.graph_layers[i](self.norms[i * 2](h), knn)\n            else:\n                h = h + self.norms[i * 2](h)\n\n            # Attention layer\n            h = h + self.attn_layers[i](self.norms[i * 2 + 1](h))\n\n        return self.head(h)\n\n# --- MAIN ---\ndef main():\n    dataset = RobotArmDataset(points_file, knn_file, rollout_steps=TRAIN_ROLLOUT_STEPS)    \n    train_size = int(len(dataset) * (1 - VAL_SPLIT))\n    val_size = len(dataset) - train_size\n    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = UnifiedInterlacer(mode=MODEL_MODE).to(DEVICE)\n    optimizer = optim.AdamW(model.parameters(), lr=LR)\n    criterion = nn.MSELoss()\n\n    # Track losses\n    train_losses = []\n    val_losses = []\n\n    print(f\"Start Training: {MODEL_MODE.upper()} with {TRAIN_ROLLOUT_STEPS}-step rollout\")\n    for ep in range(EPOCHS):\n        # Training with multi-step rollout\n        model.train()\n        epoch_train_losses = []\n        for x, knn, targets in train_loader:\n            # x: (B, N, 3), knn: (B, N, K), targets: (B, ROLLOUT_STEPS, N, 3)\n            x, knn, targets = x.to(DEVICE), knn.to(DEVICE), targets.to(DEVICE)\n            optimizer.zero_grad()\n\n            # Multi-step rollout (BPTT - no detach)\n            current_input = x\n            current_knn = knn\n            step_losses = []\n\n            for step in range(TRAIN_ROLLOUT_STEPS):\n                # Predict next frame\n                pred = model(current_input, current_knn)\n\n                # Loss against ground truth target at this step\n                gt_target = targets[:, step]  # (B, N, 3)\n                step_loss = criterion(pred, gt_target)\n                step_losses.append(step_loss)\n\n                # CLOSED LOOP: prediction becomes next input (NO detach for BPTT)\n                current_input = pred\n\n                # DYNAMIC KNN: recompute graph on predicted coordinates\n                current_knn = compute_knn_torch(current_input, K_NEIGHBORS)\n\n            # Total loss = average across all steps\n            total_loss = torch.stack(step_losses).mean()\n            total_loss.backward()\n            optimizer.step()\n            epoch_train_losses.append(total_loss.item())\n\n        # Validation with same rollout\n        model.eval()\n        epoch_val_losses = []\n        with torch.no_grad():\n            for x, knn, targets in val_loader:\n                x, knn, targets = x.to(DEVICE), knn.to(DEVICE), targets.to(DEVICE)\n\n                current_input = x\n                current_knn = knn\n                step_losses = []\n\n                for step in range(TRAIN_ROLLOUT_STEPS):\n                    pred = model(current_input, current_knn)\n                    gt_target = targets[:, step]\n                    step_loss = criterion(pred, gt_target)\n                    step_losses.append(step_loss)\n\n                    current_input = pred\n                    current_knn = compute_knn_torch(current_input, K_NEIGHBORS)\n\n                total_loss = torch.stack(step_losses).mean()\n                epoch_val_losses.append(total_loss.item())\n\n        train_loss = np.mean(epoch_train_losses)\n        val_loss = np.mean(epoch_val_losses)\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n\n        print(f\"Ep {ep+1}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n\n    # Plot and save loss curves\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\n    plt.title(f'Training and Validation Loss - {MODEL_MODE.upper()} ({TRAIN_ROLLOUT_STEPS}-step rollout)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss (MSE)')\n    plt.yscale('log')\n    plt.legend()\n    plt.grid(True, linestyle='--', alpha=0.6)\n    plt.savefig(f'{OUTPUT_PATH}loss_plot_{MODEL_MODE}.png', dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"Loss plot saved to loss_plot_{MODEL_MODE}.png\")\n\n    # Save losses to numpy file\n    np.savez(f'losses_{MODEL_MODE}.npz',\n             train_losses=np.array(train_losses),\n             val_losses=np.array(val_losses))\n    print(f\"Losses saved to losses_{MODEL_MODE}.npz\")\n    print()\n\n    # SAVE MODEL WEIGHTS\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'mean': dataset.mean,\n        'std': dataset.std,\n        'mode': MODEL_MODE,\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'rollout_steps': TRAIN_ROLLOUT_STEPS\n    }, f\"model_{MODEL_MODE}.pth\")\n    print(f\"Model saved to model_{MODEL_MODE}.pth\")\n\nif __name__ == \"__main__\":\n    # os.chdir(DATA_PATH)\n\n    for mode in ['baseline', 'mp', 'grf']:\n        MODEL_MODE = mode\n        print(f\"\\n{'='*50}\")\n        print(f\"Starting training for: {MODEL_MODE.upper()}\")\n        print(f\"{'='*50}\\n\")\n\n        main()\n\n        # --- AGGRESSIVE CLEANUP ---\n        # Clear all cached memory on GPU\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            torch.cuda.synchronize()\n\n        # Force Python garbage collection\n        gc.collect()\n\n        print(f\"\\n[Cleanup] GPU cache cleared after {MODEL_MODE.upper()} training.\\n\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6gdrR-qsgE-0","outputId":"ea8d4878-7743-48cd-af0e-beb190fbc8cf","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T09:05:09.531924Z","iopub.execute_input":"2025-12-12T09:05:09.532458Z","iopub.status.idle":"2025-12-12T09:51:41.400986Z","shell.execute_reply.started":"2025-12-12T09:05:09.532440Z","shell.execute_reply":"2025-12-12T09:51:41.400375Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n==================================================\nStarting training for: BASELINE\n==================================================\n\nStart Training: BASELINE with 3-step rollout\nEp 1: Train Loss: 2.403096, Val Loss: 1.076808\nEp 2: Train Loss: 0.768041, Val Loss: 0.585249\nEp 3: Train Loss: 0.624871, Val Loss: 0.538094\nEp 4: Train Loss: 0.602643, Val Loss: 0.737923\nEp 5: Train Loss: 0.611915, Val Loss: 0.533165\nEp 6: Train Loss: 0.605377, Val Loss: 0.537227\nEp 7: Train Loss: 0.537521, Val Loss: 0.569992\nEp 8: Train Loss: 0.547285, Val Loss: 0.475704\nEp 9: Train Loss: 0.518516, Val Loss: 0.449390\nEp 10: Train Loss: 0.534691, Val Loss: 0.474970\nEp 11: Train Loss: 0.590989, Val Loss: 0.596900\nEp 12: Train Loss: 0.556778, Val Loss: 0.475736\nEp 13: Train Loss: 0.532612, Val Loss: 0.555064\nEp 14: Train Loss: 0.509206, Val Loss: 0.458896\nEp 15: Train Loss: 0.481694, Val Loss: 0.416977\nEp 16: Train Loss: 0.488519, Val Loss: 0.427656\nEp 17: Train Loss: 0.525754, Val Loss: 0.572062\nEp 18: Train Loss: 0.537801, Val Loss: 0.485516\nEp 19: Train Loss: 0.520662, Val Loss: 0.459222\nEp 20: Train Loss: 0.498987, Val Loss: 0.460861\nEp 21: Train Loss: 0.474169, Val Loss: 0.457638\nEp 22: Train Loss: 0.468196, Val Loss: 0.516434\nEp 23: Train Loss: 0.515567, Val Loss: 0.440357\nEp 24: Train Loss: 0.501719, Val Loss: 0.436577\nEp 25: Train Loss: 0.491478, Val Loss: 0.490789\nEp 26: Train Loss: 0.503447, Val Loss: 0.577916\nEp 27: Train Loss: 0.463763, Val Loss: 0.463306\nEp 28: Train Loss: 0.494418, Val Loss: 0.476326\nEp 29: Train Loss: 0.509711, Val Loss: 0.493226\nEp 30: Train Loss: 0.477468, Val Loss: 0.436697\nEp 31: Train Loss: 0.461836, Val Loss: 0.439554\nEp 32: Train Loss: 0.485355, Val Loss: 0.440462\nEp 33: Train Loss: 0.488948, Val Loss: 0.410982\nEp 34: Train Loss: 0.502251, Val Loss: 0.467420\nEp 35: Train Loss: 0.516448, Val Loss: 0.500366\nEp 36: Train Loss: 0.463563, Val Loss: 0.404781\nEp 37: Train Loss: 0.450949, Val Loss: 0.538022\nEp 38: Train Loss: 0.484310, Val Loss: 0.471607\nEp 39: Train Loss: 0.442853, Val Loss: 0.489435\nEp 40: Train Loss: 0.488087, Val Loss: 0.483613\nEp 41: Train Loss: 0.481562, Val Loss: 0.442479\nEp 42: Train Loss: 0.458523, Val Loss: 0.404444\nEp 43: Train Loss: 0.430526, Val Loss: 0.464959\nEp 44: Train Loss: 0.503731, Val Loss: 0.500714\nEp 45: Train Loss: 0.453557, Val Loss: 0.405969\nEp 46: Train Loss: 0.433346, Val Loss: 0.498091\nEp 47: Train Loss: 0.463518, Val Loss: 0.380412\nEp 48: Train Loss: 0.422847, Val Loss: 0.400149\nEp 49: Train Loss: 0.449632, Val Loss: 0.435971\nEp 50: Train Loss: 0.472079, Val Loss: 0.416575\nEp 51: Train Loss: 0.408865, Val Loss: 0.417282\nEp 52: Train Loss: 0.442052, Val Loss: 0.483950\nEp 53: Train Loss: 0.430487, Val Loss: 0.368472\nEp 54: Train Loss: 0.414005, Val Loss: 0.494885\nEp 55: Train Loss: 0.476640, Val Loss: 0.426313\nEp 56: Train Loss: 0.416403, Val Loss: 0.424734\nEp 57: Train Loss: 0.466683, Val Loss: 0.403549\nEp 58: Train Loss: 0.436918, Val Loss: 0.382026\nEp 59: Train Loss: 0.425055, Val Loss: 0.397501\nEp 60: Train Loss: 0.440124, Val Loss: 0.423642\nEp 61: Train Loss: 0.489974, Val Loss: 0.419505\nEp 62: Train Loss: 0.431249, Val Loss: 0.425540\nEp 63: Train Loss: 0.414609, Val Loss: 0.347715\nEp 64: Train Loss: 0.418723, Val Loss: 0.450053\nEp 65: Train Loss: 0.446757, Val Loss: 0.449272\nEp 66: Train Loss: 0.390153, Val Loss: 0.385944\nEp 67: Train Loss: 0.417539, Val Loss: 0.334718\nEp 68: Train Loss: 0.385140, Val Loss: 0.381195\nEp 69: Train Loss: 0.437470, Val Loss: 0.465996\nEp 70: Train Loss: 0.460485, Val Loss: 0.410820\nEp 71: Train Loss: 0.421439, Val Loss: 0.409055\nEp 72: Train Loss: 0.448528, Val Loss: 0.469317\nEp 73: Train Loss: 0.478836, Val Loss: 0.422511\nEp 74: Train Loss: 0.420437, Val Loss: 0.425834\nEp 75: Train Loss: 0.390193, Val Loss: 0.393542\nEp 76: Train Loss: 0.384772, Val Loss: 0.385799\nEp 77: Train Loss: 0.382590, Val Loss: 0.328178\nEp 78: Train Loss: 0.387077, Val Loss: 0.430443\nEp 79: Train Loss: 0.480453, Val Loss: 0.490596\nEp 80: Train Loss: 0.452351, Val Loss: 0.375492\nEp 81: Train Loss: 0.468132, Val Loss: 0.448548\nEp 82: Train Loss: 0.387825, Val Loss: 0.383409\nEp 83: Train Loss: 0.384056, Val Loss: 0.542730\nEp 84: Train Loss: 0.391813, Val Loss: 0.340512\nEp 85: Train Loss: 0.415148, Val Loss: 0.503706\nEp 86: Train Loss: 0.448761, Val Loss: 0.366873\nEp 87: Train Loss: 0.366074, Val Loss: 0.377052\nEp 88: Train Loss: 0.422492, Val Loss: 0.385638\nEp 89: Train Loss: 0.431215, Val Loss: 0.376105\nEp 90: Train Loss: 0.389118, Val Loss: 0.419667\nEp 91: Train Loss: 0.387296, Val Loss: 0.357033\nEp 92: Train Loss: 0.385123, Val Loss: 0.366794\nEp 93: Train Loss: 0.413507, Val Loss: 0.363848\nEp 94: Train Loss: 0.386423, Val Loss: 0.414992\nEp 95: Train Loss: 0.399893, Val Loss: 0.408376\nEp 96: Train Loss: 0.352368, Val Loss: 0.329134\nEp 97: Train Loss: 0.373625, Val Loss: 0.369501\nEp 98: Train Loss: 0.371802, Val Loss: 0.344755\nEp 99: Train Loss: 0.406694, Val Loss: 0.508605\nEp 100: Train Loss: 0.432589, Val Loss: 0.417086\nLoss plot saved to loss_plot_baseline.png\nLosses saved to losses_baseline.npz\n\nModel saved to model_baseline.pth\n\n[Cleanup] GPU cache cleared after BASELINE training.\n\n\n==================================================\nStarting training for: MP\n==================================================\n\nStart Training: MP with 3-step rollout\nEp 1: Train Loss: 1.653912, Val Loss: 0.704934\nEp 2: Train Loss: 0.673172, Val Loss: 0.574231\nEp 3: Train Loss: 0.544501, Val Loss: 0.502779\nEp 4: Train Loss: 0.521632, Val Loss: 0.642838\nEp 5: Train Loss: 0.551794, Val Loss: 0.494972\nEp 6: Train Loss: 0.522717, Val Loss: 0.569901\nEp 7: Train Loss: 0.531928, Val Loss: 0.531321\nEp 8: Train Loss: 0.525460, Val Loss: 0.634064\nEp 9: Train Loss: 0.499772, Val Loss: 0.506254\nEp 10: Train Loss: 0.507906, Val Loss: 0.474266\nEp 11: Train Loss: 0.472352, Val Loss: 0.532966\nEp 12: Train Loss: 0.478512, Val Loss: 0.470593\nEp 13: Train Loss: 0.464895, Val Loss: 0.468126\nEp 14: Train Loss: 0.488895, Val Loss: 0.493651\nEp 15: Train Loss: 0.468525, Val Loss: 0.589582\nEp 16: Train Loss: 0.501124, Val Loss: 0.499247\nEp 17: Train Loss: 0.468257, Val Loss: 0.488255\nEp 18: Train Loss: 0.477657, Val Loss: 0.476933\nEp 19: Train Loss: 0.440690, Val Loss: 0.452083\nEp 20: Train Loss: 0.418839, Val Loss: 0.481457\nEp 21: Train Loss: 0.435018, Val Loss: 0.436624\nEp 22: Train Loss: 0.422929, Val Loss: 0.546247\nEp 23: Train Loss: 0.431928, Val Loss: 0.442430\nEp 24: Train Loss: 0.413590, Val Loss: 0.464994\nEp 25: Train Loss: 0.408858, Val Loss: 0.443061\nEp 26: Train Loss: 0.428282, Val Loss: 0.536325\nEp 27: Train Loss: 0.420045, Val Loss: 0.425072\nEp 28: Train Loss: 0.416733, Val Loss: 0.468711\nEp 29: Train Loss: 0.430918, Val Loss: 0.465150\nEp 30: Train Loss: 0.437647, Val Loss: 0.435171\nEp 31: Train Loss: 0.432992, Val Loss: 0.489296\nEp 32: Train Loss: 0.399253, Val Loss: 0.434145\nEp 33: Train Loss: 0.389291, Val Loss: 0.475751\nEp 34: Train Loss: 0.393383, Val Loss: 0.439159\nEp 35: Train Loss: 0.380665, Val Loss: 0.439110\nEp 36: Train Loss: 0.370409, Val Loss: 0.413949\nEp 37: Train Loss: 0.364191, Val Loss: 0.455758\nEp 38: Train Loss: 0.416579, Val Loss: 0.469771\nEp 39: Train Loss: 0.407865, Val Loss: 0.379633\nEp 40: Train Loss: 0.366059, Val Loss: 0.395269\nEp 41: Train Loss: 0.347061, Val Loss: 0.391333\nEp 42: Train Loss: 0.380497, Val Loss: 0.414929\nEp 43: Train Loss: 0.354009, Val Loss: 0.418186\nEp 44: Train Loss: 0.368708, Val Loss: 0.426031\nEp 45: Train Loss: 0.355858, Val Loss: 0.418196\nEp 46: Train Loss: 0.372117, Val Loss: 0.434637\nEp 47: Train Loss: 0.350272, Val Loss: 0.373409\nEp 48: Train Loss: 0.359657, Val Loss: 0.438414\nEp 49: Train Loss: 0.355333, Val Loss: 0.383625\nEp 50: Train Loss: 0.347745, Val Loss: 0.400640\nEp 51: Train Loss: 0.387831, Val Loss: 0.434951\nEp 52: Train Loss: 0.359191, Val Loss: 0.350885\nEp 53: Train Loss: 0.361072, Val Loss: 0.405459\nEp 54: Train Loss: 0.344013, Val Loss: 0.412862\nEp 55: Train Loss: 0.333956, Val Loss: 0.465735\nEp 56: Train Loss: 0.339213, Val Loss: 0.426936\nEp 57: Train Loss: 0.365990, Val Loss: 0.402453\nEp 58: Train Loss: 0.355807, Val Loss: 0.435164\nEp 59: Train Loss: 0.346026, Val Loss: 0.404637\nEp 60: Train Loss: 0.337919, Val Loss: 0.374423\nEp 61: Train Loss: 0.357110, Val Loss: 0.361173\nEp 62: Train Loss: 0.304379, Val Loss: 0.388163\nEp 63: Train Loss: 0.323657, Val Loss: 0.374340\nEp 64: Train Loss: 0.312515, Val Loss: 0.397702\nEp 65: Train Loss: 0.325197, Val Loss: 0.403710\nEp 66: Train Loss: 0.336776, Val Loss: 0.395372\nEp 67: Train Loss: 0.346091, Val Loss: 0.407065\nEp 68: Train Loss: 0.323877, Val Loss: 0.407130\nEp 69: Train Loss: 0.333141, Val Loss: 0.364147\nEp 70: Train Loss: 0.301841, Val Loss: 0.368333\nEp 71: Train Loss: 0.344839, Val Loss: 0.427345\nEp 72: Train Loss: 0.317134, Val Loss: 0.327018\nEp 73: Train Loss: 0.287387, Val Loss: 0.337152\nEp 74: Train Loss: 0.271800, Val Loss: 0.362007\nEp 75: Train Loss: 0.284267, Val Loss: 0.374380\nEp 76: Train Loss: 0.277097, Val Loss: 0.334533\nEp 77: Train Loss: 0.342632, Val Loss: 0.416593\nEp 78: Train Loss: 0.318718, Val Loss: 0.374253\nEp 79: Train Loss: 0.323087, Val Loss: 0.402105\nEp 80: Train Loss: 0.302513, Val Loss: 0.405398\nEp 81: Train Loss: 0.317850, Val Loss: 0.343448\nEp 82: Train Loss: 0.271507, Val Loss: 0.339797\nEp 83: Train Loss: 0.299944, Val Loss: 0.395220\nEp 84: Train Loss: 0.292489, Val Loss: 0.397093\nEp 85: Train Loss: 0.293151, Val Loss: 0.362038\nEp 86: Train Loss: 0.253880, Val Loss: 0.396875\nEp 87: Train Loss: 0.292860, Val Loss: 0.384460\nEp 88: Train Loss: 0.363630, Val Loss: 0.439941\nEp 89: Train Loss: 0.332666, Val Loss: 0.353334\nEp 90: Train Loss: 0.269059, Val Loss: 0.329145\nEp 91: Train Loss: 0.272079, Val Loss: 0.377233\nEp 92: Train Loss: 0.306200, Val Loss: 0.350097\nEp 93: Train Loss: 0.299782, Val Loss: 0.366717\nEp 94: Train Loss: 0.292015, Val Loss: 0.348148\nEp 95: Train Loss: 0.277620, Val Loss: 0.364153\nEp 96: Train Loss: 0.258462, Val Loss: 0.312984\nEp 97: Train Loss: 0.303386, Val Loss: 0.319509\nEp 98: Train Loss: 0.291205, Val Loss: 0.397885\nEp 99: Train Loss: 0.252069, Val Loss: 0.332308\nEp 100: Train Loss: 0.246661, Val Loss: 0.322463\nLoss plot saved to loss_plot_mp.png\nLosses saved to losses_mp.npz\n\nModel saved to model_mp.pth\n\n[Cleanup] GPU cache cleared after MP training.\n\n\n==================================================\nStarting training for: GRF\n==================================================\n\nStart Training: GRF with 3-step rollout\nEp 1: Train Loss: 9.410125, Val Loss: 4.278281\nEp 2: Train Loss: 3.002281, Val Loss: 2.310705\nEp 3: Train Loss: 1.860372, Val Loss: 1.157474\nEp 4: Train Loss: 0.938615, Val Loss: 0.740288\nEp 5: Train Loss: 0.703815, Val Loss: 0.653682\nEp 6: Train Loss: 0.617875, Val Loss: 0.614441\nEp 7: Train Loss: 0.578832, Val Loss: 0.531892\nEp 8: Train Loss: 0.568501, Val Loss: 0.570399\nEp 9: Train Loss: 0.558743, Val Loss: 0.518779\nEp 10: Train Loss: 0.541410, Val Loss: 0.529668\nEp 11: Train Loss: 0.509890, Val Loss: 0.505713\nEp 12: Train Loss: 0.510872, Val Loss: 0.479805\nEp 13: Train Loss: 0.481381, Val Loss: 0.502317\nEp 14: Train Loss: 0.508528, Val Loss: 0.472972\nEp 15: Train Loss: 0.494903, Val Loss: 0.494105\nEp 16: Train Loss: 0.472618, Val Loss: 0.478033\nEp 17: Train Loss: 0.493283, Val Loss: 0.498099\nEp 18: Train Loss: 0.492079, Val Loss: 0.480361\nEp 19: Train Loss: 0.494361, Val Loss: 0.504136\nEp 20: Train Loss: 0.497710, Val Loss: 0.462049\nEp 21: Train Loss: 0.463803, Val Loss: 0.455932\nEp 22: Train Loss: 0.443460, Val Loss: 0.461920\nEp 23: Train Loss: 0.437770, Val Loss: 0.452882\nEp 24: Train Loss: 0.440973, Val Loss: 0.414506\nEp 25: Train Loss: 0.442106, Val Loss: 0.443578\nEp 26: Train Loss: 0.449762, Val Loss: 0.472157\nEp 27: Train Loss: 0.454050, Val Loss: 0.464475\nEp 28: Train Loss: 0.472503, Val Loss: 0.502564\nEp 29: Train Loss: 0.493556, Val Loss: 0.490202\nEp 30: Train Loss: 0.478520, Val Loss: 0.490525\nEp 31: Train Loss: 0.464229, Val Loss: 0.459850\nEp 32: Train Loss: 0.432895, Val Loss: 0.426915\nEp 33: Train Loss: 0.424747, Val Loss: 0.513930\nEp 34: Train Loss: 0.455173, Val Loss: 0.495572\nEp 35: Train Loss: 0.549983, Val Loss: 0.521381\nEp 36: Train Loss: 0.532336, Val Loss: 0.525685\nEp 37: Train Loss: 0.467515, Val Loss: 0.505420\nEp 38: Train Loss: 0.470227, Val Loss: 0.407261\nEp 39: Train Loss: 0.427634, Val Loss: 0.451307\nEp 40: Train Loss: 0.447382, Val Loss: 0.455209\nEp 41: Train Loss: 0.439146, Val Loss: 0.511917\nEp 42: Train Loss: 0.450403, Val Loss: 0.441095\nEp 43: Train Loss: 0.409438, Val Loss: 0.427757\nEp 44: Train Loss: 0.398375, Val Loss: 0.379080\nEp 45: Train Loss: 0.376558, Val Loss: 0.408625\nEp 46: Train Loss: 0.428906, Val Loss: 0.420198\nEp 47: Train Loss: 0.509013, Val Loss: 0.549607\nEp 48: Train Loss: 0.522477, Val Loss: 0.486423\nEp 49: Train Loss: 0.460897, Val Loss: 0.442828\nEp 50: Train Loss: 0.416975, Val Loss: 0.398217\nEp 51: Train Loss: 0.389765, Val Loss: 0.435067\nEp 52: Train Loss: 0.426431, Val Loss: 0.504381\nEp 53: Train Loss: 0.418440, Val Loss: 0.427948\nEp 54: Train Loss: 0.396213, Val Loss: 0.388403\nEp 55: Train Loss: 0.378709, Val Loss: 0.434857\nEp 56: Train Loss: 0.390956, Val Loss: 0.399078\nEp 57: Train Loss: 0.401012, Val Loss: 0.415591\nEp 58: Train Loss: 0.421911, Val Loss: 0.464416\nEp 59: Train Loss: 0.385161, Val Loss: 0.347672\nEp 60: Train Loss: 0.323576, Val Loss: 0.410252\nEp 61: Train Loss: 0.365262, Val Loss: 0.383922\nEp 62: Train Loss: 0.370815, Val Loss: 0.377813\nEp 63: Train Loss: 0.367155, Val Loss: 0.468446\nEp 64: Train Loss: 0.397867, Val Loss: 0.375343\nEp 65: Train Loss: 0.387634, Val Loss: 0.413193\nEp 66: Train Loss: 0.428169, Val Loss: 0.418977\nEp 67: Train Loss: 0.347728, Val Loss: 0.382836\nEp 68: Train Loss: 0.352688, Val Loss: 0.482428\nEp 69: Train Loss: 0.382350, Val Loss: 0.356495\nEp 70: Train Loss: 0.312216, Val Loss: 0.348142\nEp 71: Train Loss: 0.348847, Val Loss: 0.383307\nEp 72: Train Loss: 0.337710, Val Loss: 0.356629\nEp 73: Train Loss: 0.356089, Val Loss: 0.392713\nEp 74: Train Loss: 0.367647, Val Loss: 0.444258\nEp 75: Train Loss: 0.377141, Val Loss: 0.405484\nEp 76: Train Loss: 0.381068, Val Loss: 0.431573\nEp 77: Train Loss: 0.365977, Val Loss: 0.373774\nEp 78: Train Loss: 0.328965, Val Loss: 0.401135\nEp 79: Train Loss: 0.398338, Val Loss: 0.360650\nEp 80: Train Loss: 0.323938, Val Loss: 0.342866\nEp 81: Train Loss: 0.347952, Val Loss: 0.376546\nEp 82: Train Loss: 0.357473, Val Loss: 0.381461\nEp 83: Train Loss: 0.328290, Val Loss: 0.340258\nEp 84: Train Loss: 0.297403, Val Loss: 0.327746\nEp 85: Train Loss: 0.305361, Val Loss: 0.294113\nEp 86: Train Loss: 0.276226, Val Loss: 0.347065\nEp 87: Train Loss: 0.344193, Val Loss: 0.373594\nEp 88: Train Loss: 0.330841, Val Loss: 0.352978\nEp 89: Train Loss: 0.343970, Val Loss: 0.473642\nEp 90: Train Loss: 0.398439, Val Loss: 0.382026\nEp 91: Train Loss: 0.348635, Val Loss: 0.370016\nEp 92: Train Loss: 0.360808, Val Loss: 0.353515\nEp 93: Train Loss: 0.319910, Val Loss: 0.362858\nEp 94: Train Loss: 0.318539, Val Loss: 0.305010\nEp 95: Train Loss: 0.299300, Val Loss: 0.349727\nEp 96: Train Loss: 0.312689, Val Loss: 0.326610\nEp 97: Train Loss: 0.281760, Val Loss: 0.337752\nEp 98: Train Loss: 0.298057, Val Loss: 0.376480\nEp 99: Train Loss: 0.336184, Val Loss: 0.386333\nEp 100: Train Loss: 0.300210, Val Loss: 0.413129\nLoss plot saved to loss_plot_grf.png\nLosses saved to losses_grf.npz\n\nModel saved to model_grf.pth\n\n[Cleanup] GPU cache cleared after GRF training.\n\n","output_type":"stream"}],"execution_count":14}]}