{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxl9dgG0gEOI",
        "outputId": "5fdc0504-57b2-4125-a6fb-f28cd86c5c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# os.chdir('/content/drive/MyDrive/gdl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gdrR-qsgE-0",
        "outputId": "ea8d4878-7743-48cd-af0e-beb190fbc8cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Start Training: BASELINE with 3-step rollout\n",
            "Ep 1: Train Loss: 2.893334, Val Loss: 0.886840\n",
            "Ep 2: Train Loss: 0.406250, Val Loss: 0.276715\n",
            "Ep 3: Train Loss: 0.260791, Val Loss: 0.173711\n",
            "Ep 4: Train Loss: 0.141841, Val Loss: 0.167370\n",
            "Ep 5: Train Loss: 0.145631, Val Loss: 0.146261\n",
            "Ep 6: Train Loss: 0.140822, Val Loss: 0.175202\n",
            "Ep 7: Train Loss: 0.122334, Val Loss: 0.106121\n",
            "Ep 8: Train Loss: 0.102938, Val Loss: 0.102509\n",
            "Ep 9: Train Loss: 0.119038, Val Loss: 0.103156\n",
            "Ep 10: Train Loss: 0.089376, Val Loss: 0.093094\n",
            "Ep 11: Train Loss: 0.082623, Val Loss: 0.072918\n",
            "Ep 12: Train Loss: 0.080195, Val Loss: 0.118304\n",
            "Ep 13: Train Loss: 0.094759, Val Loss: 0.130215\n",
            "Ep 14: Train Loss: 0.109569, Val Loss: 0.117448\n",
            "Ep 15: Train Loss: 0.082688, Val Loss: 0.102578\n",
            "Ep 16: Train Loss: 0.082252, Val Loss: 0.055985\n",
            "Ep 17: Train Loss: 0.087035, Val Loss: 0.070554\n",
            "Ep 18: Train Loss: 0.073746, Val Loss: 0.089041\n",
            "Ep 19: Train Loss: 0.068688, Val Loss: 0.085255\n",
            "Ep 20: Train Loss: 0.061405, Val Loss: 0.047159\n",
            "Ep 21: Train Loss: 0.063032, Val Loss: 0.076981\n",
            "Ep 22: Train Loss: 0.050039, Val Loss: 0.047058\n",
            "Ep 23: Train Loss: 0.058143, Val Loss: 0.070152\n",
            "Ep 24: Train Loss: 0.058838, Val Loss: 0.057763\n",
            "Ep 25: Train Loss: 0.059735, Val Loss: 0.073101\n",
            "Ep 26: Train Loss: 0.059769, Val Loss: 0.062901\n",
            "Ep 27: Train Loss: 0.069106, Val Loss: 0.086898\n",
            "Ep 28: Train Loss: 0.055575, Val Loss: 0.074158\n",
            "Ep 29: Train Loss: 0.065185, Val Loss: 0.076493\n",
            "Ep 30: Train Loss: 0.075889, Val Loss: 0.057262\n",
            "Ep 31: Train Loss: 0.054662, Val Loss: 0.055712\n",
            "Ep 32: Train Loss: 0.063345, Val Loss: 0.071998\n",
            "Ep 33: Train Loss: 0.065489, Val Loss: 0.071914\n",
            "Ep 34: Train Loss: 0.054214, Val Loss: 0.067402\n",
            "Ep 35: Train Loss: 0.058797, Val Loss: 0.058974\n",
            "Ep 36: Train Loss: 0.056243, Val Loss: 0.067912\n",
            "Ep 37: Train Loss: 0.054410, Val Loss: 0.041865\n",
            "Ep 38: Train Loss: 0.045252, Val Loss: 0.099190\n",
            "Ep 39: Train Loss: 0.055759, Val Loss: 0.047713\n",
            "Ep 40: Train Loss: 0.052350, Val Loss: 0.060301\n",
            "Ep 41: Train Loss: 0.071818, Val Loss: 0.060789\n",
            "Ep 42: Train Loss: 0.059863, Val Loss: 0.041836\n",
            "Ep 43: Train Loss: 0.051313, Val Loss: 0.042101\n",
            "Ep 44: Train Loss: 0.044213, Val Loss: 0.051528\n",
            "Ep 45: Train Loss: 0.042099, Val Loss: 0.041810\n",
            "Ep 46: Train Loss: 0.042361, Val Loss: 0.045984\n",
            "Ep 47: Train Loss: 0.065241, Val Loss: 0.060078\n",
            "Ep 48: Train Loss: 0.062922, Val Loss: 0.043516\n",
            "Ep 49: Train Loss: 0.052330, Val Loss: 0.060719\n",
            "Ep 50: Train Loss: 0.076378, Val Loss: 0.098281\n",
            "Ep 51: Train Loss: 0.079329, Val Loss: 0.063119\n",
            "Ep 52: Train Loss: 0.060312, Val Loss: 0.072716\n",
            "Ep 53: Train Loss: 0.062241, Val Loss: 0.058492\n",
            "Ep 54: Train Loss: 0.054676, Val Loss: 0.049810\n",
            "Ep 55: Train Loss: 0.062948, Val Loss: 0.063459\n",
            "Ep 56: Train Loss: 0.046002, Val Loss: 0.047754\n",
            "Ep 57: Train Loss: 0.043595, Val Loss: 0.044082\n",
            "Ep 58: Train Loss: 0.047265, Val Loss: 0.046282\n",
            "Ep 59: Train Loss: 0.045404, Val Loss: 0.055784\n",
            "Ep 60: Train Loss: 0.052628, Val Loss: 0.054460\n",
            "Ep 61: Train Loss: 0.048180, Val Loss: 0.049491\n",
            "Ep 62: Train Loss: 0.050953, Val Loss: 0.062770\n",
            "Ep 63: Train Loss: 0.055168, Val Loss: 0.040032\n",
            "Ep 64: Train Loss: 0.039479, Val Loss: 0.046033\n",
            "Ep 65: Train Loss: 0.040458, Val Loss: 0.055864\n",
            "Ep 66: Train Loss: 0.054048, Val Loss: 0.047420\n",
            "Ep 67: Train Loss: 0.050706, Val Loss: 0.066390\n",
            "Ep 68: Train Loss: 0.045862, Val Loss: 0.055825\n",
            "Ep 69: Train Loss: 0.045754, Val Loss: 0.057378\n",
            "Ep 70: Train Loss: 0.054823, Val Loss: 0.037475\n",
            "Ep 71: Train Loss: 0.041909, Val Loss: 0.043362\n",
            "Ep 72: Train Loss: 0.040583, Val Loss: 0.039382\n",
            "Ep 73: Train Loss: 0.049811, Val Loss: 0.067531\n",
            "Ep 74: Train Loss: 0.049957, Val Loss: 0.054824\n",
            "Ep 75: Train Loss: 0.045973, Val Loss: 0.049469\n",
            "Ep 76: Train Loss: 0.046171, Val Loss: 0.039361\n",
            "Ep 77: Train Loss: 0.046377, Val Loss: 0.049340\n",
            "Ep 78: Train Loss: 0.055500, Val Loss: 0.082678\n",
            "Ep 79: Train Loss: 0.055741, Val Loss: 0.057949\n",
            "Ep 80: Train Loss: 0.049579, Val Loss: 0.073108\n",
            "Ep 81: Train Loss: 0.052397, Val Loss: 0.044538\n",
            "Ep 82: Train Loss: 0.042623, Val Loss: 0.039317\n",
            "Ep 83: Train Loss: 0.041347, Val Loss: 0.041928\n",
            "Ep 84: Train Loss: 0.037128, Val Loss: 0.036828\n",
            "Ep 85: Train Loss: 0.047680, Val Loss: 0.093672\n",
            "Ep 86: Train Loss: 0.062782, Val Loss: 0.049747\n",
            "Ep 87: Train Loss: 0.051646, Val Loss: 0.053361\n",
            "Ep 88: Train Loss: 0.053675, Val Loss: 0.056286\n",
            "Ep 89: Train Loss: 0.053252, Val Loss: 0.036937\n",
            "Ep 90: Train Loss: 0.049368, Val Loss: 0.056044\n",
            "Ep 91: Train Loss: 0.041946, Val Loss: 0.047123\n",
            "Ep 92: Train Loss: 0.045093, Val Loss: 0.039906\n",
            "Ep 93: Train Loss: 0.041460, Val Loss: 0.035086\n",
            "Ep 94: Train Loss: 0.038741, Val Loss: 0.033342\n",
            "Ep 95: Train Loss: 0.045919, Val Loss: 0.047720\n",
            "Ep 96: Train Loss: 0.045223, Val Loss: 0.043758\n",
            "Ep 97: Train Loss: 0.042534, Val Loss: 0.042188\n",
            "Ep 98: Train Loss: 0.038766, Val Loss: 0.049090\n",
            "Ep 99: Train Loss: 0.038965, Val Loss: 0.036800\n",
            "Ep 100: Train Loss: 0.045091, Val Loss: 0.051363\n",
            "Loss plot saved to loss_plot_baseline.png\n",
            "Losses saved to losses_baseline.npz\n",
            "\n",
            "Model saved to model_baseline.pth\n",
            "Start Training: MP with 3-step rollout\n",
            "Ep 1: Train Loss: 0.964796, Val Loss: 0.265294\n",
            "Ep 2: Train Loss: 0.175285, Val Loss: 0.102663\n",
            "Ep 3: Train Loss: 0.136402, Val Loss: 0.107248\n",
            "Ep 4: Train Loss: 0.096724, Val Loss: 0.069540\n",
            "Ep 5: Train Loss: 0.070663, Val Loss: 0.060353\n",
            "Ep 6: Train Loss: 0.061144, Val Loss: 0.064830\n",
            "Ep 7: Train Loss: 0.070956, Val Loss: 0.076209\n",
            "Ep 8: Train Loss: 0.068249, Val Loss: 0.095184\n",
            "Ep 9: Train Loss: 0.071212, Val Loss: 0.071922\n",
            "Ep 10: Train Loss: 0.058224, Val Loss: 0.072387\n",
            "Ep 11: Train Loss: 0.065670, Val Loss: 0.096439\n",
            "Ep 12: Train Loss: 0.077498, Val Loss: 0.065519\n",
            "Ep 13: Train Loss: 0.066564, Val Loss: 0.054649\n",
            "Ep 14: Train Loss: 0.070844, Val Loss: 0.090180\n",
            "Ep 15: Train Loss: 0.058579, Val Loss: 0.065498\n",
            "Ep 16: Train Loss: 0.059289, Val Loss: 0.057378\n",
            "Ep 17: Train Loss: 0.051776, Val Loss: 0.051279\n",
            "Ep 18: Train Loss: 0.058558, Val Loss: 0.051257\n",
            "Ep 19: Train Loss: 0.055933, Val Loss: 0.073706\n",
            "Ep 20: Train Loss: 0.060499, Val Loss: 0.078749\n",
            "Ep 21: Train Loss: 0.068639, Val Loss: 0.073241\n",
            "Ep 22: Train Loss: 0.054193, Val Loss: 0.053433\n",
            "Ep 23: Train Loss: 0.047385, Val Loss: 0.068670\n",
            "Ep 24: Train Loss: 0.047170, Val Loss: 0.059249\n",
            "Ep 25: Train Loss: 0.044451, Val Loss: 0.055095\n",
            "Ep 26: Train Loss: 0.052898, Val Loss: 0.058834\n",
            "Ep 27: Train Loss: 0.047235, Val Loss: 0.049801\n",
            "Ep 28: Train Loss: 0.045058, Val Loss: 0.045969\n",
            "Ep 29: Train Loss: 0.041929, Val Loss: 0.090741\n",
            "Ep 30: Train Loss: 0.049441, Val Loss: 0.053626\n",
            "Ep 31: Train Loss: 0.045992, Val Loss: 0.053648\n",
            "Ep 32: Train Loss: 0.048772, Val Loss: 0.060702\n",
            "Ep 33: Train Loss: 0.047428, Val Loss: 0.049958\n",
            "Ep 34: Train Loss: 0.047174, Val Loss: 0.054799\n",
            "Ep 35: Train Loss: 0.039712, Val Loss: 0.046177\n",
            "Ep 36: Train Loss: 0.051469, Val Loss: 0.074086\n",
            "Ep 37: Train Loss: 0.040109, Val Loss: 0.076658\n",
            "Ep 38: Train Loss: 0.039459, Val Loss: 0.038270\n",
            "Ep 39: Train Loss: 0.036802, Val Loss: 0.037786\n",
            "Ep 40: Train Loss: 0.038775, Val Loss: 0.042558\n",
            "Ep 41: Train Loss: 0.045801, Val Loss: 0.044206\n",
            "Ep 42: Train Loss: 0.040295, Val Loss: 0.062670\n",
            "Ep 43: Train Loss: 0.056188, Val Loss: 0.059703\n",
            "Ep 44: Train Loss: 0.039790, Val Loss: 0.041061\n",
            "Ep 45: Train Loss: 0.035883, Val Loss: 0.049370\n",
            "Ep 46: Train Loss: 0.034611, Val Loss: 0.042837\n",
            "Ep 47: Train Loss: 0.034461, Val Loss: 0.036619\n",
            "Ep 48: Train Loss: 0.030735, Val Loss: 0.041726\n",
            "Ep 49: Train Loss: 0.030934, Val Loss: 0.057481\n",
            "Ep 50: Train Loss: 0.052886, Val Loss: 0.054141\n",
            "Ep 51: Train Loss: 0.050165, Val Loss: 0.055806\n",
            "Ep 52: Train Loss: 0.042507, Val Loss: 0.042258\n",
            "Ep 53: Train Loss: 0.040383, Val Loss: 0.045593\n",
            "Ep 54: Train Loss: 0.040809, Val Loss: 0.042610\n",
            "Ep 55: Train Loss: 0.039534, Val Loss: 0.040084\n",
            "Ep 56: Train Loss: 0.037166, Val Loss: 0.043125\n",
            "Ep 57: Train Loss: 0.029617, Val Loss: 0.035340\n",
            "Ep 58: Train Loss: 0.033177, Val Loss: 0.043511\n",
            "Ep 59: Train Loss: 0.033944, Val Loss: 0.037588\n",
            "Ep 60: Train Loss: 0.029975, Val Loss: 0.029388\n",
            "Ep 61: Train Loss: 0.029676, Val Loss: 0.036824\n",
            "Ep 62: Train Loss: 0.028290, Val Loss: 0.028170\n",
            "Ep 63: Train Loss: 0.036878, Val Loss: 0.048155\n",
            "Ep 64: Train Loss: 0.036718, Val Loss: 0.047271\n",
            "Ep 65: Train Loss: 0.037121, Val Loss: 0.055438\n",
            "Ep 66: Train Loss: 0.037312, Val Loss: 0.047567\n",
            "Ep 67: Train Loss: 0.036134, Val Loss: 0.032432\n",
            "Ep 68: Train Loss: 0.032235, Val Loss: 0.036645\n",
            "Ep 69: Train Loss: 0.036363, Val Loss: 0.044943\n",
            "Ep 70: Train Loss: 0.033293, Val Loss: 0.037258\n",
            "Ep 71: Train Loss: 0.028480, Val Loss: 0.042762\n",
            "Ep 72: Train Loss: 0.028680, Val Loss: 0.040243\n",
            "Ep 73: Train Loss: 0.025951, Val Loss: 0.028290\n",
            "Ep 74: Train Loss: 0.028213, Val Loss: 0.028355\n",
            "Ep 75: Train Loss: 0.032494, Val Loss: 0.039067\n",
            "Ep 76: Train Loss: 0.030511, Val Loss: 0.037395\n",
            "Ep 77: Train Loss: 0.026284, Val Loss: 0.029337\n",
            "Ep 78: Train Loss: 0.026330, Val Loss: 0.033582\n",
            "Ep 79: Train Loss: 0.026962, Val Loss: 0.053861\n",
            "Ep 80: Train Loss: 0.041083, Val Loss: 0.059941\n",
            "Ep 81: Train Loss: 0.039340, Val Loss: 0.034312\n",
            "Ep 82: Train Loss: 0.027947, Val Loss: 0.061303\n",
            "Ep 83: Train Loss: 0.035292, Val Loss: 0.044901\n",
            "Ep 84: Train Loss: 0.030288, Val Loss: 0.038102\n",
            "Ep 85: Train Loss: 0.024741, Val Loss: 0.032993\n",
            "Ep 86: Train Loss: 0.028479, Val Loss: 0.033681\n",
            "Ep 87: Train Loss: 0.027665, Val Loss: 0.043104\n",
            "Ep 88: Train Loss: 0.026011, Val Loss: 0.032445\n",
            "Ep 89: Train Loss: 0.023715, Val Loss: 0.028866\n",
            "Ep 90: Train Loss: 0.025482, Val Loss: 0.028036\n",
            "Ep 91: Train Loss: 0.026094, Val Loss: 0.029622\n",
            "Ep 92: Train Loss: 0.027095, Val Loss: 0.031387\n",
            "Ep 93: Train Loss: 0.026966, Val Loss: 0.034912\n",
            "Ep 94: Train Loss: 0.025633, Val Loss: 0.042510\n",
            "Ep 95: Train Loss: 0.034882, Val Loss: 0.039509\n",
            "Ep 96: Train Loss: 0.028394, Val Loss: 0.034564\n",
            "Ep 97: Train Loss: 0.031593, Val Loss: 0.035398\n",
            "Ep 98: Train Loss: 0.033904, Val Loss: 0.032717\n",
            "Ep 99: Train Loss: 0.024685, Val Loss: 0.030788\n",
            "Ep 100: Train Loss: 0.023228, Val Loss: 0.029060\n",
            "Loss plot saved to loss_plot_mp.png\n",
            "Losses saved to losses_mp.npz\n",
            "\n",
            "Model saved to model_mp.pth\n",
            "Start Training: GRF with 3-step rollout\n",
            "Ep 1: Train Loss: 11.249981, Val Loss: 6.694780\n",
            "Ep 2: Train Loss: 3.384034, Val Loss: 1.767885\n",
            "Ep 3: Train Loss: 1.271014, Val Loss: 1.216945\n",
            "Ep 4: Train Loss: 0.780881, Val Loss: 0.626292\n",
            "Ep 5: Train Loss: 0.543713, Val Loss: 0.531829\n",
            "Ep 6: Train Loss: 0.416477, Val Loss: 0.342563\n",
            "Ep 7: Train Loss: 0.330239, Val Loss: 0.298827\n",
            "Ep 8: Train Loss: 0.270447, Val Loss: 0.274998\n",
            "Ep 9: Train Loss: 0.246463, Val Loss: 0.286387\n",
            "Ep 10: Train Loss: 0.232715, Val Loss: 0.212399\n",
            "Ep 11: Train Loss: 0.222288, Val Loss: 0.198270\n",
            "Ep 12: Train Loss: 0.179324, Val Loss: 0.145445\n",
            "Ep 13: Train Loss: 0.143371, Val Loss: 0.116189\n",
            "Ep 14: Train Loss: 0.131672, Val Loss: 0.150109\n",
            "Ep 15: Train Loss: 0.128907, Val Loss: 0.117984\n",
            "Ep 16: Train Loss: 0.112421, Val Loss: 0.107985\n",
            "Ep 17: Train Loss: 0.096178, Val Loss: 0.096394\n",
            "Ep 18: Train Loss: 0.114555, Val Loss: 0.105533\n",
            "Ep 19: Train Loss: 0.097405, Val Loss: 0.100315\n",
            "Ep 20: Train Loss: 0.083922, Val Loss: 0.081904\n",
            "Ep 21: Train Loss: 0.074268, Val Loss: 0.067559\n",
            "Ep 22: Train Loss: 0.068972, Val Loss: 0.057566\n",
            "Ep 23: Train Loss: 0.060718, Val Loss: 0.063415\n",
            "Ep 24: Train Loss: 0.092345, Val Loss: 0.098195\n",
            "Ep 25: Train Loss: 0.079624, Val Loss: 0.064668\n",
            "Ep 26: Train Loss: 0.058529, Val Loss: 0.061378\n",
            "Ep 27: Train Loss: 0.056840, Val Loss: 0.057676\n",
            "Ep 28: Train Loss: 0.065831, Val Loss: 0.080136\n",
            "Ep 29: Train Loss: 0.065861, Val Loss: 0.061963\n",
            "Ep 30: Train Loss: 0.063973, Val Loss: 0.058696\n",
            "Ep 31: Train Loss: 0.052157, Val Loss: 0.067597\n",
            "Ep 32: Train Loss: 0.053680, Val Loss: 0.063502\n",
            "Ep 33: Train Loss: 0.057172, Val Loss: 0.059432\n",
            "Ep 34: Train Loss: 0.056411, Val Loss: 0.069285\n",
            "Ep 35: Train Loss: 0.061570, Val Loss: 0.068299\n",
            "Ep 36: Train Loss: 0.049234, Val Loss: 0.052176\n",
            "Ep 37: Train Loss: 0.046128, Val Loss: 0.043902\n",
            "Ep 38: Train Loss: 0.045864, Val Loss: 0.058052\n",
            "Ep 39: Train Loss: 0.054132, Val Loss: 0.055854\n",
            "Ep 40: Train Loss: 0.050210, Val Loss: 0.048838\n",
            "Ep 41: Train Loss: 0.045684, Val Loss: 0.037665\n",
            "Ep 42: Train Loss: 0.046351, Val Loss: 0.051267\n",
            "Ep 43: Train Loss: 0.055118, Val Loss: 0.058820\n",
            "Ep 44: Train Loss: 0.049419, Val Loss: 0.043316\n",
            "Ep 45: Train Loss: 0.044962, Val Loss: 0.052140\n",
            "Ep 46: Train Loss: 0.044770, Val Loss: 0.050495\n",
            "Ep 47: Train Loss: 0.044842, Val Loss: 0.043094\n",
            "Ep 48: Train Loss: 0.038436, Val Loss: 0.052180\n",
            "Ep 49: Train Loss: 0.043608, Val Loss: 0.037295\n",
            "Ep 50: Train Loss: 0.049033, Val Loss: 0.047740\n",
            "Ep 51: Train Loss: 0.044785, Val Loss: 0.034792\n",
            "Ep 52: Train Loss: 0.035626, Val Loss: 0.047480\n",
            "Ep 53: Train Loss: 0.041383, Val Loss: 0.048397\n",
            "Ep 54: Train Loss: 0.040140, Val Loss: 0.043431\n",
            "Ep 55: Train Loss: 0.047142, Val Loss: 0.048543\n",
            "Ep 56: Train Loss: 0.047110, Val Loss: 0.052074\n",
            "Ep 57: Train Loss: 0.055720, Val Loss: 0.044390\n",
            "Ep 58: Train Loss: 0.045305, Val Loss: 0.043680\n",
            "Ep 59: Train Loss: 0.039650, Val Loss: 0.038214\n",
            "Ep 60: Train Loss: 0.035879, Val Loss: 0.049805\n",
            "Ep 61: Train Loss: 0.043306, Val Loss: 0.045880\n",
            "Ep 62: Train Loss: 0.054786, Val Loss: 0.049307\n",
            "Ep 63: Train Loss: 0.042612, Val Loss: 0.047955\n",
            "Ep 64: Train Loss: 0.037039, Val Loss: 0.041784\n",
            "Ep 65: Train Loss: 0.034572, Val Loss: 0.030875\n",
            "Ep 66: Train Loss: 0.032016, Val Loss: 0.043053\n",
            "Ep 67: Train Loss: 0.037411, Val Loss: 0.049499\n",
            "Ep 68: Train Loss: 0.040896, Val Loss: 0.041121\n",
            "Ep 69: Train Loss: 0.038122, Val Loss: 0.036988\n",
            "Ep 70: Train Loss: 0.035872, Val Loss: 0.035812\n",
            "Ep 71: Train Loss: 0.036924, Val Loss: 0.041304\n",
            "Ep 72: Train Loss: 0.038970, Val Loss: 0.045244\n",
            "Ep 73: Train Loss: 0.040791, Val Loss: 0.045000\n",
            "Ep 74: Train Loss: 0.037257, Val Loss: 0.040727\n",
            "Ep 75: Train Loss: 0.045504, Val Loss: 0.040356\n",
            "Ep 76: Train Loss: 0.039139, Val Loss: 0.033447\n",
            "Ep 77: Train Loss: 0.043582, Val Loss: 0.048849\n",
            "Ep 78: Train Loss: 0.041542, Val Loss: 0.042642\n",
            "Ep 79: Train Loss: 0.035999, Val Loss: 0.039814\n",
            "Ep 80: Train Loss: 0.035911, Val Loss: 0.045430\n",
            "Ep 81: Train Loss: 0.036693, Val Loss: 0.038808\n",
            "Ep 82: Train Loss: 0.037867, Val Loss: 0.043657\n",
            "Ep 83: Train Loss: 0.041756, Val Loss: 0.037454\n",
            "Ep 84: Train Loss: 0.036997, Val Loss: 0.030691\n",
            "Ep 85: Train Loss: 0.031412, Val Loss: 0.030750\n",
            "Ep 86: Train Loss: 0.037639, Val Loss: 0.042720\n",
            "Ep 87: Train Loss: 0.034472, Val Loss: 0.038827\n",
            "Ep 88: Train Loss: 0.033537, Val Loss: 0.042151\n",
            "Ep 89: Train Loss: 0.034899, Val Loss: 0.033168\n",
            "Ep 90: Train Loss: 0.032561, Val Loss: 0.037103\n",
            "Ep 91: Train Loss: 0.031044, Val Loss: 0.040059\n",
            "Ep 92: Train Loss: 0.034248, Val Loss: 0.046664\n",
            "Ep 93: Train Loss: 0.035293, Val Loss: 0.030001\n",
            "Ep 94: Train Loss: 0.031641, Val Loss: 0.031609\n",
            "Ep 95: Train Loss: 0.033058, Val Loss: 0.042114\n",
            "Ep 96: Train Loss: 0.038580, Val Loss: 0.039481\n",
            "Ep 97: Train Loss: 0.045331, Val Loss: 0.049817\n",
            "Ep 98: Train Loss: 0.037785, Val Loss: 0.044736\n",
            "Ep 99: Train Loss: 0.039318, Val Loss: 0.043870\n",
            "Ep 100: Train Loss: 0.038201, Val Loss: 0.035495\n",
            "Loss plot saved to loss_plot_grf.png\n",
            "Losses saved to losses_grf.npz\n",
            "\n",
            "Model saved to model_grf.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_MODE = 'grf'  # 'grf', 'baseline', 'mp'\n",
        "DATA_PATH = \"./data/\"\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-3 # Learning Rate\n",
        "EPOCHS = 100\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "K_NEIGHBORS = 6\n",
        "VAL_SPLIT = 0.2\n",
        "TRAIN_ROLLOUT_STEPS = 3  # Number of autoregressive steps during training\n",
        "\n",
        "# --- DYNAMIC KNN FUNCTION ---\n",
        "def compute_knn_torch(points, k):\n",
        "    \"\"\"Compute KNN dynamically on GPU - works with batched tensors\"\"\"\n",
        "    B, N, D = points.shape\n",
        "    # Compute pairwise distances\n",
        "    dist_mat = torch.cdist(points, points, p=2)\n",
        "    # Get k+1 nearest neighbors (includes self), then exclude self\n",
        "    _, knn_indices = torch.topk(dist_mat, k=k+1, dim=-1, largest=False)\n",
        "    return knn_indices[:, :, 1:]  # Shape: (B, N, k)\n",
        "\n",
        "# --- DATASET ---\n",
        "class RobotArmDataset(Dataset):\n",
        "    def __init__(self, points_path, knn_path, rollout_steps=1):\n",
        "        if not os.path.exists(points_path):\n",
        "            raise FileNotFoundError(f\"File {points_path} not found.\")\n",
        "        self.points = np.load(points_path).astype(np.float32)\n",
        "        self.knn = np.load(knn_path).astype(np.int64)\n",
        "        self.rollout_steps = rollout_steps\n",
        "\n",
        "        # Normalization stats\n",
        "        self.mean = np.mean(self.points, axis=(0, 1))\n",
        "        self.std = np.std(self.points, axis=(0, 1))\n",
        "        self.points = (self.points - self.mean) / (self.std + 1e-6)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Account for needing rollout_steps future frames\n",
        "        return len(self.points) - self.rollout_steps\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return input, knn, and sequence of rollout_steps targets\n",
        "        input_frame = torch.from_numpy(self.points[idx])\n",
        "        input_knn = torch.from_numpy(self.knn[idx])\n",
        "        # Stack future targets: [t+1, t+2, ..., t+rollout_steps]\n",
        "        targets = torch.stack([\n",
        "            torch.from_numpy(self.points[idx + i + 1])\n",
        "            for i in range(self.rollout_steps)\n",
        "        ])  # Shape: (rollout_steps, N, 3)\n",
        "        return input_frame, input_knn, targets\n",
        "\n",
        "# --- MODELS ---\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k = torch.relu(q) + 1e-6, torch.relu(k) + 1e-6\n",
        "        kv = torch.einsum(\"bnd,bne->bde\", k, v)\n",
        "        z = 1 / (torch.einsum(\"bnd,bd->bn\", q, k.sum(dim=1)) + 1e-6)\n",
        "        attn = torch.einsum(\"bnd,bde,bn->bne\", q, kv, z)\n",
        "        return self.to_out(attn)\n",
        "\n",
        "class TopologicalGRFLayer(nn.Module):\n",
        "    def __init__(self, dim, k_neighbors, hops=3):\n",
        "        super().__init__()\n",
        "        self.k = k_neighbors\n",
        "        self.hops = hops\n",
        "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.to_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, knn_idx):\n",
        "        B, N, D = x.shape\n",
        "        # Sparse Matrix Construction\n",
        "        src = torch.arange(N, device=x.device).view(1, N, 1).expand(B, N, self.k)\n",
        "        batch_off = torch.arange(B, device=x.device).view(B, 1, 1) * N\n",
        "        indices = torch.stack([(knn_idx + batch_off).view(-1), (src + batch_off).view(-1)])\n",
        "        values = torch.ones(indices.shape[1], device=x.device)\n",
        "        adj = torch.sparse_coo_tensor(indices, values, (B*N, B*N))\n",
        "\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        v_f, k_f = v.view(B*N, D), k.view(B*N, D)\n",
        "\n",
        "        # Random Walk Diffusion\n",
        "        for _ in range(self.hops):\n",
        "            v_f = torch.sparse.mm(adj, v_f) / (self.k + 1e-6)\n",
        "            k_f = torch.sparse.mm(adj, k_f) / (self.k + 1e-6)\n",
        "\n",
        "        attn = (q * k_f.view(B, N, D)).sum(dim=-1, keepdim=True)\n",
        "        return self.to_out(attn * v_f.view(B, N, D))\n",
        "\n",
        "class SimpleMessagePassing(nn.Module):\n",
        "    def __init__(self, dim, k_neighbors):\n",
        "        super().__init__()\n",
        "        self.k = k_neighbors\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, knn_idx):\n",
        "        B, N, D = x.shape\n",
        "        flat_idx = knn_idx.reshape(B, N * self.k).unsqueeze(-1).expand(-1, -1, D)\n",
        "        neighbors = torch.gather(x, 1, flat_idx.reshape(B, N * self.k, D).long()).reshape(B, N, self.k, D)\n",
        "        return self.proj(neighbors.mean(dim=2))\n",
        "\n",
        "class UnifiedInterlacer(nn.Module):\n",
        "    def __init__(self, mode='grf', input_dim=3, embed_dim=128, num_layers=5):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
        "\n",
        "        # Create layer norms for each layer (2 per block: graph + attention)\n",
        "        self.norms = nn.ModuleList([nn.LayerNorm(embed_dim) for _ in range(num_layers * 2)])\n",
        "\n",
        "        # Create graph layers (GRF, MP, or Identity based on mode)\n",
        "        if mode == 'grf':\n",
        "            self.graph_layers = nn.ModuleList([TopologicalGRFLayer(embed_dim, K_NEIGHBORS) for _ in range(num_layers)])\n",
        "        elif mode == 'mp':\n",
        "            self.graph_layers = nn.ModuleList([SimpleMessagePassing(embed_dim, K_NEIGHBORS) for _ in range(num_layers)])\n",
        "        else:\n",
        "            self.graph_layers = nn.ModuleList([nn.Identity() for _ in range(num_layers)])\n",
        "\n",
        "        # Create attention layers\n",
        "        self.attn_layers = nn.ModuleList([LinearAttention(embed_dim) for _ in range(num_layers)])\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, 3)\n",
        "\n",
        "    def forward(self, x, knn):\n",
        "        h = self.embedding(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # Graph layer\n",
        "            if self.mode != 'baseline':\n",
        "                h = h + self.graph_layers[i](self.norms[i * 2](h), knn)\n",
        "            else:\n",
        "                h = h + self.norms[i * 2](h)\n",
        "\n",
        "            # Attention layer\n",
        "            h = h + self.attn_layers[i](self.norms[i * 2 + 1](h))\n",
        "\n",
        "        return self.head(h)\n",
        "\n",
        "# --- MAIN ---\n",
        "def main():\n",
        "    dataset = RobotArmDataset(\"points.npy\", \"knn_indices.npy\", rollout_steps=TRAIN_ROLLOUT_STEPS)\n",
        "    train_size = int(len(dataset) * (1 - VAL_SPLIT))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = UnifiedInterlacer(mode=MODEL_MODE).to(DEVICE)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Track losses\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(f\"Start Training: {MODEL_MODE.upper()} with {TRAIN_ROLLOUT_STEPS}-step rollout\")\n",
        "    for ep in range(EPOCHS):\n",
        "        # Training with multi-step rollout\n",
        "        model.train()\n",
        "        epoch_train_losses = []\n",
        "        for x, knn, targets in train_loader:\n",
        "            # x: (B, N, 3), knn: (B, N, K), targets: (B, ROLLOUT_STEPS, N, 3)\n",
        "            x, knn, targets = x.to(DEVICE), knn.to(DEVICE), targets.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Multi-step rollout (BPTT - no detach)\n",
        "            current_input = x\n",
        "            current_knn = knn\n",
        "            step_losses = []\n",
        "\n",
        "            for step in range(TRAIN_ROLLOUT_STEPS):\n",
        "                # Predict next frame\n",
        "                pred = model(current_input, current_knn)\n",
        "\n",
        "                # Loss against ground truth target at this step\n",
        "                gt_target = targets[:, step]  # (B, N, 3)\n",
        "                step_loss = criterion(pred, gt_target)\n",
        "                step_losses.append(step_loss)\n",
        "\n",
        "                # CLOSED LOOP: prediction becomes next input (NO detach for BPTT)\n",
        "                current_input = pred\n",
        "\n",
        "                # DYNAMIC KNN: recompute graph on predicted coordinates\n",
        "                current_knn = compute_knn_torch(current_input, K_NEIGHBORS)\n",
        "\n",
        "            # Total loss = average across all steps\n",
        "            total_loss = torch.stack(step_losses).mean()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_losses.append(total_loss.item())\n",
        "\n",
        "        # Validation with same rollout\n",
        "        model.eval()\n",
        "        epoch_val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for x, knn, targets in val_loader:\n",
        "                x, knn, targets = x.to(DEVICE), knn.to(DEVICE), targets.to(DEVICE)\n",
        "\n",
        "                current_input = x\n",
        "                current_knn = knn\n",
        "                step_losses = []\n",
        "\n",
        "                for step in range(TRAIN_ROLLOUT_STEPS):\n",
        "                    pred = model(current_input, current_knn)\n",
        "                    gt_target = targets[:, step]\n",
        "                    step_loss = criterion(pred, gt_target)\n",
        "                    step_losses.append(step_loss)\n",
        "\n",
        "                    current_input = pred\n",
        "                    current_knn = compute_knn_torch(current_input, K_NEIGHBORS)\n",
        "\n",
        "                total_loss = torch.stack(step_losses).mean()\n",
        "                epoch_val_losses.append(total_loss.item())\n",
        "\n",
        "        train_loss = np.mean(epoch_train_losses)\n",
        "        val_loss = np.mean(epoch_val_losses)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Ep {ep+1}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    # Plot and save loss curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\n",
        "    plt.title(f'Training and Validation Loss - {MODEL_MODE.upper()} ({TRAIN_ROLLOUT_STEPS}-step rollout)')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.savefig(f'loss_plot_{MODEL_MODE}.png', dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"Loss plot saved to loss_plot_{MODEL_MODE}.png\")\n",
        "\n",
        "    # Save losses to numpy file\n",
        "    np.savez(f'losses_{MODEL_MODE}.npz',\n",
        "             train_losses=np.array(train_losses),\n",
        "             val_losses=np.array(val_losses))\n",
        "    print(f\"Losses saved to losses_{MODEL_MODE}.npz\")\n",
        "    print()\n",
        "\n",
        "    # SAVE MODEL WEIGHTS\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'mean': dataset.mean,\n",
        "        'std': dataset.std,\n",
        "        'mode': MODEL_MODE,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'rollout_steps': TRAIN_ROLLOUT_STEPS\n",
        "    }, f\"model_{MODEL_MODE}.pth\")\n",
        "    print(f\"Model saved to model_{MODEL_MODE}.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.chdir(DATA_PATH)\n",
        "\n",
        "    MODEL_MODE = 'baseline'\n",
        "    main()\n",
        "    MODEL_MODE = 'mp'\n",
        "    main()\n",
        "    MODEL_MODE = 'grf'\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
