{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T11:06:06.606150Z",
     "iopub.status.busy": "2025-12-12T11:06:06.605873Z",
     "iopub.status.idle": "2025-12-12T11:06:06.609775Z",
     "shell.execute_reply": "2025-12-12T11:06:06.608948Z",
     "shell.execute_reply.started": "2025-12-12T11:06:06.606128Z"
    },
    "id": "Uxl9dgG0gEOI",
    "outputId": "5fdc0504-57b2-4125-a6fb-f28cd86c5c46",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# os.chdir('/content/drive/MyDrive/gdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-12-12T11:06:06.696961Z",
     "iopub.status.busy": "2025-12-12T11:06:06.696291Z",
     "iopub.status.idle": "2025-12-12T11:34:16.978822Z",
     "shell.execute_reply": "2025-12-12T11:34:16.978031Z",
     "shell.execute_reply.started": "2025-12-12T11:06:06.696935Z"
    },
    "id": "6gdrR-qsgE-0",
    "outputId": "ea8d4878-7743-48cd-af0e-beb190fbc8cf",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "Starting training for: GRF\n",
      "==================================================\n",
      "\n",
      "Start Training: GRF with 3-step rollout\n",
      "Ep 1: Train Loss: 1.718707, Val Loss: 1.280474\n",
      "Ep 2: Train Loss: 1.095955, Val Loss: 0.982424\n",
      "Ep 3: Train Loss: 0.932409, Val Loss: 0.872389\n",
      "Ep 4: Train Loss: 0.840883, Val Loss: 0.793772\n",
      "Ep 5: Train Loss: 0.787253, Val Loss: 0.786499\n",
      "Ep 6: Train Loss: 0.761128, Val Loss: 0.747880\n",
      "Ep 7: Train Loss: 0.741152, Val Loss: 0.691533\n",
      "Ep 8: Train Loss: 0.737461, Val Loss: 0.730153\n",
      "Ep 9: Train Loss: 0.752822, Val Loss: 0.695720\n",
      "Ep 10: Train Loss: 0.727640, Val Loss: 0.708440\n",
      "Ep 11: Train Loss: 0.736048, Val Loss: 0.769621\n",
      "Ep 12: Train Loss: 0.744013, Val Loss: 0.698718\n",
      "Ep 13: Train Loss: 0.700856, Val Loss: 0.603995\n",
      "Ep 14: Train Loss: 0.704475, Val Loss: 0.632008\n",
      "Ep 15: Train Loss: 0.671499, Val Loss: 0.632062\n",
      "Ep 16: Train Loss: 0.643837, Val Loss: 0.628893\n",
      "Ep 17: Train Loss: 0.657737, Val Loss: 0.609273\n",
      "Ep 18: Train Loss: 0.666775, Val Loss: 0.649995\n",
      "Ep 19: Train Loss: 0.650034, Val Loss: 0.628089\n",
      "Ep 20: Train Loss: 0.676764, Val Loss: 0.666583\n",
      "Ep 21: Train Loss: 0.649366, Val Loss: 0.625693\n",
      "Ep 22: Train Loss: 0.633110, Val Loss: 0.640436\n",
      "Ep 23: Train Loss: 0.636242, Val Loss: 0.566917\n",
      "Ep 24: Train Loss: 0.613184, Val Loss: 0.605978\n",
      "Ep 25: Train Loss: 0.611201, Val Loss: 0.538051\n",
      "Ep 26: Train Loss: 0.611576, Val Loss: 0.585982\n",
      "Ep 27: Train Loss: 0.653449, Val Loss: 0.655120\n",
      "Ep 28: Train Loss: 0.630058, Val Loss: 0.548837\n",
      "Ep 29: Train Loss: 0.597853, Val Loss: 0.640792\n",
      "Ep 30: Train Loss: 0.593390, Val Loss: 0.537691\n",
      "Ep 31: Train Loss: 0.586383, Val Loss: 0.547619\n",
      "Ep 32: Train Loss: 0.584423, Val Loss: 0.555720\n",
      "Ep 33: Train Loss: 0.571709, Val Loss: 0.580405\n",
      "Ep 34: Train Loss: 0.641887, Val Loss: 0.580804\n",
      "Ep 35: Train Loss: 0.603904, Val Loss: 0.546464\n",
      "Ep 36: Train Loss: 0.569622, Val Loss: 0.548350\n",
      "Ep 37: Train Loss: 0.564553, Val Loss: 0.536827\n",
      "Ep 38: Train Loss: 0.581300, Val Loss: 0.512793\n",
      "Ep 39: Train Loss: 0.618528, Val Loss: 0.564497\n",
      "Ep 40: Train Loss: 0.587666, Val Loss: 0.594336\n",
      "Ep 41: Train Loss: 0.586084, Val Loss: 0.521088\n",
      "Ep 42: Train Loss: 0.558273, Val Loss: 0.562863\n",
      "Ep 43: Train Loss: 0.549274, Val Loss: 0.516349\n",
      "Ep 44: Train Loss: 0.542021, Val Loss: 0.563123\n",
      "Ep 45: Train Loss: 0.569889, Val Loss: 0.561093\n",
      "Ep 46: Train Loss: 0.579797, Val Loss: 0.520083\n",
      "Ep 47: Train Loss: 0.621727, Val Loss: 0.506455\n",
      "Ep 48: Train Loss: 0.563097, Val Loss: 0.519910\n",
      "Ep 49: Train Loss: 0.548542, Val Loss: 0.475422\n",
      "Ep 50: Train Loss: 0.514572, Val Loss: 0.540976\n",
      "Ep 51: Train Loss: 0.582871, Val Loss: 0.465318\n",
      "Ep 52: Train Loss: 0.581006, Val Loss: 0.523162\n",
      "Ep 53: Train Loss: 0.540288, Val Loss: 0.491762\n",
      "Ep 54: Train Loss: 0.573336, Val Loss: 0.496939\n",
      "Ep 55: Train Loss: 0.557072, Val Loss: 0.465837\n",
      "Ep 56: Train Loss: 0.517339, Val Loss: 0.450423\n",
      "Ep 57: Train Loss: 0.489796, Val Loss: 0.433577\n",
      "Ep 58: Train Loss: 0.488100, Val Loss: 0.471607\n",
      "Ep 59: Train Loss: 0.563639, Val Loss: 0.503975\n",
      "Ep 60: Train Loss: 0.548039, Val Loss: 0.530264\n",
      "Ep 61: Train Loss: 0.531351, Val Loss: 0.454158\n",
      "Ep 62: Train Loss: 0.515208, Val Loss: 0.478838\n",
      "Ep 63: Train Loss: 0.505959, Val Loss: 0.411951\n",
      "Ep 64: Train Loss: 0.511311, Val Loss: 0.480449\n",
      "Ep 65: Train Loss: 0.535611, Val Loss: 0.433170\n",
      "Ep 66: Train Loss: 0.546580, Val Loss: 0.449060\n",
      "Ep 67: Train Loss: 0.532531, Val Loss: 0.426692\n",
      "Ep 68: Train Loss: 0.517482, Val Loss: 0.463817\n",
      "Ep 69: Train Loss: 0.543519, Val Loss: 0.436302\n",
      "Ep 70: Train Loss: 0.493333, Val Loss: 0.478006\n",
      "Ep 71: Train Loss: 0.520961, Val Loss: 0.473748\n",
      "Ep 72: Train Loss: 0.494820, Val Loss: 0.439166\n",
      "Ep 73: Train Loss: 0.429112, Val Loss: 0.400640\n",
      "Ep 74: Train Loss: 0.501150, Val Loss: 0.406801\n",
      "Ep 75: Train Loss: 0.505063, Val Loss: 0.464860\n",
      "Ep 76: Train Loss: 0.521015, Val Loss: 0.500503\n",
      "Ep 77: Train Loss: 0.533724, Val Loss: 0.429678\n",
      "Ep 78: Train Loss: 0.515807, Val Loss: 0.426302\n",
      "Ep 79: Train Loss: 0.446341, Val Loss: 0.419070\n",
      "Ep 80: Train Loss: 0.506841, Val Loss: 0.444354\n",
      "Ep 81: Train Loss: 0.479879, Val Loss: 0.382470\n",
      "Ep 82: Train Loss: 0.437738, Val Loss: 0.358207\n",
      "Ep 83: Train Loss: 0.423170, Val Loss: 0.370647\n",
      "Ep 84: Train Loss: 0.430119, Val Loss: 0.433755\n",
      "Ep 85: Train Loss: 0.450909, Val Loss: 0.336007\n",
      "Ep 86: Train Loss: 0.431795, Val Loss: 0.345925\n",
      "Ep 87: Train Loss: 0.456616, Val Loss: 0.422365\n",
      "Ep 88: Train Loss: 0.528550, Val Loss: 0.401141\n",
      "Ep 89: Train Loss: 0.444996, Val Loss: 0.362591\n",
      "Ep 90: Train Loss: 0.417291, Val Loss: 0.380061\n",
      "Ep 91: Train Loss: 0.388914, Val Loss: 0.384952\n",
      "Ep 92: Train Loss: 0.402093, Val Loss: 0.316823\n",
      "Ep 93: Train Loss: 0.438567, Val Loss: 0.390262\n",
      "Ep 94: Train Loss: 0.423585, Val Loss: 0.357157\n",
      "Ep 95: Train Loss: 0.444723, Val Loss: 0.406385\n",
      "Ep 96: Train Loss: 0.437639, Val Loss: 0.376914\n",
      "Ep 97: Train Loss: 0.422399, Val Loss: 0.388735\n",
      "Ep 98: Train Loss: 0.603460, Val Loss: 0.497343\n",
      "Ep 99: Train Loss: 0.552742, Val Loss: 0.437978\n",
      "Ep 100: Train Loss: 0.478058, Val Loss: 0.393861\n",
      "Loss plot saved to loss_plot_grf.png\n",
      "Losses saved to losses_grf.npz\n",
      "\n",
      "Model saved to model_grf.pth\n",
      "\n",
      "[Cleanup] GPU cache cleared after GRF training.\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting training for: MP\n",
      "==================================================\n",
      "\n",
      "Start Training: MP with 3-step rollout\n",
      "Ep 1: Train Loss: 0.986786, Val Loss: 0.764885\n",
      "Ep 2: Train Loss: 0.807435, Val Loss: 0.715161\n",
      "Ep 3: Train Loss: 0.770082, Val Loss: 0.649431\n",
      "Ep 4: Train Loss: 0.752084, Val Loss: 0.682255\n",
      "Ep 5: Train Loss: 0.743238, Val Loss: 0.657858\n",
      "Ep 6: Train Loss: 0.737359, Val Loss: 0.630202\n",
      "Ep 7: Train Loss: 0.726934, Val Loss: 0.641895\n",
      "Ep 8: Train Loss: 0.711076, Val Loss: 0.622086\n",
      "Ep 9: Train Loss: 0.680899, Val Loss: 0.599500\n",
      "Ep 10: Train Loss: 0.680025, Val Loss: 0.588568\n",
      "Ep 11: Train Loss: 0.679240, Val Loss: 0.603135\n",
      "Ep 12: Train Loss: 0.668839, Val Loss: 0.649185\n",
      "Ep 13: Train Loss: 0.689481, Val Loss: 0.608638\n",
      "Ep 14: Train Loss: 0.678707, Val Loss: 0.637534\n",
      "Ep 15: Train Loss: 0.676862, Val Loss: 0.614775\n",
      "Ep 16: Train Loss: 0.640737, Val Loss: 0.589619\n",
      "Ep 17: Train Loss: 0.631689, Val Loss: 0.612447\n",
      "Ep 18: Train Loss: 0.639262, Val Loss: 0.580420\n",
      "Ep 19: Train Loss: 0.626468, Val Loss: 0.624658\n",
      "Ep 20: Train Loss: 0.619285, Val Loss: 0.614926\n",
      "Ep 21: Train Loss: 0.623205, Val Loss: 0.615943\n",
      "Ep 22: Train Loss: 0.604887, Val Loss: 0.637508\n",
      "Ep 23: Train Loss: 0.619198, Val Loss: 0.609100\n",
      "Ep 24: Train Loss: 0.578990, Val Loss: 0.597756\n",
      "Ep 25: Train Loss: 0.569187, Val Loss: 0.635529\n",
      "Ep 26: Train Loss: 0.631401, Val Loss: 0.622066\n",
      "Ep 27: Train Loss: 0.608077, Val Loss: 0.612436\n",
      "Ep 28: Train Loss: 0.554380, Val Loss: 0.615728\n",
      "Ep 29: Train Loss: 0.558271, Val Loss: 0.596341\n",
      "Ep 30: Train Loss: 0.577126, Val Loss: 0.565900\n",
      "Ep 31: Train Loss: 0.552932, Val Loss: 0.561961\n",
      "Ep 32: Train Loss: 0.560572, Val Loss: 0.572739\n",
      "Ep 33: Train Loss: 0.545241, Val Loss: 0.559217\n",
      "Ep 34: Train Loss: 0.505522, Val Loss: 0.532532\n",
      "Ep 35: Train Loss: 0.562206, Val Loss: 0.557745\n",
      "Ep 36: Train Loss: 0.609329, Val Loss: 0.582504\n",
      "Ep 37: Train Loss: 0.555446, Val Loss: 0.511658\n",
      "Ep 38: Train Loss: 0.522321, Val Loss: 0.568365\n",
      "Ep 39: Train Loss: 0.554575, Val Loss: 0.542588\n",
      "Ep 40: Train Loss: 0.537202, Val Loss: 0.539927\n",
      "Ep 41: Train Loss: 0.526006, Val Loss: 0.566794\n",
      "Ep 42: Train Loss: 0.512664, Val Loss: 0.581681\n",
      "Ep 43: Train Loss: 0.493165, Val Loss: 0.534650\n",
      "Ep 44: Train Loss: 0.532803, Val Loss: 0.634901\n",
      "Ep 45: Train Loss: 0.518884, Val Loss: 0.555812\n",
      "Ep 46: Train Loss: 0.499504, Val Loss: 0.593198\n",
      "Ep 47: Train Loss: 0.511692, Val Loss: 0.542397\n",
      "Ep 48: Train Loss: 0.484276, Val Loss: 0.537186\n",
      "Ep 49: Train Loss: 0.473776, Val Loss: 0.565131\n",
      "Ep 50: Train Loss: 0.478154, Val Loss: 0.552710\n",
      "Ep 51: Train Loss: 0.464558, Val Loss: 0.575702\n",
      "Ep 52: Train Loss: 0.471729, Val Loss: 0.513465\n",
      "Ep 53: Train Loss: 0.472543, Val Loss: 0.563164\n",
      "Ep 54: Train Loss: 0.481694, Val Loss: 0.520326\n",
      "Ep 55: Train Loss: 0.468218, Val Loss: 0.545719\n",
      "Ep 56: Train Loss: 0.496438, Val Loss: 0.629475\n",
      "Ep 57: Train Loss: 0.498460, Val Loss: 0.624203\n",
      "Ep 58: Train Loss: 0.541931, Val Loss: 0.531352\n",
      "Ep 59: Train Loss: 0.502422, Val Loss: 0.557718\n",
      "Ep 60: Train Loss: 0.496685, Val Loss: 0.538533\n",
      "Ep 61: Train Loss: 0.481049, Val Loss: 0.506164\n",
      "Ep 62: Train Loss: 0.457083, Val Loss: 0.537659\n",
      "Ep 63: Train Loss: 0.471195, Val Loss: 0.498989\n",
      "Ep 64: Train Loss: 0.451486, Val Loss: 0.496838\n",
      "Ep 65: Train Loss: 0.448929, Val Loss: 0.511125\n",
      "Ep 66: Train Loss: 0.441144, Val Loss: 0.480238\n",
      "Ep 67: Train Loss: 0.413353, Val Loss: 0.526256\n",
      "Ep 68: Train Loss: 0.398822, Val Loss: 0.491695\n",
      "Ep 69: Train Loss: 0.398725, Val Loss: 0.523125\n",
      "Ep 70: Train Loss: 0.393545, Val Loss: 0.515721\n",
      "Ep 71: Train Loss: 0.398683, Val Loss: 0.470209\n",
      "Ep 72: Train Loss: 0.387237, Val Loss: 0.493422\n",
      "Ep 73: Train Loss: 0.435281, Val Loss: 0.487197\n",
      "Ep 74: Train Loss: 0.424325, Val Loss: 0.487072\n",
      "Ep 75: Train Loss: 0.393718, Val Loss: 0.489016\n",
      "Ep 76: Train Loss: 0.438998, Val Loss: 0.571775\n",
      "Ep 77: Train Loss: 0.465785, Val Loss: 0.520618\n",
      "Ep 78: Train Loss: 0.467902, Val Loss: 0.516220\n",
      "Ep 79: Train Loss: 0.493786, Val Loss: 0.538842\n",
      "Ep 80: Train Loss: 0.435734, Val Loss: 0.502421\n",
      "Ep 81: Train Loss: 0.420757, Val Loss: 0.510526\n",
      "Ep 82: Train Loss: 0.411252, Val Loss: 0.495953\n",
      "Ep 83: Train Loss: 0.417637, Val Loss: 0.482068\n",
      "Ep 84: Train Loss: 0.413985, Val Loss: 0.535452\n",
      "Ep 85: Train Loss: 0.427779, Val Loss: 0.497771\n",
      "Ep 86: Train Loss: 0.403343, Val Loss: 0.508342\n",
      "Ep 87: Train Loss: 0.416740, Val Loss: 0.514070\n",
      "Ep 88: Train Loss: 0.394462, Val Loss: 0.521527\n",
      "Ep 89: Train Loss: 0.391897, Val Loss: 0.476327\n",
      "Ep 90: Train Loss: 0.362639, Val Loss: 0.472063\n",
      "Ep 91: Train Loss: 0.391953, Val Loss: 0.471939\n",
      "Ep 92: Train Loss: 0.423903, Val Loss: 0.539888\n",
      "Ep 93: Train Loss: 0.425818, Val Loss: 0.540562\n",
      "Ep 94: Train Loss: 0.413565, Val Loss: 0.494933\n",
      "Ep 95: Train Loss: 0.414723, Val Loss: 0.529047\n",
      "Ep 96: Train Loss: 0.425575, Val Loss: 0.508997\n",
      "Ep 97: Train Loss: 0.395767, Val Loss: 0.500443\n",
      "Ep 98: Train Loss: 0.366416, Val Loss: 0.486833\n",
      "Ep 99: Train Loss: 0.352294, Val Loss: 0.476596\n",
      "Ep 100: Train Loss: 0.353617, Val Loss: 0.452482\n",
      "Loss plot saved to loss_plot_mp.png\n",
      "Losses saved to losses_mp.npz\n",
      "\n",
      "Model saved to model_mp.pth\n",
      "\n",
      "[Cleanup] GPU cache cleared after MP training.\n",
      "\n",
      "\n",
      "==================================================\n",
      "Starting training for: BASELINE\n",
      "==================================================\n",
      "\n",
      "Start Training: BASELINE with 3-step rollout\n",
      "Ep 1: Train Loss: 1.485790, Val Loss: 1.015853\n",
      "Ep 2: Train Loss: 0.919706, Val Loss: 0.799057\n",
      "Ep 3: Train Loss: 0.784907, Val Loss: 0.774746\n",
      "Ep 4: Train Loss: 0.774083, Val Loss: 0.819370\n",
      "Ep 5: Train Loss: 0.768508, Val Loss: 0.795186\n",
      "Ep 6: Train Loss: 0.758632, Val Loss: 0.782105\n",
      "Ep 7: Train Loss: 0.738584, Val Loss: 0.768253\n",
      "Ep 8: Train Loss: 0.732779, Val Loss: 0.767038\n",
      "Ep 9: Train Loss: 0.732826, Val Loss: 0.743417\n",
      "Ep 10: Train Loss: 0.716816, Val Loss: 0.773830\n",
      "Ep 11: Train Loss: 0.714073, Val Loss: 0.752916\n",
      "Ep 12: Train Loss: 0.709355, Val Loss: 0.795349\n",
      "Ep 13: Train Loss: 0.691594, Val Loss: 0.773116\n",
      "Ep 14: Train Loss: 0.667535, Val Loss: 0.865515\n",
      "Ep 15: Train Loss: 0.690259, Val Loss: 0.732792\n",
      "Ep 16: Train Loss: 0.675871, Val Loss: 0.810571\n",
      "Ep 17: Train Loss: 0.681636, Val Loss: 0.757968\n",
      "Ep 18: Train Loss: 0.676037, Val Loss: 0.710071\n",
      "Ep 19: Train Loss: 0.655893, Val Loss: 0.757450\n",
      "Ep 20: Train Loss: 0.646748, Val Loss: 0.785470\n",
      "Ep 21: Train Loss: 0.638872, Val Loss: 0.775691\n",
      "Ep 22: Train Loss: 0.650493, Val Loss: 0.750760\n",
      "Ep 23: Train Loss: 0.633928, Val Loss: 0.789818\n",
      "Ep 24: Train Loss: 0.614404, Val Loss: 0.814725\n",
      "Ep 25: Train Loss: 0.621701, Val Loss: 0.750111\n",
      "Ep 26: Train Loss: 0.629863, Val Loss: 0.809321\n",
      "Ep 27: Train Loss: 0.624587, Val Loss: 0.743486\n",
      "Ep 28: Train Loss: 0.620066, Val Loss: 0.789880\n",
      "Ep 29: Train Loss: 0.627814, Val Loss: 0.760832\n",
      "Ep 30: Train Loss: 0.605567, Val Loss: 0.745425\n",
      "Ep 31: Train Loss: 0.574608, Val Loss: 0.811905\n",
      "Ep 32: Train Loss: 0.611108, Val Loss: 0.748837\n",
      "Ep 33: Train Loss: 0.571765, Val Loss: 0.723392\n",
      "Ep 34: Train Loss: 0.596660, Val Loss: 0.799633\n",
      "Ep 35: Train Loss: 0.622551, Val Loss: 0.775568\n",
      "Ep 36: Train Loss: 0.581380, Val Loss: 0.793458\n",
      "Ep 37: Train Loss: 0.567392, Val Loss: 0.731402\n",
      "Ep 38: Train Loss: 0.570112, Val Loss: 0.805890\n",
      "Ep 39: Train Loss: 0.556641, Val Loss: 0.712347\n",
      "Ep 40: Train Loss: 0.562700, Val Loss: 0.722335\n",
      "Ep 41: Train Loss: 0.551686, Val Loss: 0.794735\n",
      "Ep 42: Train Loss: 0.564479, Val Loss: 0.762776\n",
      "Ep 43: Train Loss: 0.576878, Val Loss: 0.775575\n",
      "Ep 44: Train Loss: 0.582493, Val Loss: 0.750320\n",
      "Ep 45: Train Loss: 0.546789, Val Loss: 0.745917\n",
      "Ep 46: Train Loss: 0.577180, Val Loss: 0.734827\n",
      "Ep 47: Train Loss: 0.593533, Val Loss: 0.788125\n",
      "Ep 48: Train Loss: 0.573889, Val Loss: 0.683242\n",
      "Ep 49: Train Loss: 0.566725, Val Loss: 0.797747\n",
      "Ep 50: Train Loss: 0.583238, Val Loss: 0.694390\n",
      "Ep 51: Train Loss: 0.564548, Val Loss: 0.703738\n",
      "Ep 52: Train Loss: 0.523635, Val Loss: 0.814048\n",
      "Ep 53: Train Loss: 0.528089, Val Loss: 0.788824\n",
      "Ep 54: Train Loss: 0.540243, Val Loss: 0.792367\n",
      "Ep 55: Train Loss: 0.562238, Val Loss: 0.846475\n",
      "Ep 56: Train Loss: 0.513165, Val Loss: 0.740797\n",
      "Ep 57: Train Loss: 0.497729, Val Loss: 0.745501\n",
      "Ep 58: Train Loss: 0.483947, Val Loss: 0.871874\n",
      "Ep 59: Train Loss: 0.508145, Val Loss: 0.741897\n",
      "Ep 60: Train Loss: 0.474879, Val Loss: 0.769416\n",
      "Ep 61: Train Loss: 0.472344, Val Loss: 0.691353\n",
      "Ep 62: Train Loss: 0.469277, Val Loss: 0.693111\n",
      "Ep 63: Train Loss: 0.481659, Val Loss: 0.777434\n",
      "Ep 64: Train Loss: 0.509529, Val Loss: 0.811138\n",
      "Ep 65: Train Loss: 0.503933, Val Loss: 0.767927\n",
      "Ep 66: Train Loss: 0.475965, Val Loss: 0.687542\n",
      "Ep 67: Train Loss: 0.486636, Val Loss: 0.767159\n",
      "Ep 68: Train Loss: 0.484991, Val Loss: 0.703021\n",
      "Ep 69: Train Loss: 0.476022, Val Loss: 0.850302\n",
      "Ep 70: Train Loss: 0.500755, Val Loss: 0.783510\n",
      "Ep 71: Train Loss: 0.520304, Val Loss: 0.735130\n",
      "Ep 72: Train Loss: 0.465908, Val Loss: 0.786192\n",
      "Ep 73: Train Loss: 0.474295, Val Loss: 0.755805\n",
      "Ep 74: Train Loss: 0.449952, Val Loss: 0.693799\n",
      "Ep 75: Train Loss: 0.428017, Val Loss: 0.712159\n",
      "Ep 76: Train Loss: 0.451168, Val Loss: 0.735368\n",
      "Ep 77: Train Loss: 0.472241, Val Loss: 0.787808\n",
      "Ep 78: Train Loss: 0.466501, Val Loss: 0.716129\n",
      "Ep 79: Train Loss: 0.469109, Val Loss: 0.803536\n",
      "Ep 80: Train Loss: 0.474350, Val Loss: 0.745000\n",
      "Ep 81: Train Loss: 0.469577, Val Loss: 0.683756\n",
      "Ep 82: Train Loss: 0.479782, Val Loss: 0.710355\n",
      "Ep 83: Train Loss: 0.466316, Val Loss: 0.733482\n",
      "Ep 84: Train Loss: 0.444746, Val Loss: 0.720161\n",
      "Ep 85: Train Loss: 0.455712, Val Loss: 0.665782\n",
      "Ep 86: Train Loss: 0.446622, Val Loss: 0.693422\n",
      "Ep 87: Train Loss: 0.432927, Val Loss: 0.757032\n",
      "Ep 88: Train Loss: 0.459496, Val Loss: 0.784430\n",
      "Ep 89: Train Loss: 0.457617, Val Loss: 0.724701\n",
      "Ep 90: Train Loss: 0.486815, Val Loss: 0.717923\n",
      "Ep 91: Train Loss: 0.482432, Val Loss: 0.760049\n",
      "Ep 92: Train Loss: 0.464951, Val Loss: 0.778472\n",
      "Ep 93: Train Loss: 0.456815, Val Loss: 0.777058\n",
      "Ep 94: Train Loss: 0.456465, Val Loss: 0.816899\n",
      "Ep 95: Train Loss: 0.481981, Val Loss: 0.798016\n",
      "Ep 96: Train Loss: 0.463712, Val Loss: 0.770431\n",
      "Ep 97: Train Loss: 0.453070, Val Loss: 0.785239\n",
      "Ep 98: Train Loss: 0.435040, Val Loss: 0.735096\n",
      "Ep 99: Train Loss: 0.430001, Val Loss: 0.720865\n",
      "Ep 100: Train Loss: 0.449748, Val Loss: 0.761654\n",
      "Loss plot saved to loss_plot_baseline.png\n",
      "Losses saved to losses_baseline.npz\n",
      "\n",
      "Model saved to model_baseline.pth\n",
      "\n",
      "[Cleanup] GPU cache cleared after BASELINE training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import gc  # Add garbage collector\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL_MODE = 'grf'  # 'grf', 'baseline', 'mp'\n",
    "DATA_PATH = \"/kaggle/input/4096-5/data/\"\n",
    "OUTPUT_PATH = '/kaggle/working/'\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3 # Learning Rate\n",
    "EPOCHS = 100\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "points_path = os.path.join(DATA_PATH, \"points.npy\")\n",
    "knn_path = os.path.join(DATA_PATH, \"knn_indices.npy\")\n",
    "\n",
    "\n",
    "K_NEIGHBORS = 6\n",
    "VAL_SPLIT = 0.1\n",
    "TRAIN_ROLLOUT_STEPS = 3  # Number of autoregressive steps during training\n",
    "\n",
    "# --- DYNAMIC KNN FUNCTION ---\n",
    "def compute_knn_torch(points, k):\n",
    "    \"\"\"Compute KNN dynamically on GPU - works with batched tensors\"\"\"\n",
    "    B, N, D = points.shape\n",
    "    # Compute pairwise distances\n",
    "    dist_mat = torch.cdist(points, points, p=2)\n",
    "    # Get k+1 nearest neighbors (includes self), then exclude self\n",
    "    _, knn_indices = torch.topk(dist_mat, k=k+1, dim=-1, largest=False)\n",
    "    return knn_indices[:, :, 1:]  # Shape: (B, N, k)\n",
    "\n",
    "# --- DATASET ---\n",
    "class RobotArmDataset(Dataset):\n",
    "    def __init__(self, points_path, knn_path, rollout_steps=1):\n",
    "        if not os.path.exists(points_path):\n",
    "            raise FileNotFoundError(f\"File {points_path} not found.\")\n",
    "        self.points = np.load(points_path).astype(np.float32)\n",
    "        self.knn = np.load(knn_path).astype(np.int64)\n",
    "        self.rollout_steps = rollout_steps\n",
    "\n",
    "        # Normalization stats\n",
    "        self.mean = np.mean(self.points, axis=(0, 1))\n",
    "        self.std = np.std(self.points, axis=(0, 1))\n",
    "        self.points = (self.points - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Account for needing rollout_steps future frames\n",
    "        return len(self.points) - self.rollout_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return input, knn, and sequence of rollout_steps targets\n",
    "        input_frame = torch.from_numpy(self.points[idx])\n",
    "        input_knn = torch.from_numpy(self.knn[idx])\n",
    "        # Stack future targets: [t+1, t+2, ..., t+rollout_steps]\n",
    "        targets = torch.stack([\n",
    "            torch.from_numpy(self.points[idx + i + 1])\n",
    "            for i in range(self.rollout_steps)\n",
    "        ])  # Shape: (rollout_steps, N, 3)\n",
    "        return input_frame, input_knn, targets\n",
    "\n",
    "# --- MODELS ---\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k = torch.relu(q) + 1e-6, torch.relu(k) + 1e-6\n",
    "        kv = torch.einsum(\"bnd,bne->bde\", k, v)\n",
    "        z = 1 / (torch.einsum(\"bnd,bd->bn\", q, k.sum(dim=1)) + 1e-6)\n",
    "        attn = torch.einsum(\"bnd,bde,bn->bne\", q, kv, z)\n",
    "        return self.to_out(attn)\n",
    "\n",
    "class TopologicalGRFLayer(nn.Module):\n",
    "    def __init__(self, dim, k_neighbors, hops=5):\n",
    "        super().__init__()\n",
    "        self.k = k_neighbors\n",
    "        self.hops = hops\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, knn_idx):\n",
    "        B, N, D = x.shape\n",
    "        # Sparse Matrix Construction\n",
    "        src = torch.arange(N, device=x.device).view(1, N, 1).expand(B, N, self.k)\n",
    "        batch_off = torch.arange(B, device=x.device).view(B, 1, 1) * N\n",
    "        indices = torch.stack([(knn_idx + batch_off).view(-1), (src + batch_off).view(-1)])\n",
    "        values = torch.ones(indices.shape[1], device=x.device)\n",
    "        adj = torch.sparse_coo_tensor(indices, values, (B*N, B*N))\n",
    "\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        v_f, k_f = v.view(B*N, D), k.view(B*N, D)\n",
    "\n",
    "        # Random Walk Diffusion\n",
    "        for _ in range(self.hops):\n",
    "            v_f = torch.sparse.mm(adj, v_f) / (self.k + 1e-6)\n",
    "            k_f = torch.sparse.mm(adj, k_f) / (self.k + 1e-6)\n",
    "\n",
    "        attn = (q * k_f.view(B, N, D)).sum(dim=-1, keepdim=True)\n",
    "        return self.to_out(attn * v_f.view(B, N, D))\n",
    "\n",
    "class SimpleMessagePassing(nn.Module):\n",
    "    def __init__(self, dim, k_neighbors):\n",
    "        super().__init__()\n",
    "        self.k = k_neighbors\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, knn_idx):\n",
    "        B, N, D = x.shape\n",
    "        flat_idx = knn_idx.reshape(B, N * self.k).unsqueeze(-1).expand(-1, -1, D)\n",
    "        neighbors = torch.gather(x, 1, flat_idx.reshape(B, N * self.k, D).long()).reshape(B, N, self.k, D)\n",
    "        return self.proj(neighbors.mean(dim=2))\n",
    "\n",
    "class UnifiedInterlacer(nn.Module):\n",
    "    def __init__(self, mode='grf', input_dim=3, embed_dim=64, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "        # Create layer norms for each layer (2 per block: graph + attention)\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(embed_dim) for _ in range(num_layers * 2)])\n",
    "\n",
    "        # Create graph layers (GRF, MP, or Identity based on mode)\n",
    "        if mode == 'grf':\n",
    "            self.graph_layers = nn.ModuleList([TopologicalGRFLayer(embed_dim, K_NEIGHBORS) for _ in range(num_layers)])\n",
    "        elif mode == 'mp':\n",
    "            self.graph_layers = nn.ModuleList([SimpleMessagePassing(embed_dim, K_NEIGHBORS) for _ in range(num_layers)])\n",
    "        else:\n",
    "            self.graph_layers = nn.ModuleList([nn.Identity() for _ in range(num_layers)])\n",
    "\n",
    "        # Create attention layers\n",
    "        self.attn_layers = nn.ModuleList([LinearAttention(embed_dim) for _ in range(num_layers)])\n",
    "\n",
    "        self.head = nn.Linear(embed_dim, 3)\n",
    "\n",
    "    def forward(self, x, knn):\n",
    "        h = self.embedding(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # Graph layer\n",
    "            if self.mode != 'baseline':\n",
    "                h = h + self.graph_layers[i](self.norms[i * 2](h), knn)\n",
    "            else:\n",
    "                h = h + self.norms[i * 2](h)\n",
    "\n",
    "            # Attention layer\n",
    "            h = h + self.attn_layers[i](self.norms[i * 2 + 1](h))\n",
    "\n",
    "        return self.head(h)\n",
    "\n",
    "# --- MAIN ---\n",
    "def main():\n",
    "    dataset = RobotArmDataset(points_path, knn_path, rollout_steps=TRAIN_ROLLOUT_STEPS)\n",
    "    train_size = int(len(dataset) * (1 - VAL_SPLIT))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = UnifiedInterlacer(mode=MODEL_MODE).to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(f\"Start Training: {MODEL_MODE.upper()} with {TRAIN_ROLLOUT_STEPS}-step rollout\")\n",
    "    for ep in range(EPOCHS):\n",
    "        # Training with multi-step rollout\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for x, knn, targets in train_loader:\n",
    "            # x: (B, N, 3), knn: (B, N, K), targets: (B, ROLLOUT_STEPS, N, 3)\n",
    "            x, knn, targets = x.to(DEVICE), knn.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Multi-step rollout (BPTT - no detach)\n",
    "            current_input = x\n",
    "            current_knn = knn\n",
    "            step_losses = []\n",
    "\n",
    "            for step in range(TRAIN_ROLLOUT_STEPS):\n",
    "                # Predict next frame\n",
    "                pred = model(current_input, current_knn)\n",
    "\n",
    "                # Loss against ground truth target at this step\n",
    "                gt_target = targets[:, step]  # (B, N, 3)\n",
    "                step_loss = criterion(pred, gt_target)\n",
    "                step_losses.append(step_loss)\n",
    "\n",
    "                # CLOSED LOOP: prediction becomes next input (NO detach for BPTT)\n",
    "                current_input = pred\n",
    "\n",
    "                # DYNAMIC KNN: recompute graph on predicted coordinates\n",
    "                current_knn = compute_knn_torch(current_input, K_NEIGHBORS)\n",
    "\n",
    "            # Total loss = average across all steps\n",
    "            total_loss = torch.stack(step_losses).mean()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_losses.append(total_loss.item())\n",
    "\n",
    "        # Validation with same rollout\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x, knn, targets in val_loader:\n",
    "                x, knn, targets = x.to(DEVICE), knn.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "                current_input = x\n",
    "                current_knn = knn\n",
    "                step_losses = []\n",
    "\n",
    "                for step in range(TRAIN_ROLLOUT_STEPS):\n",
    "                    pred = model(current_input, current_knn)\n",
    "                    gt_target = targets[:, step]\n",
    "                    step_loss = criterion(pred, gt_target)\n",
    "                    step_losses.append(step_loss)\n",
    "\n",
    "                    current_input = pred\n",
    "                    current_knn = compute_knn_torch(current_input, K_NEIGHBORS)\n",
    "\n",
    "                total_loss = torch.stack(step_losses).mean()\n",
    "                epoch_val_losses.append(total_loss.item())\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        val_loss = np.mean(epoch_val_losses)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Ep {ep+1}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    # Plot and save loss curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, EPOCHS + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, EPOCHS + 1), val_losses, label='Validation Loss')\n",
    "    plt.title(f'Training and Validation Loss - {MODEL_MODE.upper()} ({TRAIN_ROLLOUT_STEPS}-step rollout)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.savefig(f'/kaggle/working/loss_plot_{MODEL_MODE}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved to loss_plot_{MODEL_MODE}.png\")\n",
    "\n",
    "    # Save losses to numpy file\n",
    "    np.savez(f'losses_{MODEL_MODE}.npz',\n",
    "             train_losses=np.array(train_losses),\n",
    "             val_losses=np.array(val_losses))\n",
    "    print(f\"Losses saved to losses_{MODEL_MODE}.npz\")\n",
    "    print()\n",
    "\n",
    "    # SAVE MODEL WEIGHTS\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'mean': dataset.mean,\n",
    "        'std': dataset.std,\n",
    "        'mode': MODEL_MODE,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'rollout_steps': TRAIN_ROLLOUT_STEPS\n",
    "    }, f\"{OUTPUT_PATH}model_{MODEL_MODE}.pth\")\n",
    "    print(f\"Model saved to model_{MODEL_MODE}.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # os.chdir(DATA_PATH)\n",
    "\n",
    "    for mode in ['grf', 'mp', 'baseline']:\n",
    "        MODEL_MODE = mode\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Starting training for: {MODEL_MODE.upper()}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "        main()\n",
    "\n",
    "        # --- AGGRESSIVE CLEANUP ---\n",
    "        # Clear all cached memory on GPU\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        # Force Python garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\"\\n[Cleanup] GPU cache cleared after {MODEL_MODE.upper()} training.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8999321,
     "sourceId": 14124797,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
