@inproceedings{reid2025linear,
  title={Linear Transformer Topological Masking with Graph Random Features},
  author={Isaac Reid and Kumar Avinava Dubey and Deepali Jain and William F Whitney and Amr Ahmed and Joshua Ainslie and Alex Bewley and Mithun George Jacob and Aranyak Mehta and David Rendleman and Connor Schenck and Richard E. Turner and Ren{\'e} Wagner and Adrian Weller and Krzysztof Marcin Choromanski},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=6MBqQLp17E}
}

@inproceedings{vaswani2017attention,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and \L{}ukasz Kaiser and Illia Polosukhin},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  url={https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}

@inproceedings{ying2021do,
  title={Do Transformers Really Perform Badly for Graph Representation?},
  author={Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28877--28888},
  year={2021},
  url={https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html}
}

@inproceedings{choromanski2022rethinking,
  title={From Block-Toeplitz Matrices to Differential Equations on Graphs: Towards a General Theory for Scalable Masked Transformers},
  author={Krzysztof Choromanski and Han Lin and Haoxian Chen and Tianyi Zhang and Arijit Sehanobish and Valerii Likhosherstov and Jack Parker-Holder and Tamas Sarlos and Adrian Weller and Thomas Weingarten},
  booktitle={International Conference on Machine Learning},
  pages={3962--3983},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v162/choromanski22a.html}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Fran\c{c}ois Fleuret},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/katharopoulos20a.html}
}
@inproceedings{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and others},
  booktitle={arXiv preprint arXiv:2009.14794},
  pages={},
  year={2020},
  organization={PMLR},
  url={https://doi.org/10.48550/arXiv.2009.14794}
}

@inproceedings{luo2021stable,
  title={Table, fast and accurate: Kernelized attention with relative positional encoding},
  author={Shengjie Luo and Shanda Li and Tianle Cai and Di He and Dinglan Peng and Shuxin Zheng and Guolin Ke and Liwei Wang and Tie-Yan Liu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={22795--22807},
  year={2021},
  organization={PMLR},
  url={https://doi.org/10.48550/arXiv.2106.12566}
}

@inproceedings{liutkus2021relative,
  title={Relative positional encoding for transformers with linear complexity},
  author={Antoine Liutkus and Ond{\v{r}}ej C\'ifka and Shih-Lun Wu and Umut Simsekli and Yi-Hsuan Yang and Gael Richard},
  booktitle={International Conference on Machine Learning},
  pages={7067--707},
  year={2021},
  organization={PMLR},
  url={ https://doi.org/10.48550/arXiv.2105.08399}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@InProceedings{grf-v202-choromanski23a,
  title = 	 {Taming graph kernels with random features},
  author =       {Choromanski, Krzysztof Marcin},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {5964--5977},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/choromanski23a/choromanski23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/choromanski23a.html},
  abstract = 	 {We introduce in this paper the mechanism of graph random features (GRFs). GRFs can be used to construct unbiased randomized estimators of several important kernels defined on graphsâ€™ nodes, in particular the regularized Laplacian kernel. As regular RFs for non-graph kernels, they provide means to scale up kernel methods defined on graphs to larger networks. Importantly, they give substantial computational gains also for smaller graphs, while applied in downstream applications. Consequently, GRFs address the notoriously difficult problem of cubic (in the number of the nodes of the graph) time complexity of graph kernels algorithms. We provide a detailed theoretical analysis of GRFs and an extensive empirical evaluation: from speed tests, through Frobenius relative error analysis to kmeans graph-clustering with graph kernels. We show that the computation of GRFs admits an embarrassingly simple distributed algorithm that can be applied if the graph under consideration needs to be split across several machines. We also introduce a (still unbiased) quasi Monte Carlo variant of GRFs, q-GRFs, relying on the so-called reinforced random walks that might be used to optimize the variance of GRFs. As a byproduct, we obtain a novel approach to solve certain classes of linear equations with positive and symmetric matrices.}
}
