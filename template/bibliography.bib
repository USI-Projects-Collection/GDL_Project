@inproceedings{reid2025linear,
  title={Linear Transformer Topological Masking with Graph Random Features},
  author={Isaac Reid and Kumar Avinava Dubey and Deepali Jain and William F Whitney and Amr Ahmed and Joshua Ainslie and Alex Bewley and Mithun George Jacob and Aranyak Mehta and David Rendleman and Connor Schenck and Richard E. Turner and Ren{\'e} Wagner and Adrian Weller and Krzysztof Marcin Choromanski},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=6MBqQLp17E}
}

@inproceedings{vaswani2017attention,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and \L{}ukasz Kaiser and Illia Polosukhin},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017},
  url={https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}

@inproceedings{ying2021do,
  title={Do Transformers Really Perform Badly for Graph Representation?},
  author={Chengxuan Ying and Tianle Cai and Shengjie Luo and Shuxin Zheng and Guolin Ke and Di He and Yanming Shen and Tie-Yan Liu},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28877--28888},
  year={2021},
  url={https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html}
}

@inproceedings{choromanski2022rethinking,
  title={From Block-Toeplitz Matrices to Differential Equations on Graphs: Towards a General Theory for Scalable Masked Transformers},
  author={Krzysztof Choromanski and Han Lin and Haoxian Chen and Tianyi Zhang and Arijit Sehanobish and Valerii Likhosherstov and Jack Parker-Holder and Tamas Sarlos and Adrian Weller and Thomas Weingarten},
  booktitle={International Conference on Machine Learning},
  pages={3962--3983},
  year={2022},
  organization={PMLR},
  url={https://proceedings.mlr.press/v162/choromanski22a.html}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  author={Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Fran\c{c}ois Fleuret},
  booktitle={International Conference on Machine Learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR},
  url={https://proceedings.mlr.press/v119/katharopoulos20a.html}
}
@inproceedings{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Krzysztof Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Davis and Afroz Mohiuddin and Lukasz Kaiser and others},
  booktitle={arXiv preprint arXiv:2009.14794},
  pages={},
  year={2020},
  organization={PMLR},
  url={https://doi.org/10.48550/arXiv.2009.14794}
}

@inproceedings{luo2021stable,
  title={Table, fast and accurate: Kernelized attention with relative positional encoding},
  author={Shengjie Luo and Shanda Li and Tianle Cai and Di He and Dinglan Peng and Shuxin Zheng and Guolin Ke and Liwei Wang and Tie-Yan Liu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={22795--22807},
  year={2021},
  organization={PMLR},
  url={https://doi.org/10.48550/arXiv.2106.12566}
}

@inproceedings{liutkus2021relative,
  title={Relative positional encoding for transformers with linear complexity},
  author={Antoine Liutkus and Ond{\v{r}}ej C\'ifka and Shih-Lun Wu and Umut Simsekli and Yi-Hsuan Yang and Gael Richard},
  booktitle={International Conference on Machine Learning},
  pages={7067--707},
  year={2021},
  organization={PMLR},
  url={ https://doi.org/10.48550/arXiv.2105.08399}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

