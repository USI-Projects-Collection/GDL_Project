\subsection{Vision Transformers (ViTs)}

To evaluate the effectiveness of the proposed topological masking in a domain with fixed, regular structure, the original paper \cite{reid2025linear} applies GRFs to the Vision Transformer (ViT) architecture. In this framework, the input image is decomposed into patches, and the underlying topology $\mathcal{G}$ is defined as a 2D grid graph where nodes (patches) are connected if they are spatial neighbors. \\
The central methodological contribution is the modulation of the Linear Attention mechanism by a learnable mask $M_{\alpha}(\mathcal{G}) := \sum \alpha_k W^k$.
This introduces a structural inductive bias into the transformer, which theoretical allows it to capture local dependencies more effectively than unmasked attention, while avoiding the quadratic costs of Softmax attention \cite{reid2025linear}. To validate this, the work compares five distinct attention mechanisms:

\begin{itemize}
    \item \textbf{Unmasked Softmax Attention ($\mathcal{O}(N^2)$):} The standard baseline used in "vanilla" Transformers.
    \item \textbf{Unmasked Linear Attention ($\mathcal{O}(N)$):} An efficient alternative using decompositions with feature maps $\phi(Q)(\phi(K)^\top V)$, which lacks any topological awareness \cite{katharopoulos2020transformers}.
    \item \textbf{Toeplitz-masked Linear Attention ($\mathcal{O}(N \log N)$):} A strong baseline for grid graphs (images), where the mask decays based on the Manhattan distance between nodes (patches). This represents a "hard-coded" structural bias \cite{choromanski2022rethinking}.
    \item $\mathbf{M}_{\alpha}(\mathcal{G})$\textbf{-masked linear ($\mathcal{O}(N^2)$):} The theoretical limit of the proposed method, where the full-rank mask matrix $\mathbf{M}_{\alpha}$ is explicitly computed.
    \item \textbf{GRF-masked Linear Attention ($\mathcal{O}(N)$):} The proposed method, which approximates the exact mask implicitly using sparse features generated by importance sampling of random walks.
\end{itemize}

\textit{\textbf{Ablation Studies:}} To evaluate the approximation quality of the GRF estimator in depth, the original paper\cite{reid2025linear} includes an ablation study on the number of random walks $n$. The theoretical premise is that increasing $n$ reduces the variance of the mask estimator $\hat{mathbf{M}}$ and thereby improves the approximation of the actual topological mask $\mathbf{M}_{\alpha}$. In this study, $n$ is varied logarithmically to verify whether the performance converges to the $\mathbf{M}_{\alpha}(\mathcal{G})$\textbf{-masked linear} baseline, isolating the trade-off between accuracy (estimator variance) and computational cost (feature sparsity).