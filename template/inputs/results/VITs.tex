\subsection{Vision Transformers (ViTs)}
We present the results of our reproduction experiments (Table~\ref{tab:accuracies}), comparing the proposed GRF-masked linear attention against all the other variants.

\begin{table}[H]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|}
    \toprule
    \textbf{Variant} & \textbf{\shortstack{CIFAR-10\\(15 ep)}} & \textbf{\shortstack{CIFAR-100\\(30 ep)}} & \textbf{\shortstack{CIFAR-10\\(30 ep)}} & \textbf{\shortstack{FashionMNIST\\(10 ep)}} & \textbf{\shortstack{CIFAR-10\\(Scaled)}} \\
    \midrule
    Unmasked softmax & 56.11 & 28.67 & 57.31 & 87.49 & 56.42 \\
    Toeplitz-masked linear & 57.25 & 31.80 & 60.59 & 87.92 & 57.90 \\
    $\mathbf{M_{\alpha}(\mathcal{G})}$\textbf{-masked linear} & \textbf{56.83} & \textbf{30.11} & \textbf{58.02} & \textbf{86.77} & \textbf{58.95} \\
    \hline
    Unmasked linear & 56.69 & 31.59 & 58.52 & 87.41 & 59.58 \\
    \textbf{GRF-masked linear} & \textbf{56.69} & \textbf{31.99} & \textbf{60.37} & \textbf{87.11} & \textbf{58.93} \\
    \bottomrule
\end{tabular}%
}
\caption{Accuracies results for all ViT attention variants across different datasets and training epochs.}
\label{tab:accuracies}
\end{table}
In the most challenging configurations, the proposed GRF masking demonstrates a clear advantage over standard unmasked linear attention. For instance, on CIFAR-10 (30 epochs) and CIFAR-100 dataset, GRF outperforms the unmasked variant.\\
The Toeplitz-masked baseline, which is specifically engineered for grid-structured data like images, generally performs the best (e.g., 60.59\% on CIFAR-10 30ep). However, the GRF-masked method remains highly competitive (within $\approx 0.2-0.8\%$ in most cases).\\
In this scaled-down regime, the linear attention variants often outperform the quadratic Unmasked Softmax baseline (e.g., 57.31\% on CIFAR-10 30ep vs. 60.37\% for GRF).\\
The benefits of topological masking become more pronounced with longer training. On CIFAR-10, extending training from 15 to 30 epochs widens the performance gap between GRF-masked attention and the unmasked baseline.
\paragraph{Ablation Studies:} 
Figure~\ref{fig:ablation_study} shows a clear trend: adding more walkers consistently improves accuracy. Performance rises from 51.4\% with a single walker to 55.0\% with 1000 walkers. This confirms that using more walkers creates a more precise and stable approximation of the graph mask, reducing noise and helping the model learn better spatial relationships.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/ablation_plot.png}
    \caption{Ablation study effects.}
    \label{fig:ablation_study}
\end{figure}