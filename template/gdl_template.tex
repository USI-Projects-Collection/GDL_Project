\documentclass{gdl}

\usepackage[T1]{fontenc} 
\usepackage[utf8]{inputenc} 
\usepackage{lmodern} 
\usepackage{graphicx} 
\usepackage{hyperref} 
\usepackage{geometry} 
\usepackage{longtable} 
\usepackage{tcolorbox}
\usepackage{booktabs} 
\usepackage{textcomp} 
\usepackage{fontawesome5} 
\usepackage{array} 
\usepackage{enumitem}
\usepackage{tocloft}
\usepackage{float}

% CHANGE THESE
\def\groupid{------}
\def\projectid{------}

\setlength{\parindent}{0pt}
\begin{document}

% CHANGE THIS
\title{Linear Transformer Topological Masking with Graph Random Features}

% CHANGE THIS
\author{%
Paolo Deidda, Raffaele Perri, Paul Leopold Seipl\\
\texttt{\{paolo.deidda, raffaele.perri, paul.leopold.seipl\}@usi.ch}
}

\begin{abstract}
Concise and self-contained description of your project, motivation and main findings.
\end{abstract}

\maketitle
\vfill
\tableofcontents

\newpage

\twocolumn

% ==========================================================================
% ==========================================================================
\section{Introduction}
Transformers have established themselves as a dominant architecture across various machine learning modalities, deriving their power from the attention mechanism which models complex dependencies between tokens \cite{vaswani2017attention}. However, the standard Transformer treats input data as a set, making it invariant to permutation and inherently unaware of structural dependencies, such as the connectivity in graph-structured data. To address this, \textit{Topological Masking} is often employed, where the attention mechanism is modulated by a function of the graph structure (e.g., shortest path distance or adjacency), injecting a necessary structural inductive bias \cite{ying2021do, choromanski2022rethinking}.

A fundamental computational conflict arises when scaling this approach to large graphs. Standard ``Vanilla'' Softmax attention requires explicitly computing and storing an $N \times N$ attention matrix, resulting in quadratic $\mathcal{O}(N^2)$ time and space complexity. While Linear Attention mechanisms address this bottleneck by leveraging low-rank decompositions $\phi(Q)(\phi(K)^T V)$ to achieve $\mathcal{O}(N)$ complexity \cite{katharopoulos2020transformers}, they rely strictly on the associativity of matrix multiplication. Introducing a topological mask $M$ generally breaks this associativity—since $(A \times B) \odot M \neq A \times (B \odot M)$—forcing the re-materialization of the dense attention matrix and negating the efficiency gains of linear attention.

In this work, we focus on the reproduction of ``Linear Transformer Topological Masking with Graph Random Features'' \cite{reid2025linear}. The authors propose a novel solution to the masking conflict by approximating the topological mask using Graph Random Features (GRFs). By decomposing the mask into sparse feature vectors derived from random walks, the method allows the mask to be fused with query and key features via a tensor product, preserving the $\mathcal{O}(N)$ complexity of linear attention while maintaining the expressivity of topological masking.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/schematic_overview.png}
    \caption{Conceptual comparison of attention mechanisms. The left panel illustrates the quadratic bottleneck of Softmax attention. The middle panel shows the efficient flow of Linear attention. The right panel demonstrates the proposed GRF method, which preserves linear complexity by fusing sparse topological features $\psi_M$ with the query/key representations.}
    \label{fig:attention_comparison}
\end{figure}
\paragraph{Contributions and Replication Results}
Our primary contribution is a validation of the theoretical and empirical claims presented in the original paper. We implemented the proposed GRF-based masking mechanism and conducted a rigorous analysis of its computational scaling behavior.

\begin{itemize}
    \item \textbf{Time Complexity Verification:} We replicated the scaling experiment comparing Softmax, Linear, and GRF-Masked attention. Our results confirm the theoretical claims: while Softmax attention scales quadratically, exhibiting a ``computational explosion'' for $N > 8$, the GRF-Masked method maintains strict linear scaling $\mathcal{O}(N)$.
    \item \textbf{Vision Transformers (ViT):} We extended the evaluation to image classification tasks (\textit{CIFAR-10, CIFAR-100, FashionMNIST}) by treating images as 2D grid graphs. Our results demonstrate that GRF-masked linear attention effectively captures spatial dependencies, achieving performance competitive with grid-specific baselines (Toeplitz masking) and consistently outperforming unmasked linear attention. Additionally, our ablation study confirms that increasing the number of random walkers (n) monotonically improves classification accuracy, validating the theoretical link between estimator variance and downstream performance.
    \item \textbf{Point Cloud Temporal State Prediction (PCTs):} To test the method's efficacy on dynamic, irregular topologies, we replicated the High-Density Visual Particle Dynamics (HD-VPD) experiment using a physics-based simulation of a robotic arm. Our "closed-loop" rollout evaluation reveals that the GRF-based model maintains structural integrity significantly longer than unmasked baselines. While standard message-passing approaches degrade slowly due to rigid local constraints, the GRF mechanism successfully captures broader kinematic dependencies via random walks, demonstrating superior stability in predicting future states.
\end{itemize}

% ==========================================================================
% ==========================================================================
\section{Related works}
\subsubsection{Standard and Topological Attention}
The standard self-attention mechanism, introduced by \cite{vaswani2017attention}, computes a dense $N \times N$ attention matrix $A = \text{softmax}(QK^\top / \sqrt{d})$. While highly effective at modeling global dependencies, this approach incurs quadratic $\mathcal{O}(N^2)$ time and space complexity, prohibiting its use on long sequences or large graphs.
To adapt Transformers to graph-structured data, \textit{topological masking} is often employed to inject structural inductive bias \cite{ying2021do}. This typically involves modulating the attention scores with a mask $M(\mathcal{G})$ derived from the graph topology (e.g., shortest path distances), forcing tokens to attend preferentially to their structural neighbors. However, applying such masks generally requires materializing the full $N \times N$ matrix, maintaining the prohibitive quadratic bottleneck.

\subsubsection{Linear Attention and The Masking Conflict}
To address the scalability limits of standard attention, \textit{Linear Attention} mechanisms have been proposed \cite{katharopoulos2020transformers, choromanski2020rethinking}. These methods replace the softmax kernel with a feature map decomposition $\phi(\cdot)$, allowing the computation to be reordered via associativity: $D^{-1}\phi(Q)(\phi(K)^\top V)$. This reduces complexity to $\mathcal{O}(N)$.
However, as noted in the foundational literature, introducing an element-wise topological mask $M$ breaks this associativity, i.e., $(A \times B) \odot M \neq A \times (B \odot M)$. Consequently, standard linear attention methods are generally incompatible with flexible topological masking without reverting to quadratic complexity.
\subsubsection{Efficient Masking Approaches}
Several approaches have attempted to reconcile efficiency with masking, though often with restrictions on graph topology.
\cite{luo2021stable} and \cite{choromanski2022rethinking} proposed using Toeplitz matrices and the Fast Fourier Transform (FFT) to implement masking in $\mathcal{O}(N \log N)$ time. While an improvement over $\mathcal{O}(N^2)$, these methods are largely restricted to highly structured graphs like grids or trees and do not generalize easily to arbitrary topologies.
Other methods, such as stochastic positional encoding \cite{liutkus2021relative}, achieve linear complexity but are similarly limited to sequence data (1D grids).
The method reproduced in this work \cite{reid2025linear} distinguishes itself by supporting general graphs with strict $\mathcal{O}(N)$ complexity via a randomized low-rank decomposition of the mask itself.
% ==========================================================================
% ==========================================================================
\input{inputs/methodology/methodolgy.tex}
% ==========================================================================
% ==========================================================================
\input{inputs/implementation/implementation.tex}
% ==========================================================================
% ==========================================================================
\input{inputs/results/results.tex}
% ==========================================================================
% ==========================================================================
\section{Discussion}

\subsection{Time Complexity}
Our reproduction successfully confirms that the GRF-based masking mechanism preserves the $\mathcal{O}(N)$ complexity of linear attention. We observed an empirical crossover point at $N=8$; beyond this threshold, the cost of standard Softmax attention explodes quadratically, becoming approximately $511 \times$ more expensive than linear variants at N=4096. This divergence verifies that for large-scale applications ($N \geq 30,000$), quadratic attention is computationally intractable.\\
While maintaining linear scaling, the GRF method incurs a constant overhead of $\approx 3.14 \times$ compared to unmasked linear attention. This factor corresponds directly to the sparsity of the graph features (average unique nodes visited per walk) and represents the necessary computational price for injecting structural bias. While our theoretical FLOP analysis places the crossover at $N=8$, practical speedups may require slightly larger N due to the relative hardware inefficiency of sparse matrix operations compared to optimized dense multiplication. Nevertheless, the asymptotic advantage guarantees GRF superiority for large graphs.

\subsection{Vision Transformers (ViTs)}
Our adjusted experiments only partially confirms the original paper's\cite{reid2025linear} hypothesis. While the original work reported consistent, significant gains (e.g., 3,7\% on ImageNet), our results in Table~\ref{tab:accuracies} show that GRF-masked attention provides inconsistent benefits at this smaller scale. \\
On one hand GRF significantly outperformed the unmasked linear baseline on \textbf{CIFAR-10}(+1,85\%) and marginally on \textbf{CIFAR-100}(+0,4\%). On the other hand, GRF performed as well as or worse than the unmasked baseline on \textbf{FashionMNIST} and \textbf{CIFAR-10} (15 ep). This suggests that inductive bias is less effective than claimed on simpler tasks or shorter training schedules. \\
The outstanding performance of the Toeplitz-masked baseline indicates that for fixed grid topologies such as images, the stochastic noise caused by GRF outweighs its flexibility, making deterministic relative positional encodings superior. \\
The deviation from the original paper's \cite{reid2025linear} strong results is likely due to scale. The original experiments used $16 \times 16$ patches ($N = 256$), whereas our constrained setup used $8 \times 8$ patches ($N=64$). At $N=64$, global attention is computationally cheap and easy to learn; the "long-range dependency" problem that topological masking aims to solve is virtually non-existent. Consequently, the GRF mechanism added variance without providing the necessary structural regularization required at larger sequence lengths.

\subsection{PCTs: Point Cloud Temporal State prediction}
Our replication of the experiment yielded evidence for the efficacy of topological masking. The stark contrast between the Baseline and the topological models (MP and GRF) in the autoregressive setting confirms that \textbf{structural inductive bias is not optional but necessary} for physical dynamics prediction.

\textbf{Failure of the Baseline.} The Unmasked Linear Transformer failed to maintain structural coherence over time (Figure \ref{fig:rollout_accuracy}). By forcing the model to predict 3 steps into the future during training, we exposed the limitation of the "Teacher Forcing" approach on unstructured models. Without a graph topology to constrain the particles, the Baseline treats the point cloud as a nebulous set, leading to rapid drift and "explosion" of the robotic arm structure during inference.

\textbf{GRF vs. Message Passing.} The results show that GRF is highly competitive to the standard Message Passing (MP) approach. It is worth noting that our simulation involves a \textit{rigid} robotic arm. Rigid body dynamics favor MP because constraints are strictly local (a point's position is deterministic given its neighbors). Despite this "home field advantage" for MP, the GRF model (configured with $k=4$ neighbors but 5 random walk hops) successfully matched and eventually surpassed MP stability. This suggests that the \textit{diffusion} mechanism of GRF allows it to capture kinematic chain dependencies (e.g., how the base rotation affects the finger tip) that a strictly local MP layer might miss, validating the method's robustness even outside the fluid-dynamics domain of the original paper.

\section{Conclusion}
% ==========================================================================
% ==========================================================================
% Bibliography
\onecolumn
\bibliography{bibliography}
\bibliographystyle{unsrtnat}
\clearpage

\end{document}
